\chapter{存储器层次结构设计}

\begin{flushright}
\epigraph{理想情况下，我们希望拥有无限大的内存容量，这样就可以立
刻访问任何一个特定的…机器字…但我们…不得不认识到有
可能需要构建分层结构的存储器，每一层次容量都要大于前一层次，
，但其访闷速度也要更怪一些。}{A. W.Burks、H. H.Goldstine 及
J.von Neumann，
Preliminary Discussion ofthe
Logical Design of an Electronic
Computing Instrument （1946）}
\end{flushright}
\newpage

\section{引言}
一些计算机先驱准确地预测到程序员肯定会希望拥有无限数量的快速存储器。满足这一愿
望的一种经济型解决方案是\emph{存储器层次结构}，这种方式利用了局域性原理，并在存储器技术的
性能与成本之间进行了折中。\hyperref[subsec:PrincipleOfLocality]{第1章中介绍的局域性原理}是指，大多数程序都不会均衡地访问
所有代码或数据。局域性可以在时间域发生（即时间局域性），也可以在空间域发生（即空间局
域性）。根据这一原理和“在给定实现技术和功率预算的情况下，硬件越小，速度可以越快”的
准则，就产生了存储器层次结构，这些层次由不同速度、不同大小的存储器组成。图2-1显示
了一个多级存储器层次结构，包括典型大小和访问速度。

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{imgs/sam.png}
	\caption{典型存储器层次结构中的级别，图中上部为服务器计算机中的层次结构，下部为 PMD（个人移动
            设备）中的层次结构。存储器级别离处理器越远，速度越慢、容量越大。注意，时间单位改变了
            10°倍（从皮秒变为毫秒），容量单位改变了102倍（从字节到百万兆字节）。PMD的时钟频率较
            低，缓存和主存储器较小。服务器与桌面计算机的主要区别是以磁盘存储作为层次结构中的最低
            层级，而 PMD 则采用以 EEPROM 技术制造的 Flash 作为最低层级}
\end{figure}

由于快速存储器非常昂贵，所以将存储器层次结构分为几个级别—越接近处理器，容量
越小、速度越快、每字节的成本也越高。其目标是提供一种存储器系统，每字节的成本几乎与
最便宜的存储器级别相同，速度几乎与最快速的级别相同。在大多数（但并非全部）情况下，
低层级存储器中的数据是其上一级存储器中数据的超集。这一性质称为包含性质，层次结构的
最低级别必须具备这一性质，如果这一最低级别是缓存，则由主存储器组成，如果是虚拟存储
器，则由磁盘存储器组成。

随着处理器性能的提高，存储器层次结构的重要性也在不断增加。图 2-2 是单处理器性能
和主存储器访问次数的历史发展过程。处理器曲线显示了平均每秒发出存储器请求数的增加（即
两次存储器引用之间延迟值的倒数），而存储器曲线显示每秒 \verb|DRAM| 访问数的增加（即 \verb|DRAM|
访问延迟的倒数）。在单处理器中，由于峰值存储器访问速度要快于平均速度（也就是图中绘制
的速度），所以其实际情况要稍差一些。

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{imgs/sam.png}
	\caption{以 1980年的性能为基准，处理器的存储器请求（对于单处理器或单核心）与 DRAM访问延迟之
            间性能差距的时间变化曲线，该差距是通过测量存储请求数和 DRAM访问数目得出的。注意，
            为了记录处理器与 DRAM性能差距的大小，纵轴必须采用对数刻度。存储器基准为1980年的64
            KB DRAM，延迟性能每年改进1.07（见表2-2）。处理器曲线假定1986年之前每年改进1.25,2000
            年之前每年改进1.52，2000~2005年每年改进1.20，2005~2010年处理器性能没有变化，见第1
            章的图 1-1}
\end{figure}

近来，高端处理器已经转向多核，与单核相比，进一步提高了带宽需求。事实上，总峰值
带宽基本上随核心个数的增大而增大。现代高端处理器（比如 \verb|Intel Core i7|）每个时钟周期可以
由每个核心生成两次数据存储器引用，i7有4个核心，时钟频率为\verb|3.2GHz|，除了大约128亿次
128 位指令引用的峰值措令要求之外，每秒最多还可生成256亿次64位数据存储器引用；总峰
值带宽为\verb|409.6 GB/s|！这一难以置信的高带宽是通过以下方法实现的：实现缓存的多端口和流
水线；利用多级缓存，为每个核心使用独立的第一级缓存，有时也使用独立的第二级缓存；在
第一级使用独立的指令与数据缓存。与其形成鲜明对比的是，DRAM主存储器的峰值带宽只有
它的 \verb|6%|（\verb|25 GB/s|）。

传统上，存储器层次结构的设计人员把重点放在优化存储器平均访问时间上，这一时间是由
缓存访问时间、缺失率和敏失代价决定的。但最近，功率已经成为设计人员的主要考虑事项。在
高端微处理器中，可能有\verb|10MB|或更多的片上缓存，大容量的第二或第三级缓存会消耗大量功率，
既可能是在没有操作时作为泄露功率（称为静态功率），也可能是在执行读取或写人时的活动功
率（称为动态功率），如2.3节所示。这一问题在 PMD的处理器中甚至更为突出，它的CPU的主
动性较低，功率预算可能降至\verb|1/20~1/50|。在此类情况下，缓存消耗的功率可能占总功耗的\verb|25%~50%|。
因此，需要更多地考虑性能与功率之间的权衡，在本章将对这两个因素进行研究。

\subsubsection{存储器层次结构基础：快速回顾}

前面曾经提到，存储器性能和处理器性能之间的差距越来越大，这一点越来越重要，所以
存储器层次结构的基础知识已经出现在计算机体系结构本科课程中，甚至还出现在操作系统和
编译器的相关课程中。因此，我们将首先快速回顾一下缓存及其操作。不过，本章的主要内容
是介绍一些用来应对处理器-存储器性能差距的更高级创新技术。

如果在缓存中找不到某一个字，就必须从层次结构的一个较低层级（可能是另一个缓存，
也可能是主存储器）中去提取这个字，并把它放在缓存中，然后才能继续。出于效率原因，会
一次移动多个字，称为块（或行），这样做还有另外一个原因：由于空间局域性原理，很可能马
上就会用到这些学。每个缓存块都包括一个标记，指明它与哪个存储器地址相对应。

在设计时需要作出一个非常重要的决策：哪些块（或行）可以放在缓存中。最常见的方案
是\verb|组相联|，其中组是指缓存中的一组块。一个块首先被映射到一个组上，然后可以将这个块放
到这个组中的任意位置。要查找一个块，首先将这个块的地址映射到这个组，然后再搜索这个
组（通常为并行搜索），以找到这个块。这个组是根据数据地址选择的：

\begin{equation}
    (Block address) MOD (Number of sets in cache)
\end{equation}

如果组中有n个块，则缓存的布局被称为\verb|n路组相联|。组相联的端点有其自己的名字。\verb|直接映射缓存|
每组中只有一个块（所以块总是放在同一个位置），全相联缓存只有一个组（所以块可以
放在任何地方）。

要缓存只读数据是一件很容易的事情，这是因为缓存和存储器中的副本是相同的。缓存写
入难一些；比如，缓存和存储器中的副本怎样才能保持一致呢？共有两种主要策略。一种是直
写（write-through）缓存，它会更新缓存中的项目，并直接写入主存储器，对其进行更新。另一
种是回写（write-back）缓存，仅更新缓存中的副本。在马上要替换这个块时，再将它复制回存
储器。这两种写入策略都使用了一种写缓冲区，将数据放入这个缓冲区之后，马上就可以进行
缓存操作，不需要等待将数据写入存储器的全部延迟时间。

为了衡量不同缓存组织方式的优劣，可以采用一个名为\verb|缺失率|的指标。\verb|缺失率|是指那些未
能找到预期目标的缓存访问所占的比例，即未找到目标的访问数目除以总访问数目。

为了深刻理解高缺失率的原因，从而更好地设计缓存，一种“3C”模式将所有这些缺失情
景分为以下三个简单的类别。

\begin{itemize}
    \item 强制（Compulsory）：对一个数据块的第一次访问，这个块不可能在缓存中，所以必须将
    这个块调人缓存中。即使拥有无限大的缓存，也会发生强制缺失。
    \item 容量（Capacity）：如果缓存不能包含程序运行期间所需要的全部块，就会因为有些块被
    放弃，之后再被调入，从而导致容量缺失（还有强制缺失）。
    \item 冲突（Contlict）：如果块放置策略不是全相联的，如果多个块映射到一个块的组中，对
    不同块的访问混杂在一起，一个块可能会被放弃，之后再被调人，从而发生冲突缺失（还
    有强制缺失和容量缺失）。
\end{itemize}

表B-4和图B-5是根据3C模型对缓存缺失进行分解后，各种缓存觖失的相对频率。在第3
章和第5章将会看到，多线程和多核都增加了缓存的复杂性，都加大了出现容量缺失的可能性，
而且还因为缓存刷新增加了第4个C\dash 一致性（Coherency）缺失，之所以进行缓存刷新是内
了保持多处理器中多个缓存的一致性；我们将在第5章考虑这些问题。

要小心，缺失率可能会因为多个原因而产生误导。因此，一些设计人员喜欢测量每条指令
的缺失次数，而不是每次存储器引用的缺失次数（缺失率）。这两者之间的关系如下：
\begin{equation}
    \frac{Misses}{Instruction} = \frac{Miss rate \times Memory accesses}{Instruction count} = Miss rate \times \frac{Memory accesses}{Instruction}
\end{equation}

（为了避免出现小数，通常用每1000条指令的缺失次数来表示这一指标，使其变为整数形式。）
这两种度量指标的问题在于它们都没有考虑缺失成本的因素。一种更好的度量指标是存储
器平均访问时间：
\begin{equation}    
    Average memory access time = Hit time + Miss rate \times Miss penalty
\end{equation}

其中，\emph{命中时间}是指在缓存中命中目标花费的时间，\emph{缺失代价}是从内存中替代块的时间（即缺
失成本）。存储器平均访问时间仍然是一个间接性能测量指标，尽管它比缺失率好一些，但不能
代替执行时间。在第3章，我们将会看到推理处理器可以在缺失期间执行其他指令，从而降低
实际缺失代价。多线程的使用（将在第3章介绍）也允许一个处理器容忍一些缺失，而不会被
强制转入空闲状态。我们稍后将会研究，为了利用这些容忍延迟的技术，就需要一些缓存，以
便在处理一次未完成缺失时为请求提供服务。

如果读者是新接触这一资料，或者说这一快速回顾浏览过快，可以参阅附录B。它更深
入地涵盖了同一介绍性内容，并包括一些真实计算机的缓存示例，还对其有效性进行了量化
评估。

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{imgs/sam.png}
	\caption{图2-3 访问时间通常会随着缓存大小和相联程度的增大而增加。这些数据来自 Tarjan、Thoziyoor 和
            Jouppi［2005］提出的CACTI模型6.5。这些数据假定采用40 nm 制程（介于Intel 最快速i7和第二
            快速 i7所用技术之间，与最快速的 ARM 嵌人处理器所用技术相同）、单一存储器组、块大小为
            64字节。由于对缓存布局所做的假定，以及在互连延迟（通常取决于正在访问的缓存块的大小）
            和标记检查与多工之间的复杂权衡，会得到一些有时看起来令人惊奇的结果，比如对于两路组相
            联的64KB缓存，其访问时间会低于直接映射。与此类似，当缓存大小增大时，八蹄组相联产生
            的结果也会导致一些不同寻常的行为。由于这些观察结构都严重依赖于技术和具体设计假定，所
            以诸如 CACTI之类的工具用于缩小权衡过程的搜索空间，而不是对权衡结果进行准确分析}
\end{figure}

附录B的B.3节给出了6种基本的缓存优化方法，我们在此概括性地看一下。这个附录给
出了这些优化方法收益的量化示例。我们还将对这些折中的功率因素进行简要评述。

\begin{enumerate}
    \item \textbf{增大块以降低缺失率}。这是降低缺失率的最简单方法，它利用了空间局域性，并增大了
    块的大小。使用较大的块可以减少强制缺失，但也增加了缺失代价。因为较大块减少了标记数
    目，所以它们可以略微降低静态功率。较大块还可能增大容量缺失或冲突缺失，特别是当缓存
    较小时尤为如此。选择合适的块大小是一项很复杂的权衡过程，具体取决于缓存的大小和缺失
    代价。
    \item \textbf{增大缓存以降低缺失率}。要减少容量缺失，一个显而易见的方法就是增大缓存容量。其
    缺点包括可能会延长较大缓存存储器的命中时间，增加成本和功率。较大的缓存会同时增大静
    态功率和动态功率。
    \item \textbf{提高相联程度以降低缺失率}。显然，提高相联程度可以减少冲突缺失。较大的相联程度
    是以延长命中时间为代价的。稍后将会看到，相联程度也会增大功耗。
    \item \textbf{采用多级缓存以降低缺失代价}。是加快缓存命中速度，以跟上处理器的高速时钟频率，
    还是加大缓存，以缩小处理器访问和主存储器访问之间的差距，这是一个艰难的决策。在原缓
    存和存储器之间加人另一级缓存可以降低这一决策的难度（见图2-3）。第一级缓存可以非常小，
    足以跟上快速时钟频率，而第二级（或第三级）缓存可以非常大，足以收集容纳许多本来要对
    主存储器进行的访问。为了减少第二级缓存中的缺失，需要采用更大的块、更大的容量和更高
    的相联程度。与一级总缓存相比，多级缓存的功率效率更高。如果用L1 和L.2分别指代第一级
    和第二级缓存，可以将平均存储器访问时间重新定义为：
    \begin{equation}
        {Hit time}_{L1} + {Miss rate}_{L1} \times ({Hit time}_{L2} + {Miss rate}_{L2} \times {Miss penalty}_{L2})
    \end{equation}
    \item \textbf{为读取缺失指定高于写入操作的优先级，以降低缺失率}。写缓冲区是实现这一优化的好
    地方。因为写缓冲区拥有在读取缺失时所需位置的更新值，所以写缓冲区存在隐患，即通过存
    储器进行先写后读的隐患。一种解决方案就是在读取缺失时检查写缓冲区的内容。如果没有冲
    突，如果存储器系统可用，则在写入操作之前发送读取请求会降低缺失代价。大多数处理器为
    读取指定的优先级要高于写入操作。这种选择对功耗几乎平没有什么影响。
    \item \textbf{在缓存索引期间避免地址转换，以缩短命中时间}。缓存必须妥善应对从处理器虚拟地址
    到访问存储器的物理地址之间的转换。（虚拟存储器在2.4节和B.4节介绍。）一种常见的优化方
    法是使用页偏移地址（虚拟地址和物理地址中的相同部分）来索引缓存，如附录B所述。这种
    虚拟素引/物理标记方法增加了某些系统复杂度以及（或者）对\verb|L1|缓存大小与结构的限制，但
    从关键路径消除转换旁视缓冲区（\verb|TLB|）访问这一优点抵得过这些敏点。
\end{enumerate}

注意，上述6种优化方法中，每一种都有自己潜在的弱点，可能会导致存储器平均访问时
间的延长，而不是缩短。

本章后续部分假定读者熟悉上述材料及附录B 中的详细内容。在“融会贯通”一节中，我
们将研究 \verb|Intel Core i7| 的存储器层次结构，\verb|Intel Core i7|是一种为高端服务器设计的微处理器，
还将研究 \verb|Arm Cortex-A8| 的存储器层次结构，\verb|Arm Cortex-A8|是一种 PMD设计的处理器，它
是 \verb|Apple iPad|和几种高端智能手机所用处理器的基础。由于这些处理器是为不同计算机应用而
设计的，所以每一类采用的方法都有明显的不同。与桌面计算机中使用的 Intel处理器相比，服
务器中使用的高端处理器拥有更多的核心和更大的缓存，但这些处理器的体系结构是类似的。
差异性主要体现在性能和工作负载的性质上，桌面计算机在某一时刻只有一位用户，主要在操
作系统的顶层运行一个程序，而服务器计算机可能会同时拥有数百位运行几十个应用程序的用
户。由于这些工作负载的差异性，桌面计算机通常更多地考虑存储器层次结构带来的平均延迟，
而服务器计算机还要考虑存储器带宽。即使同为桌面计算机，也是品种繁多，既有低端的上网
本（其处理器有所缩减，类似于高端 PMD中使用的处理器），也有高端桌面计算机，其处理器
包括多个核心，组成方式与低端服务器类似。

与之相对，PMID不但只为一位用户提供服务，而且其操作系统通常要更小一些，通常很少
采用多任务工作方式（同时运行几个应用程序），应用程序也要更简单一些。另外，PMD通常
采用闪存而不是磁盘，大多要同时考虑性能和能耗，能耗决定着电池寿命。

\section{缓存性能的10种高级优化方法}
上面的存储器平均访问时间公式提供了三种缓存优化度量：命中时间、缺失率和缺失代价。
根据最近的发展趋势，我们向这个列表中添加了缓存带宽和功耗两个度量。根据这些度量，可
以将我们研究的10种高级缓存优化方法分为以下5类。

\begin{enumerate}
    \item \textbf{缩短命中时间}。小而简单的第一级缓存和路预测。这两种技术通常还都能降低功耗。
    \item \textbf{增加缓存带宽}。流水化缓存、多组缓存和无阻塞缓存。这些技术对功耗具有不确定影响。
    \item \textbf{降低缺失代价}。关键字优化，合并写缓冲区。这两种优化方法对功率的影响很小。
    \item \textbf{降低缺失率}。编译器优化。显然，缩短编译时间肯定可以降低功耗。
    \item \textbf{通过并行降低缺失代价或缺失率}。硬件预取和编译器预取。这些优化方法通常会增加功
    耗，主要是因为提前取出了未用到的数据。
\end{enumerate}

一般来说，在采用这些技术方法时，硬件复杂度会增加。另外，这些优化技术中有几种需
要采用高级编译器技术。后面将在表2-1中总结这10种技术的实现复杂度和性能优势。对其中
比较简单的优化方法仅作简单介绍，而对其他技术将给出更多描述。

\subsection{第一种优化：小而简单的第一级缓存，用以缩短命中时间、降低功率}
提高时钟频率和降低功率的双重压力都推动了对第一级缓存大小的限制。与此类似，使用
较低级别的相联度，也可以缩短命中时间、降低功率，不过这种权衡要比限制大小涉及的权衡
更复杂一些。

缓存命中过程中的关键计时路径由3个步骤组成：使用地址中的索引确定标记存储器的地
址，将读取的标签值与地址进行比较。接下来，如果缓存为组相联缓存，则设置多路转换器以
选择正确的数据项。直接映射的缓存可以将标记核对与数据传输重叠起来，有效缩短命中时间。
此外，在采用低相联度时，由于减少了必须访问的缓存行，所以通常还可以降低功率。

尽管在各代新型微处理器中，片上缓存的总数已经大幅增加，但由于大容量L1缓存带来的
时钟频率影响，所以L1缓存大小最近的涨幅很小，甚至根本没有增长。选择相联度时的另一个
考虑因素是消除地址别名的可能性；我们稍后对此进行讨论。

使用 \verb|CAD| 工具，可以在制造芯片之前判断各项选择对命中时间和功耗的影响。\verb|CACTI| 程
序可以在 \verb|CMOS| 微处理器上估算各种缓存结构的访问时间和能耗，它的详细程度在 \verb|CAD| 工具
中排在前十名之内。对于一个给定的最小工艺尺寸，\verb|CACTI| 估算在不同缓存大小、相联度、读/
写端口数目和更复杂参数条件下的缓存命中时间。根据缓存大小的不同，对于模型建议的这些
参数，直接映射的命中时间略快于两路组相联，两路组相联四路的 1.2倍，四路组相联是八
路的1.4倍。当然，这些估计值受技术及缓存大小的影响。

\begin{exercise}
    利用图2-3和附录B中表B-4中的数据，判断32KB四路组相联L1缓存的存储器
    访问时间是否快于32 KB两路组相联L1缓存器。假定L.2 的缺失代价是快速L1
    缓存访问时间的15倍。忽略超出L.2 之外的缺失。哪种缓存的存储器平均缓存时
    间较短？
\end{exercise}

\begin{solution}
    设两路组相联缓存的访问时间为1。则，对于两种缓存：

    存储器平均访问时间需哪二命中时间＋觖失率 ×缺失代价
    =1+0.038×15=1.57

    对于四路缓存，访问速度是它的1.4倍。缺失代价占用的时间为 15/1.4=10.1。为简
    单起见，设其为10：

    存储器平均访问时间四命中时间邮路×1.4+缺失率 ×缺失代价
    =1.4+0.037×10-=1.77

    显然，采用较高的相联度看起来是一种糟糕的权衡选择；不过，由于现代处理器
    中的缓存访问通常都实现了流水化，所以很难评估对时钟周期时间的确实影响。
\end{solution}

如图2-4所示，在选择缓存大小和相联度时，能耗也是一个考虑因素。在\verb|128KB|或\verb|256 KB|
缓存中，当从直接映射变到两路组相联时，高相联度的能量消耗从大于2变到可以忽略。

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{imgs/sam.png}
	\caption{每次读取操作的能耗随缓存大小、相联度的增加而增加。和图2-3一样，使用CACTI对相同技术
            参数进行建模。八路组相联缓存的代价之所以很高，是因为并行读取8个标签及相应数据的成本
            造成的}
\end{figure}

在最近的设计中，有三种其他因素导致了在第一级缓存中使用较高的相联度。第一，许多
处理器在访问缓存时至少需要两个时钟周期，因此命中时间较长可能不会产生太过严重的影响。
第二，将 TLB排除在关键路径之外（TLB带来的延迟可能要大于高相联度导致的延迟），几乎
所有L1缓存都应当是变址寻址的。这就将缓存的大小限制为页大小与相联度的乘积，这是因为
只有页内的位才能用于变址。在完成地址转换之前对缓存进行变址的问题还有其他一些解决方
案，但提高相联度是最具吸引力的一种，它还有其他一些好处。第三，在引入多线程之后（参
见第3章），冲突缺失会增加，从而使提高相联度更具吸引力。

\subsection{第二种优化：采用路预测以缩短命中时间}

这是另外一种可以减少冲突缺失，同时又能保持直接映射缓存命中速度的方法。在路预测
技术中，缓存中另外保存了一些位，用于预测下一次缓存访问组中的路或块。这种预测意味着
尽早设定多工选择器，以选择所需要的块，在与缓存数据读取并行的时钟周期内，只执行一次
标签比较。如果缺失，则会在下一个时钟周期中查看其他块，以找出匹配项。

在一个缓存的每个块中都添加块预测位。根据这些位选定要在下一次缓存访问中优先尝试
哪些块。如果预测正确，则缓存访问延迟就等于这一快速命中时间。如果预测错误，则尝试其
他块，改变路预测器，延迟会增加一个时钟周期。模拟表明，对于一个两路组相联缓存，组预
测准确度超过90\%；对于四路组相联缓存，超过80\%，对I-缓存的准确度优于对D-缓存的准确
度。如果路预测能够至少快10\%（这是非常可能的），路预测方法可以缩短两路组相联缓存的存
储器平均访问时间。路预测最早于20世纪90年代中期应用在 \verb|MIPS R10000| 中。这在使用两路
组相联缓存的处理器中很常见，在使用四路组相联缓存的ARM \verb|Cortex-A8| 中也采用了这一技术。
对于非常快速的处理器，要实现一个周期的时延是非常具有挑战性的，而这对于降低路预测代
价非常关键。

还有一种扩展形式的路预测，它使用路预测位来判断实际访问的缓存块，可以用来降低功
耗（路预测位基本上就是附加地址位）；这种方法也可称为路选择，当路预测正确时，它可以节
省功率，但在路预测错误时则会显著增加时间，这是因为需要重复进行访问，而不仅是重复标
记匹配与选择过程。这种优化方法只有在低功率处理器中才可能有意义。Inoue、Ishihara 和
Murakasi ［1999］根据 SPEC95基准测试进行估算，对于四路组相联缓存使用路选择方法，可以
使1-缓存的平均访问速度提高1.04倍，D-缓存提高1.13倍，与正常的四路组相联缓存相比，
I-缓存的平均缓存功耗降为原来的 0.28，D-缓存降为0.35。路选择方法的一个重要缺点就是它
增大了实现缓存访问流水化的难度。

\begin{exercise}
    假定在一个普通四路组相联实现中，D-缓存访问次数是1-缓存访问次数的一半，
    1-缓存和 D-缓存分别占该处理器功耗的25\%~15\%。根据上述研究的估计值，判
    断路选择方法是否提高了每瓦功耗的性能。
\end{exercise}

\begin{solution}
    对于1-缓存，节省的功率为总功率的25×0.28-0.07，对于D-缓存，为1S ×0.35=0.05，
    共节省0.12。路预测版本需要的功率为标准四路缓存的0.88。缓存访问时间的增加
    等于1-缓存平均访问时间的增加量加上D-缓存访问时间增加量的一半，即1.04+0.5
    x0.13=1.11倍。这一结果意味着路选择的性能是标准四路缓存的0.90。因此，路
    选择方法略微提高了每焦功耗的性能，比值为 0.90/0.88=1.02。这一优化方法最适
    合用在看重功率胜于看重性能的情景。
\end{solution}

\subsection{第三种优化：实现缓存访问的流水化，以提高缓存带宽}

这种优化方法就是实现缓存访问的流水化，使第一级缓存命中的实际延迟可以分散到多个
时钟周期，从而缩短时钟周期时间、提高带宽，但会减缓命中速度。例如，对于20世纪90年
代中期的 Intel Pentium处理器，指令缓存访问的流水线需要1个时钟周期，对于从 20世纪90
年代中期到2000年的 Pentium Pro 到 Pentium II，需要2个时钟周期，对于2000年出现的 Pentium4
和现在的 Intel Core 17，需要4个时钟周期。这一变化增加了流水线的段数，增加了预测错误分
支的代价，延长了从发出载入指令到使用数据之间的时钟周期数（参见第3章），但的确更便于
采用高相联度的缓存。

\subsection{第四种优化：采用无阻塞缓存，以提高缓存带宽}

对于允许乱序执行（在第3章讨论）的流水化计算机，其处理器不必因为一次数据缓存觖失
而停顿。例如，在等待数据缓存返回缺失数据时，处理器可以继续从指令缓存中提取指令。无阻
塞缓存（或称为自由查询缓存）允许数据缓存在一次缺失期间继续提供缓存命中，进一步加强
了这种方案的潜在优势。这种“缺失时仍然命中”优化方法在缺失期间非常有用，它不会在此期
间忽略处理器的请求，从而降低了实际缺失代价。还有一种精巧而复杂的选项：如果能够重叠多
个缺失，那缓存就能进一步降低实际缺失代价，这种选项被称为“多次缺失时仍然命中"（hit under
multiple miss） 或者“缺失时缺失”（miss under miss）优化方法。只有当存储器系统可以为多次
缺失提供服务时，第二种选项才有好处，大多数高性能处理器（比如 Intel Core i7）通常都支持
这两种优化方法，而低端处理器，比如 ARMA8，仅在L.2提供了有限的无阻塞支持。

为了研究无阻塞缓存在降低缓存缺失代价方面的有效性，Farkas 和Jouppi ［1994］做了一项
研究：假定8KB缓存的缺失代价为14个周期；在缺失情况下允许一次命中时，他们都观测到
实际缺失代价的降低，对于 SPECINT92 基准测试降低20%，对于 SPECFP92 基准测试降低30%。
Li、Chen、Brockman 和 Jouppi2011］最近对这一研究内容进行了更新，他们采用多级缓存，
根据最近的技术情况对缺失代价作出了假设，并采用了规模更大、要求更严格的 SPEC2006 基准
测试。这项研究采用一种基于 Intel i7单核心的模型（见2.6节）运行 SPEC2006基准测试。图2-5
显示了一次缺失时允许1、2、64次命中时，数据缓存访问延迟的降低情况。自从研究早期就采用
了大缓存容量、增加了L.3缓存，所以采用“缺失时命中”的好处显得不够明显，SPECINT2006
基准测试显示缓存延迟平均降低大约9%，而采用 SPECFP2006 基准测试结果大约为12.5%。

例题
解答
对于浮点程序来说，主数据缓存的两路组相联和一次缺失时仍然命中，哪个更重
要一些？对整数程序来说呢？假定32KB数据缓存的平均缺失率如下：对于采用
直接映射缓存的浮点程序为5.2%，对于采用两路组相联缓存的浮点程序为4.9%，
采用直接映射缓存的整数程序为 3.5%，对于采用两路组相联缓存的整数程序为
3.2%。假定1.2的缺失代价为10个周期，L2的缺失率和缺失代价相同。

对于浮点程序，存储器平均停顿时间为
缺失率 D ×缺失代价=5.2% × 10=0.52
缺失率两略 ×缺失代价=4.9%×10=0.49
两路相联缓存的访问延迟（包括停顿）为0.49/0.52，或者为直接映射缓存的94%。
图2-5的标题表明允许一次缺失时仍然命中，可以将浮点程序的平均数据缓存访问
延迟缩短为一次阻塞缓存的87.5%。因此，对于浮点程序而言，支持一次缺失时仍
然命中一次的直接映射数据缓存，在性能上要优于在缺失时阻塞的两路相联缓存。

对于整数程序，计算如下：
缺失率 DM ×缺失代价=3.5% ×10=0.35
缺失率两路× 缺失代价=3.2%×10=0.32
两种组相联缓存的数据缓存访问延迟为 0.32/0.35，或直接映射缓存的91%，
而在一次缺失时允许一次命中可以将访问延迟降低9%，使两种选择的性能大
体相当。

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.7\textwidth]{imgs/sam.png}
	\caption{用9个 SPECINT（左）和9个 SPECFP（右）基准测试评估在1、2或64次缓存缺
            失时仍然允许命中情况下非阻塞缓存的效果。这个数据存储器系统以 Intel i7建模，它
            包括32KBL.1缓存，访问延迟为4个周期。I.2缓存（与指令共享）为256KB，访问
            延迟为10个时钟周期。L3为2MB，访问延迟为35个周期。所有访问都是八路组相
            联，大小为64字节块大小。在缺失时允许一次命中可以使整数基准测试的缺失代价
            降低9\%，浮点基准测试降低12.5\%。允许第二次命中会将这些结果提高 10\%~16\%，
            得到64个没有多少额外改进的结果}
\end{figure}

对非阻塞缓存进行性能评估时，真正的难度在于一次缓存缺失不一定会使处理器停顿。在
这种情况下，很难判断一次缺失造成的影响，因此也就难以计算存储器平均访问时间。实际缺
失代价并不等于这些缺失时间之和，而是处理器的非重叠停顿时间。非阻塞缓存的优势评估非
常复杂，因为它取决于存在多次缺失时的缺失代价、存储器引用模式以及处理器在有未处理缺
失时能够执行多少条指令。

通常，对于一个能够在L.2缓存中命中的L.1 数据缓存缺失，乱序处理器能够隐藏其大部分
缺失代价，但不能隐藏更低层次缓存敏失的大部分代价。而在决定支持多少个未处理缺失时，
需要考虑多种因素，如下所述。

\begin{itemize}
    \item 缺失流中的时间与空间局域性，它决定了一次缺失是否会触发对低级缓存或对存储器的
    新访问操作。
    \item 对访问请求作出回应的存储器或缓存的带宽。
    \item 为了允许在最低级别的缓存中出现更多的未处理缺失（在这一级别的缺失时间是最长
    的），需要在较高级别上支持至少数目相等的缺失，这是因为这些失必须在最高级别缓
    存上启动。
    \item 存储器系统的延迟。
\end{itemize}

下面这个简化示例表明了这一关键思想。
例题
假定主存储器的访问时间为36ns，存储器系统的持续传输速率为16GB/s。设块大
小为64字节，如果在给定请求流的情况下能够保持峰值带宽，而且访问永远不会
冲突，则需要支持的最大未完成缺失数目为多少？如果一次引用与前4次引用发
生冲突的概率为 50%，估计最大未完成引用数目。为简单起见，忽略缺失之间的
时间。

解答
在第一种情况下，假定我们可以保持峰值带宽，存储器系统可以支持每秒（16x
10）%64-250百万次引用。由于每次引用耗时36ns，因此可以支持250 ×10°×36x
10--9次引用。如果发生冲突的概率大于0，由于我们不能在这些引用时开始工
作，所以需要更多的未完成引用；存储器系统需要的独立引用数应当更多而不是
更少！为了估计这一数目，可以假定有一半存储器引用不需要发送到存储器。这
就意味着必须能够支持两倍数量的未完成引用，即18次。

Li、Chen、Brockman 和 Jouppi在研究中发现：对于在缺失时允许1次命中的情况，整数程
序的CPI 减少大约7%，而在允许64 次命中时，减少大约12.7%。对于浮点程序，允许1次命
中时减少12.7%，64 次时减少17.8%。这些减少数值与图2-5中所示数据缓存访问延迟的减少
值非常接近。

\subsection{第五种优化：采用多种缓存以提高缓存带宽}
我们可以将缓存划分为几个相互独立、支持同时访问的缓存组，而不是将它们看作一个整
体。分组方式最初用于提高主存储器的性能，现在也用于 DRAM 芯片和缓存中。Arm Cortex-A8
在其L2缓存中支持1至4个缓存组；Intel Core i7 的L1 中有4个组（每个时钟周期内可以支持
2次存储器访问），L2有8个组。

显然，当访问请求很自然地分布在缓存组之间时，分组方式的效果最佳，所以将地址映射
到缓存组的方式影响着存储器系统的行为。一种简单有效的映射方式是将缓存块地址按顺序分
散在这些缓存组中，这种方式称为顺序交错。例如，如果有4个缓存组，0号缓存组中的所有
缓存块地址对4求模后余0，1号缓存组中的所有缓存块地址对4求模后余1，以此类推。图2-6
显示了这种交错方式。采用分组方式还可以降低缓存和 DRAM 中的功耗。


图2-6 使用块寻址的四路交错缓存组。假定每个块64个字节，这些地址需要分别
乘以64 才能实现字节寻址

\subsection{第六种优化：关键字优先和提前重启动以降低缺失代价}
这一技术的事实基础是人们观测到处理器在某一时刻通常仅需要缓存块的一个字。这一策
略显得“不够耐心”：无须等待完成整个块的载入过程，一旦载人被请求字之后，立即将其发出，
然后就重启处理器。下面是两个特定策略。

\begin{itemize}
    \item 关键字优先：首先从存储器中请求缺失的字，在其到达缓存之后立即发给处理器；使处
    理器能够在载入块中其他字时继续执行。
    \item 提前重启动：以正常顺序提取字，但只要块中的被请求字到达缓存，就立即将其发送给
    处理器，让处理器继续执行。
\end{itemize}

通常，只有在采用大型缓存块的设计中，这些技术才有用武之地，如果缓存块很小，它们
带来的好处是很低的。注意，在载人某个块中的其余内容时，缓存通常可以继续满足对其他块
的访问请求。

不过，根据空间局域性原理，下一次引用很可能就会指向这个块的其余内容。和非阻塞
缓存一样，其缺失代价的计算也不是非常容易。在采用关键字优先策略时，如果存在第二次
请求，实际缺失代价等于从本次引用开始到第二部分内容到达之前的非重叠时间。关键字优
先策略和提前重启动策略的好处取决于块的大小以及在尚未获取某部分内容时又出现另一次
访问的机率。

\subsection{第七种优化：合并写缓冲区以降低缺失代价}

因为所有存储内容都必须发送到层次结构的下一层级，所以直写缓存依赖于写缓冲区。
即使是写回缓存，在替代一一个块时也会使用一个简单的缓冲区。如果写缓冲区为空，则数据
和整个地址被写到缓冲区中，从处理器的角度来看，写人操作已经完成；在写缓冲区准备将
字写到存储器时，处理器继续自己的工作。如果缓冲区中包含其他经过修改的块，则检查它
们的地址，看看新数据的地址是否与写缓冲区中有效项目的地址匹配。如果匹配，则将新数
据与这个项目合并在一起。这种优化方法称为写合并。Intel Core i7 和其他许多处理器都采用
了写合并方法。

如果缓冲区已满，而且没有匹配地址，则缓存（和处理器）必须等待，直到缓冲区中拥有
空白项目为止。由于多字写人的速度通常快于每次只写人一个字的写人操作，所以这种优化方
法可以更高效地使用存储器。Skadron 和Clark［1997］发现：即使在一个合并四项的写缓冲区中，
所生成的停顿也会导致5%~10%的性能损失。

这种优化方式还会减少因为写缓冲区已满而导致的停顿。图2-7显示了一个写缓冲区在采
用和不采用写合并时的情况。假定这个写缓冲区中有四项，每一项有4个 64位的字。在来用
这一优化方法时，图中的4个字可以完全合并，放在写缓冲区的一个项目中，而在不采用这一
优化方法时，对写缓冲区的连续地址执行4次存储操作，将整个缓冲区填满，每个项目中保存
一个字。

注意，输人/输出设备寄存器经常被映射到物理地址空间。由于1/O寄存器是分享的，不能
像存储器中的字数组那样操作，所以这些1/O地址不能允许写合并。例如，它们可能要求为每
个 1O 寄存器提供一个地址和一个数据字，而不能只提供一个地址进行多字写人。为了实现这
些副作用，通常由缓存将这些页面进行标记，表明其需要采用非合并直写方式。

图2-7 为了说明写合并过程，上面的写缓冲区未采用该技术，下面的写缓冲区采用了这一技术。在进行
合并时，4次写人内容被合并到一个缓冲区项目中；而未进行合并时，4次写人操作就填满了整个
缓冲区，每个项目的四分之三被浪费。这个缓冲区有四个项目，每一项保存4个64位字。每个项
目的地址位于左侧，有效位（V）指明这个项目的下面8个连续字节是否被占用（未采用写合并
时，图中上半部分右侧的字只会用于同时写多个字的指令。）

\subsection{第八种优化：采用编译器优化以降低缺失率}
前面介绍的技术都需要改变硬件。下面这种技术可以在不做任何硬件改变的情况下降低续
失率。

这种神奇的降低效果来自软件优化—硬件设计人员最喜爱的解决方案！处理器与主存储
器之间的性能差距越拉越大，已经促使编译器编写人员深入研究存储器的层次结构，以了解能
否通过编译时间优化来提高性能。同样，研究内容包括两个方面：指令缺失性能改进和数据缺
失性能改进。下面给出的优化技术在很多现代编译器中均有应用。

1. 循环交换
一些程序中存在嵌套循环，它们会以非连续顺序访问存储器中的数据。只要交换一下这些
循环的嵌套顺序，就可能使程序代码按照数据的存储顺序来访问它们。如果缓存中无法容纳这
些数组，这一技术可以通过提高空间局域性来减少缺失；通过重新排序，可以使缓存块中的数
据在被替换之前，得到最大限度的充分利用。例如，设x是一个大小为［5000,100］的两维数据，
其分配方式使得×［1，J］和x［1，J+2］相邻（由于这个数组是按行进行排列的，所以我们说这种顺
序是以行为主的），以下两段代码说明可以怎样来优化访问过程：

\begin{verbatim}
    /*优化之前*/
    for （j= 0; jc 100;j = j+1）
    for （i= 0; i≤ 5000; i = i+1）
    ×［i1［3］=2*x［E］；
    /*优化之后*/
    for （i= 0; i≤ 5000; i = i+1）
    for （j= 0; j< 100; j = j+1）
    x［i［3=2*x［1［3］；
\end{verbatim}
原代码以100个字的步幅跳跃式浏览存储器，而修改后的版本在访问了一个缓存块中的所
有字之后才进人下一个块。这一优化方法提高了缓存性能，却没有影响到所执行的指令数目。
2.分块

这一优化方法通过提高时间局域性来减少缺失。这一次还是要处理多个数组，有的数组是
按行来访问的，有的是按列来访问的。由于在每个循环选代中都用到了行与列，所以按行或按
列来存储数组并不能解决问题（按行存储称为行主序，按列存储称为列主序）。这种正交访问方
式意味着在进行循环交换之类的转换操作之后，仍然有很大的改进空间。

分块算法不是对一个数组的整行或整列进行操作，而是对其子矩阵（或称块）进行操作。
其目的是在缓存中载人的数据被替换之前，在最大限度上利用它。下面这段执行矩阵乘法的示
例可以帮助理解这一优化方法的动机：

\begin{verbatim}
    /*优化之前*/
    for （i= 0; i< N;i= i+1）
    for （j= 0; j<N;j= j+1）
    ｛r= O；
    for （k = O; k< N; k- k+1）
    r= r+ y［i］［K］*Z［K［S］；
    x［i］［j］ = r；
    ｝；
\end{verbatim}
两个内层循环读取乙的所有NxN个元素，重复读取y中一行的同一组 N个元素，再写人X
的一行 N个元素。图 2-8是访问这三个数组的一个快照。深色阴影区域表示最近的访问，浅色
阴影区域表示较早的访问，白色表示还没有进行访问。

容量缺失的数目显然取决于N和缓存的大小。如果它能容纳所有这3个N×N矩阵，只要没
有缓存冲突，就一切正常。如果缓存可以容纳一个 NxN矩阵和包含N个元素的一行，则至少y
的第i行和数组可以停留在缓存中。如果缓存的容量更小，那可能对于x和z都会发生觖失。
在最差情况下，进行N次操作可能需要从存储器中读取 2N’+N个字。

图 2-8
三个数组 ×、y和z的快照，其中 N=6，1=1。对数组元素访问的前后时间用阴影表示：白色表
示还没有被访问过，浅色表示较早的访问，深色表示最近的访问。与图2-9相对照，为了计算
x的新元素，会重复读取y和z的元素。在行或列的旁边显示了用于访问这些数组的变量i、了
和k


为了确保正在访问的元素能够放在缓存中，对原代码进行了修改，改为计算一个大小为B
xB的子矩阵。两个内层循环现在以大小为B的步长进行计算，而不是以x和z的完整长度为步
长。B被称为分块因子。（假定x被初始化为0。）
\begin{verbatim}
    /*优化之后*/
    far （jj= 0;j
    《kk = 0;kk <N; kk = kk+B）
    for （i= O; i< N;i- i+l）
    for （j= jj: j < nin（jj+B,N）； j = j+1）
    for （k = kk; k ≤ min（kk+B,N）； k - k +1）
    r=r+ y［i］［k*z［K 3］；
    x［i［j］ = x［i］［j］ + r；
    ｝；
\end{verbatim}
图2-9展示了使用分块方法对三个数组的访问。仅观察容量缺失，从存储器中访问的总字
数为2N/B+N。这一总数的改善因素大约为B。由于y获益于空间局域性，2获益于时间局域性，
所以分层方法综合利用了空间局域性和时间局域性。

尽管我们的目标是降低缓存缺失，但分块方法也可用于帮助寄存器分配。通过设定一个
较小的分块大小，使这个块能够保存在寄存器中，可以在最大程度上降低程序中的载入与存
储数量。

在4.8节将会看到，对于以矩阵主要数据结构的程序，要在基于缓存的处理器上获得出色
性能，缓存分块方法是必不可少的。

图 2-9

当B=3时，对数组x、y和z的访问历史。注意，与图2-8相比，访问的元素数目较少

\subsection{第九种优化：对指令和数据进行硬件预取，以降低缺失代价或缺失率}
通过执行过程与存储器访问的重叠，无阻塞缓存有效地降低缺失代价。而另一种方法是在
处理器请求项目之前，预先提取它们。指令和数据都可以预先提取，既可以直接放在缓存中，
也可以放在一个访问速度快于主存储器的外部缓冲区中。

擔令预取经常在缀存外部的硬件中完成。通常，处理器在一次映失时提取两个块：被请求
块和下一个相邻块。被请求块放在它返回时的指令缓存中，预取块被放在指令流缓冲区中。如
果被请求块当前存在于指令流缓冲区中，则原缓存请求被取消，从流缓冲区中读取这个块，并
发出下一条预取请求。

类似方法可应用于数据访问［Jouppi 1990］。Palacharla 和 Kessler ［1994］研究了一组科学程序，
并考查了多个可能处理指令或数据的流缓冲区。他们发现，对于一个具有两个64 KB 四路组相
联缓存的处理器（一个用于缓存指令，另一个用于缓存数据），八个流缓冲区可以捕获其所有缺
失的50%~70%。

Intel core i7 支持利用硬件预先提取到L1和L2中，最常见的预取情况是访问下一行。一些
较早的 Intel处理器使用更主动的硬件预取，但会导致某些应用程序的性能降低，一些高级用户
会因此而关闭这一功能。

图2-10显示了在启用硬件预取时，部分SPEC2000 程序的整体性能改进。注意，这一数字
仅包含12个整数程序中的2个，而大多数 SPEC浮点程序则都包含在内。

图 2-10
在 intel Pentium 4上启用硬件预取之后，12个 SPECint2000 基准测试中的2个测试、14个
SPEC p2000 基准测试中的9个测试获得的加速比。图中仅给出从预取中获益最多的程序，对
于图中未给出的15个 SPEC基准测试，预取加速比低于15%［Singhal 2004］
预取操作需要利用未被充分利用的存储器带宽，但如果它干扰了迫切需要的缺失内容，反
而可能会实际降低性能。在编译器的帮助下，可以减少无用预取。当预取操作正常执行时，它
对功率的影响可以怒略。如果未用到预取数据或者替换了有用数据，预取操作会对功率产生非
常负面的影响。

\subsection{第十种优化：用编译器控制预取，以降低缺失代价或缺失率}
作为硬件预取的替代方法，可以在处理器需要某一数据之前，由编泽器插入请求该数据的
预取指令。共有以下两种预取。

\begin{itemize}
    \item 寄存語预取将数据值载入到一个寄存器中。
    \item 缓存预取仅将数据载入到缓存中，而不是寄存器中。
\end{itemize}

这两种预取都可能是故障性的，也都可能是非故障性的；也就是说，其地址可能会导致虚
拟地址错误异常和保护冲突异常，也可能不会导致。利用这一术语，正常的载入指令可能被看
作是“故障性寄存器预取指令”。非故障性预取只是在通常导致异常时转换为空操作，而这正是
我们想要的结果。

最有效的预取对程序来说是“语义透明的”：它不会改变寄存器和存储器的内容，也不会导
致虚拟存储器错误。今天的大多数处理器都提供非故障性缓存预取。本节假定非故障性缓存预
取，也称为非绑定预取。

只有当处理器在预取数据时能够继续工作的情况下，预取才有意义；也就是说，缓存在等
待返回预取数据时不会停顿，而是继续提供指令和数据。可以想到，这些计算机的数据缓存通
常是非阻塞性的。

与硬件控制的预取操作类似，这里的目标也是将执行过程与数据预取过程重叠起来。循环
是重要的预取优化目标，因为它们本身很适于进行预取优化。如果缺失代价很小，编译器只需
要将循环展开一两次，在执行时调度这些预取操作。如果缺失代价很大，它会使用软件流水线
（参见附录H）或者将循环展开多次，以预先提取数据，供后续迭代使用。

不过，发出预取指令会导致指令开销，所以编译器必须非常小心地确保这些开销不会超过
所得到的好处。如果程序能够将注意力放在那些可能导致缓存缺失的引用上，就可以避免不必
要的预取操作，局时明显缩短存储器平均访问时间。

例题
解答
对于以下代码，判断哪些访问可能导致数据缓存缺失。然后，插入预取指令，以减
少缺失。最后，计算所执行的预取指令数目和通过预取避免的缺失数。假设有一个
8KB 直接映射的数据缓存，块大小为16字节，它是一个执行写人分派的写回缓存。
a 和b是双精度浮点数组，所以它们的元素长8个字节。a有3行、100列，b有101
行、3列。另外假定在程序开始时，这些数据没有在缓存中。
\begin{verbatim}
    for （i = 0; 1<3;i= i+1）
    for （j= 0; j< 100; j= j+1）
    a［i［j］- b［jlCo］*b［j+1［0］：
\end{verbatim}
编译器首先判断哪些访问可能导致缓存缺失，否则可能会为那些能够命中的数据发
出预取指令，白白浪费时间。a的元素是以它们在存储器中的存储顺序写人的，所
以a可以受益于空间局域性：j的偶数值会缺失，奇数值会命中。由于a有3行、
100列，所以对它的访问将会导致3×（100/2）=150次缺失。

数组b不会从空间局域性中获益，因为对它的访问不是按照存储顺序执行的。数组
b 可以从时间局域性中获得双重受益：每次对1进行迭代时会访问相同元素，每次
对j进行迭代时使用的b元素值与上一次迭代相同。忽略可能存在的冲突缺失，由
b导致的缺失在1=0时访问bLj+1］［0］出现，还有就是在j=0时首次访问b［jE0］出
现。当1=0时，j从0增至99，所以对b的访问将导致100+1=101次缺失。

因此，这次循环将会出现的数据缓存缺失大约包括a的150次和b的101次，也就
是251 次缺失。

为了简化优化过程，我们不用费心为循环中的第一次访问进行预取。这些内容可能
已经放在缓存中了，或者我们需要为a或b的前几个元素承担缺失代价。在到达循
环末尾时，预取操作会尝试提前提取超出a末端之外的内容（a［i］［100］

ari］［106］）和b末端之外的内容（bC101］［0］..b［107］［0］），我们并不需要费心来禁
用这些预取。如果这是一些故障性预取，那我们也许不能承担如此之大的开销。我
们假定缺失代价非常大，需要至少提前（比如）7次迭代来开始预取。（换句话说，
我们假定在第8次迭代之前进行预取不会得到任何好处。）以下代码中加下划线的
部分，是为了添加预取优化而对前面代码所做的修改。

\begin{verbatim}
    for （j-0: j<100:J=1+1）1
    Prefetch（bli+71O］）：
    /* b（J.0） for ! iterations later */
    Prefetch（alO1 li+71）i
    1 a!0.j） fon J iterations_later */
    a01C1-b［j110］ *b［j+11［01jl：
    for （i = 1;i≤ 3; i = i+1）
    for （i - o: j < 100: J-j+1）｛
    Prefetch（aJd］ Ti+7］）；
    a1.i tor.t/ iterations
    ［iJLjl = bljlCoJ * bLj+lJ［oJ；
\end{verbatim}
这一经过修订的代码预取 a［1C7］至 a［1］［99］和b［7］［0］至 b［100］［0］，将非预取敏失
的数目减至：
\begin{itemize}
    \item 第一次循环中访问元素 b［0］［0］，b［1J［0］，，b［6］［0］时的7次缺失
    \item 第一次循环中访问元素 aC0］［0］，a［0］［1］，，a［036］时的4次缺失（［7/2］）
\end{itemize}
（利用空间局域性将缺失数减少为每16字节缓存块一次缺失）
\begin{itemize}
    \item 第二次循环中访问元素 aC1］［0］，a［1］［1］，，a［11［6］的4次缺失（［7/2］）
    \item 第二次循环中访问元素 a［23［0］，a［2］［1］，，a［2J［6］的4次鍊失（［7/2］）
\end{itemize}
即总共19次非预取缺失。避免232次缓存毓失的成本是执行了400条预取指令，
这很划算。
例题
计算上面例题中节省的时间。忽略指令缓存缺失，并假定数据缓存中没有冲突缺失
或容量缺失。假定预取过程可以相互重叠，并能与缓存缺失重養。因此可以用最高
存储器带宽传输。下面是忽略缓存缺失的关键循环次数：原循环每次迭代需要7个
时钟周期，第一次预取循环每次迭代需要9个时钟周期，第二次预取循环每次选代
需要8个周期（包含循环外部的开销）。一次缺失需要100个时钟周期。

解答

原来的双层嵌套循环执行3x100-300次。由于该循环每次选代需要7个时钟周期，
所以总共为300 x 7=2100时钟周期再加上缓存缺失。缓存缺失增加 251 × 100-
25 100个时钟周期，总共27 200个时钟周期。第一次预取循环迭代100次，每次迭
代为9个时钟周期，所以总共900个时钟周期再加上缓存缺失。现在加上缓存缺失的
11x 100=1100个时钟周期，总共2000个时钟周期。第二个循环执行2x100-200次，
每次选代为8个时钟周期，所以需要1600个时钟問期，再加上缓存缺失8 × 100=800
个时钟周期。总共2400个时钟周期。由上个例子可以知道，这一代码为了执行这两
个循环，在 2000+2400-4400个时钟周期内执行了400条预取指令。如果我们假定这
些预取操作完全与其他执行过程相重叠，那么这一预取代码要快27200/4400-6.2倍。
尽管数组优化理解起来比较容易，但现代程序更可能使用指针。Luk 和Mowry［1999］已经
证明，基于编译器的预取优化有时也可以扩展到指针。在10个使用递归数据结构的程序中，在
访问一个节点时预取所有指针，可以使半数以上的程序性能提高4%~31%。而另一方面，其他
程序的性能变化不超过原性能的2%。问题症结包括两个方面，一是要预先提取的数据是否已经
存在于缓存中，二是预取数据能否在需要它们时提前到达缓存。

许多处理器支持缓存预取指令，高端处理器（比如 Intel Core i7）还经常在硬件中完成某种
类型的自动预取。

\subsection{缓存优化小结}
用于改进命中时间、带宽、缺失代价及缺失率等性能的技术通常会影响到存储器平均访问
时间公式的其他部分，而且还会影响到存储器层次结构的复杂性。表2-1总结了这些技术，并
估计了它们对复杂性的影响，其中“+”表明该技术会改进该项因素，“\_”表示会损害该项因
素，空白则表示没有影响。一般来说，没有哪种技术能够改进多项因素。

\begin{verbatim}
    表2-1 10种高级缓存优化技术小结，给出了对缓存性能、功耗和复杂度的影响
    技术
    命中
    缺失
    硬件成本/
    时间
    带宽
    缺失率
    功耗
    注
    代价
    复杂度
    小而简单的缓存
    路预测缓存
    流水化缓存访问
    非阻塞缀存
    分组缓存
    释
    很普通、应用广泛
    在Pentium 4中使用
    应用广泛
    应用广泛
    在i7和Cortex-A8的L2中
    使用
    应用广说
    关键字优先和提前
    重启动
    合并写缓冲区
    以编译器技术减少
    缀存缺失
    +
    +
    1
    0
    和直写法一起广泛应用
    软件是一个挑战，但许
    多编译器都能处理常见
    的线性代数计算
    指令与数据的硬件
    预取
    2（指令）
    3（数据）
    大多提供预取指令，现
    代高端处理器还会在硬
    件中实现自动预取
    编译器控制的预取
    需要非阻塞缓存，可能
    存在指令开销，在许多
    CPU中得到应用
\end{verbatim}
*尽管一种技术通常仅能改进一个因素，但如果能够尽早完成，预取可以减少缺失；如果不能尽早完成，則可以減少
敏失代价。“+” 表示该技术会改进该项因素，

“\_”表示会损害该项因素，空白則表示没有影响。复杂度的评估具
有主观性，0表示最容易，了标志着一项挑战。

\section{存储器技术与优化}
…唯一使计算机站稳脚跟的发展成果就是可靠存储器的发明，也就是核心存储器它的
成本在含理范围内，它是可靠的，而且因为它是可靠的，所以能够在适当的时候扩大其容量。［第
209 页］

—Maurice Wilkes
Memoirs ofa Computer Pioneer （1985）

主存储器在层次结构上位于缓存的下一层级。主存储器满足缓存的需求，并充当1/O接口，
既是输人的目的地，又是输出的源头。对主存储器的性能度量同时强调延迟和带宽。传统上，
主存储器延迟（它会影响缓存缺失代价）是缓存的主要考虑因素，而主存储器带宽则是微处理
器和1/O的主要考虑困素。

尽管缓存可以从低延迟存储器中获益，但一般来说，通过新的组织方式来提高存储器带宽
要比降低延迟更容易一些。由于多级缓存日益普及，而且采用较大的分块，使主存储器带宽对
于缓存也非常重要。事实上，缓存设计人员增大块大小就是为了利用更高的存储器带宽。

前面几节描述了可以通过缓存优化来缩小处理器与 DRAM之间的性能差距，但仅仅增大缓
存或增加更多的缓存级别并不能消除这一差距。主存储器也需要进行革新。

在过去，对主存储器的革新就是改变构成主存储器的众多 DRAM 芯片（比如多个存储器组）
的组织方式。在使用存储器组时，通过拓宽存储器或其总线，或者同时加宽两者，可以提供更
大的带宽。但是，具有讽刺意味的是，随着单片存储器芯片容量的增加，在同样大小的存储器
系统中，所需要的芯片越来越少，从而降低了存储器系统容量不变、带宽加大的可能性。

为使存储器系统跟上现代处理器的带宽需求，存储器革新开始在 DRAM 芯片自身内部展
开。本节描述存储器芯片内部的技术及其具有创新性的内部组成。在描述这些技术与选项之前，
我们首先来复习一下存储器的性能度量指标。

随着突发传输存储器的引入（这种存储器现在在 Flash 和 DRAM 中都得到广泛应用），存
储器延迟主要由两部分组成—访问时间和周期时间。访问时间是发出读取请求到收到所需字
之间的时间，周期时间是指对存储器发出两次不相关请求之间的最短时间。

自 1975年以来，几平所有计算机都采用 DRAM作为主存储器，采用SRAM作为缓存，在
处理器芯片中集成一到三级缓存，与CPU集成在一起。在PMD中，存储器技术经常要在功率
和速度之间进行平衡，高端系统会使用快速、高带宽存储器技术。

\subsection{SRAM技术}
SRAM 的第一个字母表示静态（static）。DRAM 电路的动态本质要求在读取数据之后将其
写回，因此在访问时间和周期时间之间存在差异，并需要进行刷新。SRAM 不需要刷新，所以
存在时间与周期时间非常接近。SRAM通常使用6个晶体管保存1位数据，以防止在读取信息
时对其造成干扰。在待机模式中，SRAM只需要很少的功率来维持电荷。

在早些时候，大多数桌面系统和服务器系统使用 SRAM芯片作为其主、辅或第三缓存；今
天，所有这三级缓存都被集成在处理器芯片上。目前最大的片上第三级缓存为12 MB，而这样
一种处理器的存储器系统可能拥有4~16 GB的DRAM。大容量第三级片上缓存的访问时间通
常是第二级缓存的2~4倍，而它仍然要比访问 DRAM存储器快3~5倍。
\subsection{DRAM技术}
早期 DRAM的容量增大时，由于封装上需要提供所有必要的地址线，所以封装成本较高。
解决方案是对地址线进行复用，从而将地址管脚数砍去一半。图2-11给出基本的 DRAM组成
结构。先在行选通（RAS）期间发送一半地址。然后在列选通（CAS）期间发送另一半地址。
行选通和列选通等名字源于芯片的内部结构，这些存储器的内部是一个按行和列寻址的长方
形矩阵。

对 DRAM 的另一要求来自其第一个字母 D 表示的特性，即动态（dynamic）。为了在每个
芯片中容纳更多的位，DRAM仅使用一个晶体管来存储一位数据。信息的读取过程会破坏该信
息，所以必须进行恢复。这是 DRAM周期时间一般要长于访问时间的原因之一；近来，DRAM
已经引入了多个组，从而可以隐藏访问周期中的重写部分，见图2-11。此外，为了防止在没有
读写某一个位时丢失信息，必须定期“刷新”该位。幸运的是，只需对一行进行读取就可以同
时刷新该行。因此，存储器系统中的每个 DRAM 必须在某一特定时间窗口内（比如8 ms）访
问每一行。存储器控制器包括定期刷新 DRAM的硬件。


图 2-11
DRAM 中的内部组成结构。现代 DRAM 是以“组”为单位进行组织的，DDR3通常有4组。
每一组由一系列行构成。发送 PRE（precharge）命令会打开或关闭一个组。行地址随 Act
（activate）命令发送，该命令会将一个行传送到缓冲区中。将行放人缓冲区后，就可以采用两
种方式进行传送，一种是根据 DRAM 的宽度采用连续列地址传送（在DDR3中，这一宽度通
常为4位、8位、16位），另一种是指定块传送方式，并给出起始地址。每个命令和块传送过
程，都以一个时钟进行同步

这一要求意味着存储器系统偶尔会有不可用时间，因为它要发出一个信号，告诉每个芯片
进行刷新。刷新时间通常是对DRAM 中每一行进行完整存储器访问（RAS 和 CAS）的时间。
由于 DRAM 中的存储器矩阵在概念上是方形的，所以一次刷新中的步骤通常是 DRAM容量的
平方根。DRAM设计人员尽力将刷新时间保持在总时间的5%以下。

到目前为止，我们对主存储器运作模式的描述就好像瑞士火车一样，完全根据时刻表持续
不断地提供货物。而刷新操作与这一类比有些矛盾，因为某些访问操作花费的时间要比其他访
问操作长得多。因此，刷新操作是存储器延迟发生变化的另一个原因，从而也会影响缓存缺失
代价的变化。

Amdahl 提出一条经验规律：要保持系统的平衡，存储器容量应当随处理器的速度线性增
长，所以一个运算速度为1000 MIPS 的处理器应当拥有1000 MB的存储器。处理器设计人员
依靠 DRAM来满足这一要求。过去，他们可以指望存储器容量每三年翻两番，也就是年增长
率为55%。遗憾的是，DRAM现在的性能增长速度非常慢。表2-2给出了行访问时间的性能改
变，它与延迟有关，每年大约为5%。CAS或数据传输时间与带宽有关，其增长速度超过上述
速度的两倍。

尽管我们前面讨论的是独立芯片，但DRAM通常是放在被称为双列直插存储模块（DIMIM）
的小电路板上出售的。DIMIM通常包括4~16个 DRAM，对于桌面系统和服务器系统，它们通
常被组织为8字节宽度（+ECC）。

除了 DIMM封装和用于提高数据传输速度的新接口之外（这些内容将在下面几个小节中讨
论），DRAM的最大改变是容量增长速度已经变缓。DRAM在20年里都符合摩尔定律，每三年
推出一代容量增长4倍的新芯片。从1998年开始，由于单比特 DRAM 所面对的制造难题，只
能每两年推出一代容量加倍的新芯片。2006年，增长速度进一步减缓，从2006年到2010年的
4年时间里，容量只提高了1倍。

表2-2 每一代 DRAM 的最快速、最慢速访问时间
行选通（RAS）
生产年份
芯片大小
DRAM类型
最慢DRAM（ns） 最快DRAM （ns）
1980
64K bit
DRAM
180
150
1983
256K bit
DRAM
150
120
1986
1M bit
DRAM
120
100
1989
4M bit
DRAM
100
80
1992
16M bit
DRAM
80
60
1996
64M bit
SDRAM
70
1998
128M bit
SDRAM
70
s0
2000
256M bit
DDR1
65
45
2002
512M bit
DDR1
60
40
2004
1G bit
55
35
列选通（CAS）/数
据传輸时间（ns）
75
s0
25
20
15
12
10
7
s
周期时
间 （ns）
DDR2
2006
2G bit
DDR2
50
30
2.5
2010
4G bit
DDR3
36
28
1
2012
8G bit
DDR3
30
24
0.5

*（周期时间的定义参见73页。行访问时间的性能改进为每年大約5%。1986年，由于从 NMOS DRAM特* CMOS DRAM，
所以列访问性能提高了2倍。20世纪90年代中期引入的各种突发传输模式以及20世纪90年代后期引入的 SDRAM
显菁增加了数据块访问时间的计算复杂度。在本节后面讲到 SDRAM 访问时间和功率时将对此进行讨论。DDR4设
计预计会在2012年的中后期完成。我们将在后面几页中讨论这些不同形式的 DRAM。

\subsection{提高DRAM芯片内部的存储器性能}
晶体管的增长速度仍然符合摩尔定律，存储器与处理器之间的性能差距对存储器性能施加
的压力也是日益增大，所以上一节介绍的各种思想已经开始寻求在DRAM芯片闯出一条道路。
概括来说，有些创新技术已经提高了存储器的带宽，但有时是以增大延迟为代价的。本节将介
绍一些充分利用 DRAM自身特性的技术。

前面曾经提到，对 DRAM的访问分为行访问和列访问两部分。DRAM必须在 DRAM内部
缓冲一行中的所有位，为列访问做好准备，一行的位数通常等于 DRAM大小的平方根—一比如，
4MbitDRAM的每行有2Kbit。随着 DRAM 容量的增大，添加了一些附加结构，也多了几种提
高带宽的可能性。

第一，DRAM添加了定时信号，允许重复访问行缓冲区，从而节省了行访问时间。这种缓
冲区的出现非常自然，因为每个数组会为每次访问操作缓冲1024~4094位信息。最初，在每次
传输时都需要发送不同的列地址，从而在每组新的列地址之后都有一定的延迟时间。

DRAM 原来有一个与存储器控制器相连的异步接口，所以每次传输都需要一定的开销，以
完成与控制器的同步。第二，向 DRAM接口添加一个时钟信号，使重复传输不需要承担这一开
销。这种优化的名字就是同步 DRAM（SDRAM）。SDRAM通常还有一个可编程寄存器，用于
保存请求字节数，因此可以在几个周期内为每个请求发送许多字节。通常，将 DRAM 置于突发
模式，不需要发送任何新地址就能进行8次或更多次的16位传输；这种模式支持关键字优先传
输方式，是唯一能够达到表2-3所示峰值带宽的方法。

第三，随着存储器系统密度的增加，为了从存储器获得较宽的比特流，而又不必使存储器
系统变得过大，人们拓展了DRAM 的宽度。原来提供一种 4位传输模式；2010年、DDR2和
DDR3 DRAMS 已经拥有宽达16位的总线。

第四种提高带宽的 DRAM重要创新是在 DRAM 时钟信号的上升沿和下降沿都传输数据，
从而使峰值数据传输率加倍。这种优化方法被称为双倍数据率（DDR）。

为了提供交错（interleaving）的一些优点，并帮助进行电源管理，SDRAM 还引入了分組，
将一块 SDRAM分为2~8个可以独立运行的块（在目前的 DDR3 DRAM 中为8块）。（我们已
经见过分组在内部缓存中的应用，它们也经常用于大容量主存储器中。）在一个 DRAM 中创建
多个组可以有效地添加另一个地址段，它现在由组号、行地址和列地址组成。在发送指定一个
新组的地址时，这个组必须已经打开，会增加一些延迟。分组与行缓冲区的管理完全由现代存
储器控制接口处理，这样，如果后续地址指定了一个开放组的相同行时，只需要发送列地址就
能快速完成访问。

在将 DDR SDRAM封装为 DIMM形式时，会用 DIMIM 峰值带宽进行标记，这种标记方法
很容易引起混淆。比如，之所以得到 PC2100这样一个 DIMM 名称，是源于133 MHz×2×8字
节=2100MIB/s。为了避免这种混淆，在对芯片本身进行标记时，给出的不是其时钟频率，而是
每秒比特数，因此一个 133MHz的 DDR芯片被称为 DDR266。表2-3给出了时钟频率、每芯片
每秒钟的传送数目、芯片名称、DIMM 带宽和 DIMM名称之间的关系。

表2-3
2010年 DDR DRAM 和DIMM 的时钟频率、带宽和名称
标准 时钟频率（MHZ）
每秒传输个数（百万个）
DRAM名称
MB/s/DIMM
DIMM名称
DDR
133
266
DDR266
2128
PC2100
DDR
150
300
DDR300
2400
PC2400
DDR
200
400
DDR400
3200
PC3200
DDR2
266
533
DDR2-533
4264
PC4300
DDR2
333
667
DDR2-667
5336
PC5300
DDR2
400
800
DDR2-800
6400
PC6400
DDR3
533
1066
DDR3-1066
8528
PC8500
DDR3
666
1333
DDR3-1333
10 664
PC10700
DDR3
800
1600
DDR3-1600
12 800
PC12800
DDR4
1066-1600
2133-3200
DDR4-3200
17 056-25 600
PC25600
*请注意各列之间的数字关系。第三列是第二列的2倍，第四列 DRAM 芯片名称中使用了第三列中的数字。第五列是
第三列的8倍，DIMIM 名称中使用了这一数字的四舍五入值。根据 DDR 标准，DDR还以 4个数字指明了时钟周期中
的延迟，但本表没有给出这一信息。例如，DDR3-2000 CL 9的延迟*9-9-9-28。这是什么意思呢？对于一个周期为
1 ns 的时钟（时钟周期是传榆速度的一半），这一数字意味着行至列地址为9ns（RAS 时间），列地址到数据为9ns
（CAS 时间），最短读取时间为 28 ns。关闭该行将需要9ns 进行预克电，但只有在完成该行的读取时才会进行这一
操作。在突发模式中，经过第一RAS时间与 CAS时间之后，在每个时钟的上下沿都会进行传输。此外，在读取整行
之前不需要预充电。DDR4将于2012年投产，预计在 2014 年将达到 1600 MHz的时钟频率，届时将会升級至 DDRS。
在练习中将进一步探讨这些细节。

DDR现在已经成为一个标准序列。DDR2将电压由2.5伏降至1.8伏，从而降低了功率，并
提供了更高的时钟频率：266 MIHz、333 MIHz 和400 MIHz。DDR3将电压降至1.5伏，最高时钟频
率为800MHz。计划于2014年投产的DDR4将电压降到1至1.2伏，预计最高时钟频率为1600 MHz。
DDR5大约会在2014年或2015年跟进。（在下一节将会讨论，GDDRS 是一种图形 RAM，以
DDR3 DRAM为基础。）

图形数据 RAM
GDRAM或GSDRAM（图形 DARM或图形同步 DRAM）是一种特殊类型的 DRAM，它们
以SDRAM设计为基础，但为满足图形处理器中的高带宽需求进行了特别定制。GDDR5以 DDR3
为基础，较早的GDDR 以 DDR2为基础。GPU（见第4章）对每个 DRAM芯片的带宽要求要
1
高于CPU，因此，GDDR有以下几点重要不同。

\begin{enumerate}
    \item GDDR 的接口更宽，为32位，而目前的设计为4、8或16位。
    \item GDDR 数据管脚上的时钟频率更高。为了在不招致信号发送问题的前提下提高传输速
    率，GDRAMS通常直接与GPU 相连，将它们焊接在电路板上，这一点与 DRAM 不同，它们通
    常放置在可扩展的 DIMM阵列中。
\end{enumerate}

这些特性综合在一起，使GDDR每块 DRAM的带宽比DDR3 DRAM 高2~5倍，可以更好
地为 GPU提供支持。由于 GPU 中存储器请求的局域性较低，所以突发模式对GPU一般没有太
大用处，但保持多个存储器组的开放状态并合理安排其应用，可以提高有效带宽。

\subsection{降低SDRAM中的功耗}
动态存储器芯片中的功耗由静态（或待机）功率和读写期间消耗的动态功率构成，这两著
取决于工作电压。在大多数高端 DDR3 SDRAM 中，工作电压已经降到1.35~1.5伏，与DDR2
SDRAM 相比，显著降低了功率。分组的增加也降低了功率，这是因为每次仅读取并预充电一
个分组中的行。

除了这些变化之外，所有最新 SDRAM都支持一种省电模式，通知 DRAM 忽略时钟即可进
入这一模式。省电模式会禁用 SDRAM，但内部自动刷新除外（如果没有自动刷新，当进入省
电模式时间长于刷新时间后，将会导致存储器内容丢失）。图 2-12给出了一个 2 Gbit DDR3
SDRAM在三种情况下的功耗。从低功率模式返回正常模式所需的确切延迟时间取决于
SDRAM，但从自动刷新低功率模式返回正常状态的时间一般为200个时钟周期；在第一个命令
之前复位模式寄存器可能需要延长一些时间。

600
500
400
300

读、写、终止功率
激活功率
背最功率
200
100
0
低功率
典型应用
完全活
模式
动模式
图 2-12 DDR3 SDRAM 在3种不同运行条件下的功耗：低功率（关闭）模式、典型系统模式。（在读
取操作中，DRAM 有30%的时间处于活动状态，在写人操作中有15%的时间处于活动状态）和
完全活动模式，在这种模式下，DRAM 在非预充电状态下持续读取或写人。读取和写人采用由
8次传输组成的突发形式。这些数据系根据 Micron 1.5V2 Gbit DDR3-1066测得
\subsection{闪存}
闪存是一种 EEPROM（电可擦可编程只读存储器），它通常是只读的，但可以擦除。闪存
的另一个重要特性是能在没有任何供电的情况下保存其内容。

闪存在PMID 中用作备份存储，其工作方式与笔记本型计算机或服务器中的磁盘相同。此外，
由于大多数PMD的 DRAM 数量有限，所以闪存在很大程度上还要充当存储器层次结构的一级，
这种可能要比在桌面计算机或服务器中高出很多，因为后者的主存储器可能要大出10~100倍。
闪存使用的体系结构与标准 DRAM有很大不同，性质也有所不同。最重要的区别在于以下
几方面。

\begin{enumerate}
    \item 在改写内存之前，必须对其进行擦除（因此，“闪存”名字中的“闪”字就是指快速
    擦除的意思），（在高密度闪存中，称为 NAND闪存，大多数计算机都采用这种闪存）它的擦
    除过程是按块进行的，而不是单独擦除各个字节或各个字。这意味着在需要向闪存中写入数
    据时，必须对整个块进行处理，或者全是新数据，或者将要写人的数据与块中的其他内容合
    并在一起。
    \item 闪存是静态的（也就是说，即使在没有供电的情况下，它也能保持其内容），在未进
    行读写时，只消耗非常低的一点功率（在待机模式下会低于—-半，在完全非活动状态上可以
    为零）。
    \item 对任何一个块来说，闪存为其提供有限数目的写入周期，通常至少为100000个。这样，
    系统可以确保写人块均匀分布在整个存储器中，从而在最大程度上延长闪存系统的寿命。
    \item 高密度闪存比 SDRAM 便宜，但比磁盘贵：闪存的价格大约是2美元/GB，SDRAM为
    20美元到40美元/GB，磁盘为0.09美元/GB。
    \item 闪存的速度比 SDRAM 慢得多，但比磁盘快得多。例如，从一个典型高密度闪存传送
    256字节数据大约需要6.5ps（它使用的突发传送模式与SDRAM类似，但要慢一些）。从 DDR
    SDRAM进行类似传输需要的时间大约长四分之一，而从磁盘上传输则大约慢1000倍。对于写
    入操作，这种区别更是大得多，SDRAM 至少比闪存快10倍，也可能达到100倍，具体数值取
    决于环境。
\end{enumerate}

高密度闪存在过去十年里的快速发展，已经使这一技术成为移动设备存储器层次结构中最
具活力的组成部分，也成为磁盘的固态替代技术。随着 DRAM密度增长速度的持续下降，闪存
将在未来存储器系统中扮演越来越重要的角色，既可取代硬盘，也可作为 DRAM与磁盘之间的
中间存储。

\subsection{提高存储器系统的可靠性}
缓存和主存储器容量的增大也大幅提高了在制造过程期间和对存储器单元进行动态冲击
时（主要来自宇宙射线）出现错误的概率。这些动态错误会改变存储器单元的内容，但不会改
变电路，称之为软错误。所有DRAM、闪存和许多 SRAM 在制造时都留有备用行，这样可以
容忍少量的制造缺陷，只需要通过编程方式用备用行替代缺陷行即可。除了必须在配置期间纠
正的制造错误之外，还可能在运行时发生硬错误，它可能会永久改变一个或多个存储器单元的
运行方式。

动态错误可以使用奇偶校验位检测，可以使用纠错码（ECC）检测和纠正。因为指令缓存
是只读的，用奇偶校验位就足够了。在更大型的数据缓存和主存储器中，则使用ECC技术来检
测和纠正错误。奇偶校验位只需要占用一个数据位就可以检测一系列数据位中的一个错误。由
于无法使用奇偶校验位来检测多位错误，所以必须限制用奇偶校验位提供保护的位数。一个常
用比值是每8个数据位使用一个奇偶校验位。如果采用ECC技术，在每64个数据位中，8位的
开销成本可以检测两个错误，纠正一个错误。

：巫华竹咱的化娅必似
在规模庞大的系统中，出现多个错误乃至单个存储器芯片完全失效的机率都大大增加。IBM
引人了 Chipkill 来解决这一问题，许多大规模系统，比如 IBM 和 SUN 服务器和 Google Chusters
都使用这一技术。（Intel 将其自己的版本命名为 SDDC。）Chipkill 在本质上类似于磁盘中使用
RAID 方法，它分散数据和 ECC信息，在单个存储器芯片完全失效时，可以从其余存储器芯片
中重构丢失数据。根据 IBM 的分析，假定有10 000个服务器（每个处理器有4 GB 存储器），
在三年的运行中出现不可恢复错误的数目如下所示。

\begin{itemize}
    \item 仅采用奇偶校验位——大约90 000个，或者说每17分钟一个不可恢复（或未检测到）的故障。
    \item 仅采用 ECC--大约3500个，或者说大约每7.5 小时一个不可恢复（或未检测到）的故障。
    \item Chipkil— 6个，或者说大约每2个月一个不可恢复（或未检测到）的故障。
\end{itemize}

另一种研究方法是在错误率与 Chipkill 保持一致的情况下求出其他两种方式能够保护的
最大服务器数目（每个服务器拥有4GB存储器）。采用奇偶校验位方法，即使是一个仅包括
一个处理器的服务器，其不可恢复错误率也要高于由10 000个服务器组成、受 Chipkill保护
的系统。采用ECC方法，一个包含17个服务器的系统与包含10 000个服务器的Chipkill系
统的故障率大体相同。对于仓库级计算机中的50 000~100 000个服务器来说，需要采用
Chipkill方法（见6.8节）。
\section{保护：虚拟存储器和虚拟机}
虚拟机被用作真实机器的一种高效、独立副本。我们適过虚拟机监视（VMM）的思
想来解释这些概念VMIM 有三个基本特性。第一，VMIM为程序提供了一种与原机器基
本相同的运行环境；第二，在这种环境中运行的程序最糟糕的情况也不过是速度略有降低；
最后，VMIM可以完全控制系统资源。

Gerald Popek 和 Robert Goldberg
“Formal requirements for virtualizable third generation architectures，”
Communications ofthe ACM （1974年7月）

2011年，在信息技术面对的最令人困扰的挑战中，安全与保密占据了两个席位。人们不断
看到有关电子窃案的报道，其中经常涉及大量信用卡号，人们普遍认为还有大量此类案件没有
报道出来。因此，研究人员和业内人员都在探寻能够提高计算机系统安全性的新方法。信息保
护并非仅限于硬件方面，我们认为真正的安全与保密需要在计算机体系结构和系统软件方面均
有所创新。

本节首先回顾体系结构如何通过虚拟内存来保护进程，避免它们相互伤害。接下来介绍虚
拟机增加的保护措施、虚拟机在体系结构方面的需求以及虚拟机的性能。在第6 章将会看到，
虚拟机是实现云计算的基础技术。

\subsection{通过虚拟存储器提供保护}
页式虚拟存储器（包括缓存页表项目的变换旁视缓冲区）是保护进程免受相互伤害的主要
机制。附录B的B.4节、B.5节回顾了虚拟存储器的相关内容，详细介绍了80x86中通过分段、
分页提供的保护。这一小节仅作一个快速回顾；如果过于简短，请参考上述章节。

多道程序设计（multiprogramming，几个同时运行的程序共享一台计算机的资源）需要在各
个程序之间提供保护和共享，从而产生了进程概念。打个比方，进程就是一个程序呼吸的空气、
生存的空间—也就是一个正在运行的程序加上它持续运行所需要的全部状态。在任意时刻，
必须能够从一个进程切换到另一个进程。这种交换被称为进程切换或上下文切换。

操作系统和体系结构联合起来就能使进程共享硬件而不会相互干扰。为此，在运行一个用
户进程时，体系结构必须限制用户进程能够访问的资源，但要允许操作系统进程访问更多资源。
体系结构至少要做到以下几点。

\begin{enumerate}
    \item 提供至少两种模式，指出正在运行的进程是用户进程还是操作系统进程。后者有时被称
    为内核进程或管理进程。
    \item 提供一部分处理器状态信息，用户进程可以使用但不能写人。这种状态信息包括用户/
    管理模式位、异常启用/禁用位和存储器保护位。之所以禁止用户写人这些状态信息，是因为
    如果用户可以授予自己管理权限、禁用异常或者改变存储器保护，那操作系统就不能控制用户
    进程了。
    \item 提供处理器借以从用户模式转为管理模式及反向转换的机制。前一种转换通常通过系统
    调用完成，使用一种特指令将控制传递到管理代码空间的一个专用位置。保存系统调用时刻
    的程序计数器，处理器转人管理模式。返回用户模式的过程类似于一个全程返回过程，恢复到
    先前的用户/管理模式。
    \item 提供限制存储器访问的机制，在上下文切换时不需要将一个进程切换到磁盘就能保护该
    进程的存储器状态。
\end{enumerate}

附录A介绍了几种存储器保护机制，但到目前为止，最流行的机制还是添加对虚拟存储器
各个页面的保护性限制。固定大小的页面（通常长4KB或8KB）通过一个页表由虚拟地址空
间映射到物理地址空间。这些保护性限制就包含在每个页表项中。保护性限制可以决定一个用
户进程能否读取这个页面，一个用户进程能否写这个页面以及能否从这个页面执行代码。此外，
如果一个进程没有包含在页表中，那它就既不能读取也不能写人一个页面。由于只有操作系统
才能更新页表，所以分页机制提供了全面的访问保护。

分页虚拟存储器意味着每次存储器访问在逻辑上都要花费至少两倍的时间，一次存储器访
问用以获取物理地址，第二次访问用于获取数据。这种成本可能太过高昂了。解决方案就是依
靠局域性原理，如果这些访问具有局域性，那么访问操作的地址转换也肯定具有局域性。只要
将这些地址转换放在一个特殊的缓存中，存储器访问就很少再需要第二次访问操作来转换地址
了。这种特的地址转换缓存被称为变换旁视缓冲区（TLB）。

TLB项目类似于缓存项目，其中的标记保存虚拟地址部分，数据部分保存物理页地址、保
护字段、有效位，通常还有一个使用位和一个更改位（dirty bit ）。操作系统在改变这些位时，
改变页表中的值，然后使相应的TLB项失效。当这个项目重新载入到页表中时，TLB 即获得这
些位的准确副本。

如果计算机严格遵守对页面的限制，将虚拟地址映射到物理地址，那看起来我们就万事大
吉了。但报纸头条向人们描述的可是另一番景象。

我们之所以还有任务要做，是因为我们依赖于操作系统和硬件的准确性。今天的操作系统
由数千万行代码组成。由于Bug的计算是以每千行代码中的个数为单位的，所以在一个生产操
作系统中可能会出现数干个bug。操作系统中的缺陷产生了经常会被利用的系统漏洞。
与过去相比，现在不实施保护措施所导致的代价很可能要比过去高昂得多，这种可能性的
存在再加上上述问题，已经促使人们寻找一种保护模型，其基本代码要比完整的操作系统小许
多，比如虚拟机就是这样一种模型。

\subsection{通过虚拟机提供保护}
有一个与虚拟存储器相关而且几乎与它一样古老的概念，那就是虚拟机（VM）。它们最
早是在20世纪60年代后期提出的，多年以来一直是大型机计算的重要组成部分。尽管它在
20 世代80年代和90年代的单用户计算机领域被广泛忽视，但近来再度得到广泛关注，原因
如下：

\begin{enumerate}
    \item 隔离与安全在现代系统中的重要性提高；
    \item 标准操作系统的安全性和可靠性出现问题；
    \item 许多不相关用户（比如一个数据中心或云中的用户）共享同一计算机；
    \item 处理器原始速度的飞速增大，使虚拟机的开销更容易接受。
\end{enumerate}

最广义的虚拟机定义基本上包括了所有提供标准软件接口的仿真方法，比如 Java VM。我
们感兴趣的是那些在二进制指令集体系结构（ISA）级别提供完整系统级环境的虚拟机。最常
见的情况是，VM 支持的ISA 与底层硬件相同；但也有可能支持不同的ISA，在ISA之间迁移
时经常采用这种方法，这样，在能够迁移到新ISA之前，使软件能够继续在原ISA上使用。在
我们重点关注的虚拟机中，所提供的ISA与其底层硬件相匹配。这种虚拟机称为（操作）系统
虚拟机。IBM VM/370、VMware ESX Server 和Xen 都属于此类虚拟机。它们让虚拟机用户感觉
到自己拥有整个计算机，包括操作系统的副本在内。一台计算机可以运行多个虚拟机，可以支
持多种不同操作系统（OS）。在传统平台上，一个操作系统“拥有”所有硬件资源，但在使用
虚拟机时，多个操作系统一起共享硬件资源。

为虚拟机提供支持的软件称为虚拟机监视器（VM）或管理程序，VMIM 是虚拟机技术的
核心。底层硬件平台称为主机，其资源在来宾 VM之间共享。VMIM 决定了如何将虚拟资源映
射到物理资源：物理资源可以分时共享、划分，甚至可以在软件内模拟。VMM 要比传统操作
系统小得多，VMIM 的一个隔离部分大约只有10000行代码。

一般来说，处理器虚拟化的成本取决于工作负载。用户级别的处理器操作密集型程序（比
如 SPEC CPU 2006）的虚拟化开销为零，这是因为很少会调用操作系统，所有程序都以原速运
行。与之相对的是 V/O 操作密集的工作负载，它们通常也会大量涉及操作系统，执行许多系统
调用（以满足 1/O需求）和可能导致高虚拟化开销的特权指令。这一开销的大小取决于必须由
VMIM模拟的指令数目和模拟这些指令的缓慢度。因此，根据我们的假定，如果一个来宾 VM
运行的ISA与主机相同，则这个体系结构和 VMIM 的目的就是直接在原始硬件上运行几乎所有
指令。另一方面，如果涉及大量1/O操作的工作负载也是1/O密集型的，由于处理器经常要等待
I/O，所以处理器虚拟化的成本可以完全被较低的处理器利用率所隐藏。

尽管我们这里关心的是VM提供保护的功能，但VM还提供了其他两个具有重要商业价值
的优点。

\begin{enumerate}
    \item 软件管理—VM 提供一种能够运行整个软件栈的抽象层，甚至包括诸如 DOS之类的
    旧操作系统。一种典型部署是用一部分 VM运行原有操作系统，大量VM运行当前稳定的操作
    系统版本，而一少部分 VM用于测试下一代OS版本。
    \item 硬件管理—需要多个服务器的原因之一是希望每个应用程序都能在独立的计算机
    上与其兼容的操作系统一起运行，这种分离可以提高系统的可靠性。VM允许这些分享软件
    栈独立运行，却共享硬件，从而减少了服务器的数量。还有一个例子，一些 VMM允许将正
    在运行的 VM迁移到不同计算机上，既可能是为了平衡负载，也可能是为了撤出发生故障的
    硬件。
\end{enumerate}

这两个原因说明了云服务器（比如 Amazon 的云服务器）为什么要依赖于虚拟机的原因。

\subsection{对虛拟机监视器的要求}
一个VM 监视器必须完成哪些任务？它向来宾软件提供一个软件接口，必须使不同来宾
的状态相互隔离，还必须保护自己免受客户端软件的破坏（包括来宾操作系统）。定性需求
包括：
\begin{itemize}
    \item 来宾软件在 VM上的运行情况应当与在原始硬件上完全相同，当然，与性能相关的行为
    或者因为多个 VM共享固定资源所造成的局限性除外；
    \item 来宾软件应当不能直接修改实际系统资源的分配。
\end{itemize}

为了实现处理器的“虚拟化”，VMIM 必须控制几乎所有操作—特权状态的访问、地址转
换、1/O、异常和中断，即使目前正在运行的来宾 VM和操作系统正在临时使用它们，也不应当
影响到这些控制。

例如，在计时器中断时，VMM将挂起当前正在运行的来宾 VM，保存其状态、处理中断、
判断接下来运行哪个来宾 VM，然后载人其状态。依赖计时器中断的来宾 VM 都会有一个由
VMM提供的虚拟计时器和仿真计时器。

为了进行管理，VMIM的管理权限必须高于来宾 VM，后者通常运行于用户模式；这样还能
确保任何特权指令的执行都由 VMIM 处理。系统虚拟机的基本需求几乎与上述分页虚拟存储器
的要求相同。

\begin{itemize}
    \item 至少两种处理器模式：系统模式和用户模式。
    \item 指令的一些特权子集只能在系统模式下使用，如果在用户模式下执行将会导致陷阱。所
    有系统资源都只能通过这些指令进行控制。
\end{itemize}

\subsection{虚拟机（缺少）的指令集体系结构支持}

如果在设计ISA期间已经为 VM作了规划，那就可以比较轻松地减少VMIM必须执行的指
令数、缩短模拟这些指令所需要的时间。如果一种体系结构允许 VM直接在硬件上运行，则为
其冠以可虚拟化的头衔，IBM370体系结构很骄傲地拥有了这一头衔。

遗憾的是，由于为桌面系统和基于 PC 的服务器应用序考虑VM 只是最近的事情，
所以大多数指令集在设计时都没有考虑虚拟化问题。80x86 和大多数 RISC体系结构都属于
此类。

由于 VMM必须确保客户系统只能与虚拟资源进行交互，所以传统的来宾操作系统是作为
一种用户模式程序在VMIM 的上层运行的。因此，如果一个来宾操作系统试图通过特权指令访
问或修改与硬件相关的信息（比如，读取或写人页表指针），它会问 VIMM发出陷阱中断。VMM
随后可以对相应的实际资源实进行适当修改。

因此，如果任何以用户模式执行的指令试图读写此类敏感信息陷阱，VMM可以截获它，
根据来宾操作系统的需要，向其提供敏感信息的一个虚拟版本。

如果缺乏此类支持，则必须采取其他措施。VIMM必须采取特殊的防范措施，找出所有存
在同题的指令，并确保来宾操作系统执行它们时能够正常运行，这样自然就会增加 VMM的复
杂度，降低 VM的运行性能。

2.5节和2.7节给出了80x86体系结构一些问题指令的具体示例。

\subsection{虚拟机对虚拟存储器和1/0的影响}

由于每个VM中的每个来宾操作系统都管理其自己的页表集，所以虚拟存储器的虚拟化就
成为另一项挑战。为了实现这一功能，VMIM 区分了实际存储器（real memory）和物理存储器
的概念（这两个词经常看作是同义词），使实际存储器成为虚拟存储器与物理存储器之间的独立、
中间级存储器。（有人用虚拟存储器、物理存储器和机器存储器来命名这三个层级。）来宾操作
系统通过它的页表将虚拟存储器映射到实际存储器，VMM 页表将来宾的实际存储器映射到物
理存储器。虚拟存储器体系结构可以通过页表指定，IBM VM/370 和 80x86 属于此类，也可以
通过 TLB结构指定，许多 RISC体系结构属于此类。

VMM没有再为所有存储器访问进行一级间接迁回，而是维护了一个影子页表，直接从来
宾虚拟地址空间映射到硬件的物理地址空间。通过检测来宾页表的所有修改，VIMIM 就能保证
硬件在转换地址时使用的影子页表项与来宾操作系统环境的页表项一一对应，只是用正确的物
理页代替了来宾表中的实际页。因此，只要来宾试图修改它的页表，或者试图访问页表措针，
VMM都必须加以捕获。这一功能通常通过以下方法来实现：对来宾页表提供写保护，并捕获
来宾操作系统对页表措针的所有访问尝试。前面曾经指出，如果对页表指针的访问属于特权操
作，就会很自然地实现捕获。

IBM370体系结构在20世纪70年代添加了一个由VMM管理的迁回层，解决了页表问题。
来宾操作系统和以前一样保存自己的页表，所以就不再需要影子页表。AMD在本公司与80x86
相对应的 Pacifica版本中采用了一种类似方案。

在许多 RISC计算机中，为了实现 TLB的虚拟化，VMM管理实际 TLB，并拥有每个来宾
VM的TLB 内容副本。为实现这一功能，所有访问 TLB 的功能都必须被捕获。具有进程 ID 标
记的TLB 可以将来自不同VM与VMIM的项目混合在一起，所以不需要在切换VM时刷新TLB。
与此同时，VMIM在后台支持 VM 的虚拟进程 ID 与实际进程 D之间的映射。

体系结构中最后一个要虚拟化的部分是1O。到目前为止，这是系统虚拟化中最困难的一
部分，原因在于连接到计算机的1O设备数目在增加，而且这些1/O设备的类型也变得更加多样
化。另外一个难题是在多个 VM之间共享实际设备，还有一个难题是需要支持不同的设备驱动
程序，在同一VM系统上支持不同来宾操作系统时尤为困难。为了仍然能够实现 VM，可以为
每个VM提供每种 1/O设备驱动程序的一个通用版本，然后交由 VMM来处理实际 1/O。

将虚拟1O设备映射到物理1/O设备的方法取决于设备类型。例如，VMM通常会对物理进
行分区，为来宾 VM创建虚拟磁盘，而 VMIM 会维护虚拟磁道与扇区到物理磁盘与扇区的映射。
网络接口通常会在非常短的时间片内在 VM之间共享，VMIM 的任务就是跟踪虚拟网络地址的
消息，以确保来宾 VM只收到发给自己的消息。

\subsection{VMM实例：Xen虚拟机}

在 VM发展的早期，发现了许多效率低下的问题。例如，来宾操作系统管理自己从虚拟页
到实际页的映射，但这种映射会被 VMM忽略，到物理页的实际映射是由 VMM执行的。换句
话说，仅仅为了“取悦”来宾操作系统就浪费了大量精力。为了减少这些低效问题，VMM开
发人员认为有必要让来宾操作系统知道它自己是在 VM上运行的。例如，来宾操作系统可以假
定实际存储器与它的虚拟存储器一样大，所以来宾操作系统不需要进行存储器管理。

为了简化虚拟化而允许对来宾操作系统进行微小修改的做法称为泛虚拟化（paravirtuali-
zation），开源Xen VMM是很好的一个例子。Amazon Web服务数据中心使用的就是Xen VMM，
它为来宾操作系统提供了一个与物理硬件相似的虚拟机抽象，但去掉了许多很麻烦的部分。
例如，为了避免刷新 TLB，Xen 将它自己映射到每个 VM的高位64 MB地址空间。它允许来
宾操作系统分配页，只要核实没有违犯保护约束条件即可。为了保护来宾操作系统免受VM
中用户程序的破坏，Xen利用了80x86中可以使用的4种保护级别。Xen VMM 以最高级权限
运行（0级），来宾操作系统运行在下一级别（1级），应用程序以最低权限级别运行（3级）。
大多数针对80x86 的操作系统以0级或3级权限运行所有程序。

为了使各部分能够协调工作，Xen 对来宾操作系统进行了修改，不再使用体系结构中容易
产生问题的部分。例如，Linux 到 Xen 的端口大约修改了3000行代码，大约占 80x86 专用代码
的1\%。但是这些修改不会影响来宾操作系统的应用二进制接口。

为了简化VM的1/O难题，Xen为每个硬件1/O设备指定了具有特权的虚拟机。这些特殊
VM称为驱动程序域。（Xen 将它的 VM称为“域”）。驱动程序域运行物理设备驱动程序，但在
向适当的驱动程序域发送中断之前，这些中断仍然由VMIM处理。常规VM称为来宾域，运行
简单的虚拟设备驱动程序，它们必须通过一个信道与驱动程序域中的物理设备驱动程序进行通
信，以访问物理1/O硬件。数据通过页面再映射在来宾域和驱动程序域之间传送。

\section{交叉问题：存储器层次结构的设计}
这一节介绍三个将在其他章节讨论的主题，它们是存储器层次结构的基础。

\subsection{护和指令集体系结构}
保护是由体系结构和操作系统协力完成的，但是当虚拟存储器变得更为普及时，体系结构
必须修改现有指令集体系结构中一些不便使用的细节。例如，为了在IBM 370中支持虚拟存储
器，架构师必须对成功的 IBM 360 指令集体系结构进行修改，而这一指令集是6年前刚刚发布
的。为了适应虚拟机，今天也要进行一些类似调整。

例如，80x86 指令 POPF 从存储器栈的顶端载人标志寄存器，其中一个标志是中断使能（I）
标志。在最近为了支持虚拟化而作出修改之前，以用户模式运行 POPF 指令（而不是采用陷阱中
断捕获它）将会改变除IE 之外的所有标志位。而在系统模式下运行时，的确会修改IE标志。
由于来宾操作系统是以用户模式在VM 中运行的，而它希望修改 正标志位，所以这就成为一个
问题。为了支持虚拟化而对80x86体系结构的扩展，消除了这一问题。

在历史上，IBM大型机硬件和 VMIM通过以下3个步骤来提高虚拟机的性能。
\begin{enumerate}
    \item 降低处理器虚拟化的成本。
    \item 降低由于虚拟化而造成的中断开销成本。
    \item 将中断传送给正确的 VM，而不调用 VMM，以降低中断成本。
\end{enumerate}

IBM仍然是虚拟机技术的黄金标准。例如，一台IBM 大型机在2000年运行数千个 Linux
VM，而Xen 在2004年仅运行25个 VM［Clark 等人 2004］。Intel 和 AMID 芯片集的最近版本添
加了一些指令，用以支持VM 中的设备，在较低层级屏蔽来自每个 VM 的中断，将中断发送到
适当的 VM。

\subsection{缓存数据的一致性}

数据可以同时出现在存储器和缓存中。只要处理器是唯一修改或读取数据的组件，并且缓
存存在于处理器和存储器之间，那处理器看到旧副本或者说过期副本的危险性就很低。后面将
会看到，使用多个处理器和I/O设备增大了副本不一致及读取错误副本的机会。

处理器出现缓存一致性问题的频率与 1/O 不同。对1/0来说，出现多个数据副本是非常罕
见的情况（应当尽可能避免这种情况的发生），而一个在多处理器器上运行的程序会希望在几个
缓存中拥有同一数据的多个副本。多处理器程序的性能取决于系统共享数据的性能。

V/O 缓存一致性问题可以表述如下：1/O发生在计算机中的什么地方—是在1/O设备与缓
存之间，还是在 I/O 设备与主存储器之间？如果输人将数据放在缓存中，而且输出从缓存中读
取数据，那 I/O和处理器会看到相同数据。这种方法的难点在于它干扰了处理器，可能会导致
处理器因为等待1/O而停顿。输入还可能会用某些不会马上用到的新数据来取代缓存中的某些
信息，从而对缓存造成干扰。

在带有缓存的计算机中，1/0系统的目标应当是防止出现数据过期问题，同时尽可能减少干
扰。因此，许多系统喜欢直接对主存储器进行V/O操作，把主存储器当作一个1/0缓冲区。如果
使用直写缓存，存储器中将拥有最新的信息副本，在输出时不存在过期数据问题。（这一好处也
是处理器使用直写方式的一个原因。）遗憾的是，现在通常只会在第一级数据缓存中使用直写方
式，由使用写回方式的L2缓存为其提供后盾。

输人操作还需要另外做点功课。软件解决方案是保证输人缓冲区的所有数据块都没有在缓
存中。可以将包含缓冲区的页标记为不可缓存，操作系统总是可以向这样一个页面中输入数据。
或者，可以由操作系统在输人之前从缓存刷新缓冲区地址。硬件解决方案则是在输人时检查1/0
地址，查看它们是否在缓存中。如果在缓存中找到了 V/O地址的匹配项，则使缓存项失效，以
避免过期数据。所有这些方法也都能用于带有写回缓存的输出操作。

在多核处理器的发展过程中，处理器缓存一致性是一个关键问题，我们将在第5章对其进
行深入研究。

\section{融会贯通：ARM Cortex-A8 和 Intel Core i7 中的存储器层次结构}

本节揭示 ARM Cortex-A8（下文称为 Cortex-A8）和 Intel Core i7（下文称为i）的存储器
层次结构，并根据一组单线程基准测试展示其组件的性能。由于 Cortex-A8 的存储器系统更简
单一些，所以首先来研究它；之后将更详细地研究门，具体介绍一个存储器引用。本节假定读
者熟悉一种使用虚索引缓存的两级缓存层次结构的组织方式。这种存储器系统的基础知识在附
录B中进行了详细解释，如果读者不熟悉这种系统的组织方式，强烈建议复习附录 B 中的 Opteron
示例。该者一旦理解了 Opteron 的组织方式，就能很轻松地看懂 Cortex-A8 系统的简要解释了，它
们是类似的。

\subsection{ARM Cortex-A8}

Cortex-A8 是一种支持 ARMv7指令集体系结构的可配置核心。它是作为一种IP（知识产权）
核心交付的。在嵌人式、PMD 和相关市场上，IP核心是主要的技术交付形式；利用这些IP核
心已经生成了数十亿个 ARM 和 MIPS 处理器。注意，IP核心不同于 Intel i7 中的核心和 AMD
Athlon 多核心。一个IP核心（它本身可能是多核心）就是为与其他逻辑集成而设计的（因此，
它是一个芯片的核心），这些其他逻辑包括专用处理器（比如视频编解码器）、1/0接口和存储器
接口，从而制造出一种专门针对特定应用进行优化的处理器。例如，在 Apple iPad 和几家制造
商（包括摩托罗拉和三星）生成的智能手机中都使用了 Cortex-A8 IP 核心。尽管这些处理器核
心基本上是一样的，但最后得到的芯片有许多区别。

整体来说，IP核心分为两类。硬核心针对特定半导体厂家进行优化，是一些具有外部接口
的黑盒（不过这些接口仍然在片上）。硬核心通常仅允许对核心外面的逻辑进行参数设定，比如
L.2 缓存大小，不能对IP 核心本身进行修改。软核心的交付形式通常采用一个标准的逻辑元件
库。软核心可以针对不同的半导体厂家进行编译，也可以进行修改，不过由于当今IP核心的复
杂度，很难对其进行大幅修改。一般来说，硬核心的性能较高、晶片面积较小，而软核心则允
许针对不同厂家进行调整，其修改更容易一些。

当时钟频率高达1 GHz时，Cortex-A8每个时钟周期可以发出两条指令。它可以支持一种
两级缓存层次结构，第一级是一个缓存对（I和D），分别为16 KB或32KB，其组织形式为四
路组相联缓存，并使用路预测和随机替代。其目的是使缓存的访问延迟只有一个周期，使
Cortex-A8 将从载人到使用的延迟时间保持在一个周期，简化指令提取，在分支缺失导致预取了
错误指令时，降低提取正确指令的代价。第二级缓存是可选的，如果存在这一级缓存，则采用
八路组相联，容量可达128 KB~1 MB；它的组织形式分为1~4组，允许同时从存储器进行多
次传输。一个64位至128位的外部总线用来处理存储器请求。第一级缓存为虚索引、物理标记，
第二级缓存为物理索引与标记；这两级缓存的块大小都是64字节。当D缓存为32 KB 且页大
小为4KB时，每个物理页可以映射到两个不同的缓存地址；在发生缺失时，通过硬件检测可以
避免出现混淆现象，附录B中的B.3节对此进行说明。

存储器管理由 TLB对处理（I和D），每个 TLB与32个项目完全相关，页面大小可变（4KB、
16KB、64KB、1 MB 和16 MB）：TLB 中的替换用一种轮询算法完成。TLB 缺失在硬件中处理，
它会遍历存储器中的一个页表结构。图2-13显示如何使用32位虚拟地址来索引 TLB 和缓存，
假定主缓存为32 KB，次级缓存为512KB，页面大小为16KB。

\subsubsection{Cortex-A8 存储骼层次结构的性能}

Cortex-A8的存储器层次结构使用32 KB 主缓存和1 MB八路组相联L2缓存来模拟，用整
数 Minnespec 基准测试进行测试（见 KleinOsowski 和 Lilia ［2002］）。Minnespec 是一个基准测
试集，由 SPEC2000 基准测试组成，但其输人不一样，将运行时间缩短了几个数量级。尽管使
用较小规模的输人并不会改变指令混合比例，但它的确会影响缓存行为。例如，根据 mcf的测
试结果（它是存储器操作最密集的 SPEC2000整数基准测试），当缓存为32 KB 时，Minnerspec
的缺失率只有完整 SPEC版本缺失率的65\%。当缓存为1MB时，这种差距为6倍。根据许多
其他基准测试，这些比值都与 mcf的测试结果类似，但绝对敏失率要小得多。由于这一原因，
不能将 Minniespee 基准测试与 SPEC2000基准测试进行对比。不过这些数据对于研究L1和L.2
缺失对整体 CPI 的相对影响是很有用的，下一章就将进行这些研究。

图 2-13 ARM Cortex-A8 数据缓存和数据TLB 的虚拟地址、物理地址、索引、标记和数据块。由于指
令与数据层次结构是对称的，所以这里只给出其中一个。TLB（指令或数据）与32个项目完全
相关联。L1缓存是四路组相联，块大小为 64个字节，容量为32 KB。L.2缓存是八路组相联，
块大小为64 个字节，容量为1MB。本图没有显示缓存和TLB 的有效位和保护位，也没有使用
可以指示L1 缓存预测组的路预测位

即使仅对于LI，这些基准测试（以及作为 Minniespec 基础的完全 SPEC2000版本）的指
令缓存缺失率也非常低：大多接近于零，都低于1\%。这种低缺失率的原因可能是因为SPEC
程序在本质上是计算密集型的，而且四路组相联缓存消除了大多数冲突缺失。图2-14给出了
数据缓存结果，这些结果中的L.1和L2缺失率非常高。以DDR SDRAM为主存储器时，1GHz
Cortex-A8的L.1 缺失代价为11个时钟周期，L2缺失代价为60个时钟周期。通过这些缺失代
价数据，图2-15给出了每次数据存取的平均代价。在下一章，我们将研究缓存缺失对繁体CPI
的影响。


根据整数 Minnespec 基准测试的结果，对于采用32 KB L1 的ARM 的数据缺失率和1 MB L2
的全局数据缺失率受到应用程序的影很大。应用程序的存储器足印越大，L1 和 L.2 的缺失率
可能越高。注意，L2 缺失率为全局缺失率，它对所有引用进行计数，包括在L.1 中命中的情景。
rncf被称为缓存克星

图 2-15 ARM处理器在运行 Minniespec 时源于L1 和L2 的每次数据存储器引用的平均存储器访问代价。
尽管L.1的缺失率要高出许多，但L2的缺失代价（要高出5倍）意味着L2 的缺失占据主导地位

\subsection{Intel Core i7}
i7支持x86-64 指令集体系结构，它是 80x86体系结构的64 位扩展。i7是包含四个核心的
乱序执行处理器。本章主要从单核心角度来研究存储器系统的设计与性能。多处理器设计的系
统性能（包括17多核）将在第5章详细研究。

i7中的每个核心采用一种多次发送、动态调度、16级流水线（将在第3章详细介绍），每
个时钟周期可以执行多达4个80x86指令。订还使用一种名为“同时多线程”的技术（将在第
4章介绍），每个处理器可以支持两个同时线程。2010年，最快速17的时钟频率为3.3GHz，指
令的峰值执行速度为每秒132亿条指令，四核芯片超过每秒500万条指令。

i7可以支持多达三个存储器通道，每个通道由独立的 DIMM组构成，它们能够并行传输数
据。i7采用 DDR3-1066（DIMM PC8500），峰值存储器带宽超过25 GB/s。

i使用48 位虚拟地址和36位物理地址，物理存储器容量最大为36GB。存储器管理用一
个两级 TLB处理（见B.4节），表2-4中对此进行了总结。
\begin{verbatim}
    表2-4 i7 TLB 结构的特性，这种结构的第一级指令、数据TLB 是分离的，第二级 TLB 合而为一
    特性
    指令TLB
    数据DLB
    第二级TLB
    大小
    128
    64
    SI2
    相联廋
    四路
    四路
    四路
    替换
    伪LRU
    伪LRU
    伪LRU
    访问延迟
    1个周期
    1个周期
    6个周期
    觖失
    7个周期
    7个周期
    访问页表需要数百个周期
\end{verbatim}
*第一级TLB 支持标准4KB 页面大小，在2MB~4MIB 的大型页面上拥有有限教目的项目；在第二級 TLB 中仅支持4
KB页面。

表2-5总结了i7 的三级缓存层次结构。第一级缓存为虚索引、物理标记（见B.3节），而
L2和L3则采用物理索引。图2-16标有对存储器层次结构进行存取的步骤。首先，向指令缓存［18
发送程序计数器。指令缓存索引为：

缓存大小
24-政大樂存有A联限”2
A=128=2

也就是7位。指令地址的页帧（36-48-12位）被发送给指令 TLB（第1步）。同时，来自虛拟
地址的7位索引（再加上来自块偏移量的另外两位，用以选择适当的16字节，指令提取数）被
发送给指令缓存（第2步）。注意，对于四路相联指令缓存，缓存地址需要13位：7位用于索引
缓存，再加上64字节块的6位块偏移量，但页大小为4KB=2”，这意味着缓存索引的一位必须
来自虚拟地址。使用1位虚拟地址意味着对应块实际上可能位于缓存中的两个不同位置，这是
因为对应的物理地址在这一位置既可能为0也可能为1。对指令来说，这样不会有什么问题，
因为即使一条指令出现在缓存中的两个不同位置，这两个版本也必然是相同的。但如果允许对
数据进行此类重复或别名设置，那在改变页映射时就必须对缓存进行检查，这一事件并非经常
出现。注意，只要很简单地应用页着色（见B.3节）就能消除出现这种混淆的可能性。如果偶
数地址的虚拟页被映射到偶数地址的物理页（奇数页也一样），那么因为虚拟页号和物理页中的
低阶位都是相同的，所以就不可能发生这种混淆。

表2-5 i7 中三级缀存层次结构的特性
\begin{verbatim}
    特
    性
    L1
    L2
    L3
    大小
    32 KB 132 KBD
    256 KB
    每个核心2 MB
    相联度
    四路 I八路D
    八路
    十六路
    访问延迟
    4个周期、流水化
    10个周期
    35个周期
    替代方案
    伪LRU
    伪LRU
    伪LRU，但采用一种有序选择算法
\end{verbatim}
*所有这三级雏存都采用写回方式，块大小都为 64个字节。每个核心的L1和L.2缓存分惠，而L3線存在一个芯片
的所有械心之间共享，每个核心总共2MB。所有这三級组存都是非阻塞的，允许存在多个来完成写入。为L1線
存使用一个合并写缓冲区，在要写入数据但LI中没有行时，用这个缓冲区保存数据。（即，一次LI写入缺失不
会导致为其分配行。）L3是L1和L2 共有的；我们将在解释多械心缓存时更详细地研究这一属性。替換由伪 LRU
算法的一种变化方式究成。在L3中，被替换的块总是其访问位被关闭的具有最低编号的那一路。这种做法的随机
性较弱，但易于计算。

为查找地址与有效页表项（PTE）之间的匹配项而访问指令TLB（第3步和第4步）。除了
转换地址之外，TLB还会进行检查，以了解这一访问操作所需要的PTE是否会因为非法访问而
产生异常。

指令 TLB缺失首先进入L2TLB，它包含512个页大小为4KB的PTE，为四路组相联。从
L.2 TLB 中载人L1TLB需要两个时钟周期。如果L2TLB 缺失，则使用一种硬件算法遍历页表，
并更新TLB 项。在最糟糕情况下，这个页不在存储器中，操作系统从磁盘中获取该页。由于在
页面错误期间可能执行数百万条指令，所以这时如果有另一进程正在等待，操作系统将转入该
进程。否则，如果没有 TLB异常，则继续访问指令缓存。

地址的索引字段被发送到指令缓存的所有4个组中（第5步）。指令缓存标记为36-7位（索
引） -6位（块偏移）=23位。将4个标记及有效位与来自指令 TLB 中的物理页帧进行对比（第
6步）。由于i7希望每个指令获取 16个字节，所以使用6位块偏移量中的2位来选择适当的16
个字节。因此，在向处理器发送16字节指令时使用了7+2=9位。L1缓存实现了流水化，一次
命中的延迟为4个时钟周期（第7步）。一次缺失将进入第二级缓存。

前面曾经提到，指令缓存采用虚寻址、物理标记。因为第二级缓存是物理寻址的，所以来
自TLB 的物理页地址包含页偏移量，构成一个能够访问L2缓存的地址。L2索引为：

2米 =
缓存大小
256K
1=
=512=2%
块大小x组相联度 64×8

所以长30位的块地址（36位物理地址-6位块偏移）被分为一个21位的标记和一个9位的索引
（第8步）。索引和标记再次被发送给统一L.2缓存的所有8个组（第9步），同时对它们进行比
较。如果有一个匹配且有效（第10步），则在开头的10周期延迟之后按顺序返回该块，返回速
度为每个时钟周期8个字节。

如果L.2缓存缺失，则访问1.3缓存。对于一个四核i7（它的L.3为8MB），其索引大小为：
缓存大小
M=8192=21
这个长13位的地址（第11步）被发送给L3的所有16个组（第12步）。L3标记的长度为
36-（13-6）=17位，将其与来自 TL.B 的物理地址进行对比（第13步）。如果发生命中，则在
初始延迟之后以每个时钟周期16字节的速度返回这个块，并将它放在L1 和L3中。如果L3
缺失，则启动存储器访问。

国 2-16
；Intel i7 存储器层次结构及指令与数据访问步骤。我们只给出了数据读取步骤。写人步骤与其类
似，因为它们也是以读取操作开始的（由于缓存采用写回方式）。由于L！缓存没有进行写人分
配，所以只需要将数据放在写缓冲区中就可以处理缺失问题

如果在 L.3缓存中没有找到这个指令，片上存储器控制器必须从主存储器获取这个块。i7
有三个64位存储器通道，它们可以用作一个 192位通道，这是因为只有一个存储器控制器，在
两个通道上发送的是相同地址（第14步）。当两个通道具有相同的 DIMM时，就可以进行宽通
道传送。每个通道最多支持4个 DDR DIMM（第15步）。由于L3包含在内，所以在数据返回
时，会将它们放在L3和L1中（第16步）。

在发生指令缺失时，由主存储器提供这一指令的总延迟包括用于判断发生了L.3缺失的约
35个处理器周期，再加上关键指令的 DRAM延迟。对于一个单组 DDR1600 SDRAM 和3.3 GHz
CPU来说，在接收到前16个字节之前的 DRAM延迟为大约35ns 或100个时钟周期，所以总的
缺失代价 135个时钟周期。存储器控制器以每个存储器时钟周期16个字节的速度填充64字
节缓存块的剩余部分，这将另外花费15 ns 或45个时钟周期。

由于第二级缓存是一个写回缓存，任何缺失都会导致将旧块写回存储器中。i7 有一个10
项合并写缓冲区，当缓存的下一级未用于读取时写回脏缓存行。在发生任何缺失时都会查看此
写缓冲区，看看该缓存地是否在这个缓冲区中；如果在，则从缓冲区中获取缺失内容。在LI
和L2缓存之间使用了一个类似缓冲区。

如果初始指令是一个载人指令，则将数据地址发送给数据缓存和数据TLB，与指令缓存访
问非常类似，但有一个非常关键的区别。第一级数据缓存为八路组相联，也就是说索引是6位
（指令缓存为7位），用于访同此缓存的地址与页偏移相同。因此，就不再需要担心数据缓存中
的混淆问题。

假定这个指令是存储指令，而不是载人指令。在发出存储指令时，它会像载入指令一样进
行数据缓存查询。发生缺失时，会将这个块放到写缓冲区中，这是因为L1缓存在发生写缺失时
不会分配该块。在命中时，存储不会立即更新L1（或L2）缓存，而是要等到确认没有疑问时
才会进行更新。在此期间，存储指令驻存在一个“载入-存储”队列中，这是处理器乱序控制机
制的一个组成部分。

i7还支持从层次结构的下一层级为LI和L2 进行预取。在大多数情况下，预取行就是缓存
中的下一个块。在仅为L1 和L.2预取时，就可以避免向存储器执行高成本的提取操作。

\subsubsection{i7 存储器系统的性能}

我们使用 SPECCPU2006 基准测试中的19个基准测试（12个整型和7个浮点）来评估i7
缓存结构的性能，这些基准测试在第1章进行了介绍。本节的数据由路易斯安那州大学的 Lu
Peng 教授和 Ying Zhang 博士生收集。

我们首先来看L1缓存。这个32KB、4路组相联指令缓存的指令缺失率非常低，最主要的
原因是因为i7的指令预取十分有效。当然，由于i不会为单个指令单元生成单独的请求，而是
预取16字节的指令数据（通常介于4~5个指令之间），所以如何评估这一缺失率需要一点技巧。
为了简单起见，如果我们就像处理单一指令引用那样研究指令缓存缺失率，那么LI 指令缓存缺
失率的变化范围为0.1\%~1.8\%，平均略高于0.4\%。这一比率与利用 SPECCPU2006 基准测试对
指令缓存行为进行的其他研究一致，这些研究也显示指令缓存缺失率很低。

L1数据缓存更有趣，对它的评估需要更强的技巧性，原因如下所述。

（1）因为LI数据缓存不进行写人分派，所以写人操作可以命中，从来不会真正敏失，之所
以这么说，是因为那些没有命中的写人操作会将其数据放在写缓冲区中，而不会记录为缺失。
（2）因为推测有时可能会错误（请参阅第3章的详尽讨论），所以会有一些对L1数据缓存的
引用，它们没有对应最终会完整执行的载人或存储操作。这样的缺失应当怎样处理呢？
（3） 最后，L1数据缓存进行自动预取。发生缺失的预取是否应当计算在内？如果要计算在
内，如何计算？

为了解决这些问题，在保持数据量合理的情况下，图2-17以两种方式显示了L.1 的数据缓
存缺失：一种是相对于实际完成的载入指令数（通常称为“已完成”或“中途退出”），另一种
是相对于从任意来源执行的L1数据缓存访问数。可以看到，在仅相对于已完成载入指令测试的
缺失率要高出1.6倍（平均9.5\%对5.9\%）。表2-6以表格形式显示了相同数据。

图 2-17 以两种方式给出了17个 SPECCPU2006 基准测试的L1 数据缓存缺失率：相对于成功完成的
实际载入指令和相对于对L1的所有引用，这些引用中还包括没有完成预取、未完成的推测载入、
写入，它们会被记作引用，但没有生成缺失。与本节其余部分一样，这些数据也由来自路易斯
安那州大学的 Lu Peng教授和 Ying Zhang 博士生根据先前对 Iatel Core Due 和其他处理器的研究
进行收集。（见［Peng等人2008］）

表2-6 相对于已完成的全部载入操作数及全部引用数（包括预测及预取请求）给出的主要数据缓存缺失
基准测试
L1数据缺失/已完成载入
L1数据缺失儿1数据缀存引用
PERLBENCH
2%
1%
BZIP2
5%
3%
GCC
14%
6%
MCF
46%
24%
GOBMK
3%
2%
HMIMER
4%
3%
SJENG
2%
1%
LIBQUANTUM
18%
10%
H264REF
4%
3%
OMNETPP
13%
8%
ASTAR
9%
6%
XALANCBMK
9%
7%
MILC
8%
5%
NAMD
4%
3%
DEALII
6%
5%
SOPLEX
13%
9%
POVRAY
7%
5%
LBM
7%
4%
SPEIINX3
10%
8%

由于L1数据缓存缺失率达到5%~10%，有时还会更高一些，所以L2和L3缓存的重要性
应当就非常明显了。图2-18给出了1.2和L3缓存相对于 L1 引用的缺失率（表2-7以表格形式
给出同一数据）。由于对存储器的一次缺失需要超过100个周期的成本，而且L2中的平均数据
缺失率达4%，所以L3的重要性就不言而喻了。如果没有L3，并假定一半指令是载入或存储指
令，L2缓存缺失会使CPI增加每条指令2个周期！作为对比，L.3的平均数据缺失率为1%，仍
然非常显眼，但只有L2缺失率的1/4，是L.1 缺失率的1/6。在下一章，我们将研究i7CPI 与缓
存缺失之间的关系，以及其他流水线效果。

图2-18 相对于L1的所有引用给出17个 SPECCPU2006 基准测试的L2 和L3 数据缓存缺失，这些引
用中还包括预取、未完成的预测载入和程序生成的载入和存储指令。和本节其余数据一样，这
些数据也由来自路易斯安那州大学的Lu Peng教授和 Ying Zhang 博士生收集

\begin{verbatim}
    PERLBENCH
    BZIP2
    GcC
    MCF
    GOBMK
    HMMER
    SJENG
    表2-7 以表格形式给出相对于数据请求个数的L2和L3缺失率
    L2缺失数/所有数据缓存引用
    1%
    2%
    6%
    15%
    1%
    2%
    0%
    L3缺失数/所有数据缓存引用
    0%
    0%
    1%
    $%
    0%
    0%
    0%
    2.7 缪论与场犯错误
    95
    LIBQUANTUM
    H264RBF
    OMNETPP
    ASTAR
    XALANCBMK
    MILC
    NAMD
    DEALII
    SOPLEX
    POVRAY
    LBM
    SPHINX3
    L2缺失数/所有数据缓存引用
    3%
    1%
    7%
    3%
    4%
    6%
    0%
    4%
    9%
    0%
    4%
    7%
    （续）
    L3缺失数/所有数据缓存引用
    0%
    0%
    3%
    1%
    1%
    1%
    0%
    0%
    1%
    0%
    4%
    0%
\end{verbatim}
\section{谬论与易犯错误}
作为计算机体系结构中最容易实现量化的部分，存储器层次结构看起来似乎不太容易产生
谬论与错误。但事实并非如此，在编写这部分内容时，困扰我们的不是没有问题可讲，而是苦
于篇幅所限！

\textbf{谬论 由一个程序推测另一个程序的缓存性能。}
图2-19显示在缓存大小变化时，由 SPEC2000基准测试测得三个程序的指令鲈失率和数据
缺失率。根据程序的不同，对于一个容量为4096 KB 的缓存，每千条指令的数据敏失率分别为
9、2和90，对于一个容量为4KB 的缓存，每千条指令的指令缺失分别为SS、19和0.0004。诸
如数据库之类的商业程序甚至在大型第二级缓存中的缺失率也非常高，而对 SPEC程序来说一
般不是这种情况。图2-18提醒我们，测量结果的变化很大，从 rncf 和 sphnix3可以看出，甚至
关于整数和浮点密集型程序相对缺失率的预测也可能是错误的！

缓存大小（KB）
当容量大小从4 KB 变化至4096KB时，每1000条指令的指令缺失与数据缺失。gco
失比 lucas 大30000~40000倍，与之相反，lucas 的数据敏失比 gce大2至60倍。程序g
和lucas 均来自 SPEC2000基准测试套件

\textbf{易犯错误 模拟足够多的指令以获取存储器层次结构的准确性能测量值。}

这里实际上有三处陷阱。一是试图通过使用小型轨迹来预测大型缓存的性能。二是程序的
局域特性在运行整个程序期间不是恒定的。三是程序的局域特性可能随输人的变化而变化。
图2-20显示为一个SPEC2000程序提供5个输人时，每千条指令的累积平均指令缺失。
对于这些输人，前 19亿条指令的平均存储器缺失率与执行其余指令时的平均缺失率有很大
不同。

为 SPEC2000中的 perl 基准测试提供5种输入时，每1000次引用发生的指令缺失。对于前
19 亿条指令，缺失的变化不大，5种不同输人之间的区分也很小。运行到结束之后，将会看出
在该程序的整个生存期内缺失率是如何变化的，以及它们与输入有什么样的关系。上图显示前
19亿条指令的平均缺失率，对于全部5种输入，开始时每1000次引用的锁失大约为2.5个，结
束时大约为4.7个。下图给出运行到结束后的平均缺失，根据输人的不同，它需要160~410亿
条指令。在前19亿指令之后，根据输人的不同，每1000次引用的缺失从 2.4变化到7.9。这些
仿真是对 Alpha处理器实现的，它为指令和数据采用分离的L1缓存，每个缓存为两路 64 KB，
采用LRU算法，共用一个1MB 的直接映射L2缓存

\textbf{易犯错误 没有在基于经存的系统中提供高存储器带宽。}

缓存可以帮助缩短平均缓存存储器延迟，但可能不会提供应用程序进入主存储器所必需的
高存储器带宽。架构师必须在这种缓存背后为这些应用程序设计一种高带宽存储器。我们将在
第4章和第5章再次讨论这一易犯错误。

\textbf{易犯错误 在一个指令集体系结构上实施虚拟机监视器，而这种体系结构的设计是不能虚拟
化的。}

20世纪70年代和80年代的许多架构师都没有非常认真地确保所有读写硬件资源相关信息
的指令都是特权指令。这一放任态度为所有这些体系结构上的 VMM 都带来了问题，其中就包
括我们这里当作示例的80x86。

表2-8描述了18种可能会为虚拟化带来问题的指令［Robin 和 Irvine 2000］。这些指令可以分
为两大类：

\begin{itemize}
    \item 以用户模式读取控制寄存器，表明来宾操作系统运行在虚拟机中（比如前面提到的 POPF）；
    \item 根据分段体系结构的要求提供检查保护，但假定操作系统是以最高权限级别运行。
\end{itemize}

虚拟存储器仍然富有挑战性。因为 80x86 TLB 和大多数 RISC体系结构一样，不支持进程
ID标记，所以 VMM和来宾操作系统共享TLB的成本要更高一些；每次地址空间的变化通常都
需要进行一次 TLB刷新。

问题分类
当运行于用户模式时，在没有采用陷阱
中断的情况下访问敏感寄存器
表2-8 虚拟化时导致问题的18个 80x86 指令小结［Robin 和 Irvine 2000］
存在问的80×86指令
存储全局描述符表寄存器（SGDT）
存储局部描述符表寄存器（SLDT）
存储中断描述符表寄存器（SIDT）
存储机器状态字（SMSW）
压人标志（PUSHF、PUSHFD）
弹出标志（POPF、POPFD）
在以用户模式访问虚拟存储器机制时，
指今未能进行80x86保护检查
从分段描述符载人存取权限（LAR）
从分段描述符载入分段界限（LSL）
验证分段描述符是否可读（VERR）
验证分段描述符是否可写（VERW）
弹至分段寄存器（POP CS、POP SS，）
压入分段寄存器（PUSH CS、PUSH SS， ….）
调用不同的权限级别（CALL）
返回不同的权限级别（RET）
跳至不同的权限级别（JMP）
软件中断（INT）
存储分段选择器寄存器（STR）
移至/移出分段寄存器（MODVE）

*上面-组中的前 5 个指今允许一个以用户模式运行的程序在没有产生陷阱中断的情况下读取控制寄存器，比如描述
符表寄存器。弹出标志指今修改拥有敏感信息的控制寄存器，但在用户模式上会失敗，不会給出提示消息。80x86分
段体系结构的保护检童在下面一组中，在读取一个控制寄存器时，这些指令都会隐式检查权限级别，这一检查将作
为执行过程的组成部分。这一检查过程假定必须以最高权限級别运行，而来宾 VM并非如此。只有 MOVE 到分段寄
存器才会尝试修改控制状态，保护检查功能也会阻止它。

对80x86来说，1/O的虚拟化也是一项挑战，部分原因在于它既支持存储器映射1/O，也拥
有独立的1/O指令，但更重要的原因在于PC拥有数目庞大、种类繁多的设备和设备驱动程序，
需要 VMIM 进行处理。第三方供应商提供他们自己的驱动程序，它们也许不能正确地虚拟化。
传统 VM 实现提出的一种解决方案是将实际设备驱动程序直接载入到 VMM中。

为了简化 80x86上的 VMM实现，ADM 和 Intel都对体系结构进行了扩展。Intel 的VT-x为
运行 VM提供了一种新的运行模式，VM状态的体系结构定义、快速切换 WM的指令，还有一
大组参数，用于在必须调用 VMIM时选择环境。VT-x总共为 80x86 添加了11种新指令。AMID
的安全虚拟机（SVM）提供了类似功能。

在打开支持VT-x的模式之后（通过VMXON 指令），VT-x 为来宾操作系统提供了4种权限级
别，它们的优先级要低于原来的4个级别（解决了前面提到的 POPF 指令的一些问题）。在虚拟
机控制状态（VMCS）中，VT-x捕获一个虚拟机的全部状态，然后提供了用于保存和恢复 VMCS
的原子指令。除了关键状态之外，VMCS包含一些配置信息，用于判断什么时候调用 VMM，
以及是什么导致了 VMM 的调用。为了减少必须调用 VMM 的次数，这种模式添加了一些敏感
寄存器的影子版本，并添加了一些掩码，用于判断一一个敏感寄存器的关键位是否会在捕获之前
发生改变。为了减少虚拟化虚拟存储器的成本，AMD的SVM另外添加了一个间接层级，称为
嵌套页表。有了它就不再需要影子页表了。

\section{结语：展望}
在过去三十年里，已经多次有人预测计算机的性能提升将会停止。这些预测都错了，
其原因在于它们未经证实的假设依据都被后来的事实推翻。例如，由于未能预测到从离散
组件到集成电路的转变，从而错误地预测到光速将会限制计算机的速度，预测值要比现在
的实现际低几个数量级。我们对存储器墙（memory wall）的预测也可能是错误的，但它提
醒我们必须开始换种考虑方式了。

［129］
--Wm.A. Wulf和 Sally A. McKee
Hitting the Memor, Wall: Implications ofthe Obvious
弗吉尼亚州大学计算机系（1994年12月）
这篇论文中提出了存储踞墻一词。

使用存储器层次结构的可能性可回溯到20世纪40年代后期到50年代早期通用数字计算机
的最早时期。20世纪60年代早期，在研究计算机中引入了虚拟存储器，70年代引人了IBM大
型机。缓存大约在同一时间出现。随着时间的推移，这些基本概念已经进行了扩展和延伸，以
帮助缩小主存储器与处理器在访问时间方面的差距，但基本概念没有根本性变化。

有一个趋势可能会导致存储器层次结构设计的巨大改变，那就是 DRAM 密度和访问时间的
持续变缓。在过去十年里，人们已经观察到这两种趋势。尽管DRAM 带宽有所提高，但访问时
间的缩短速度要慢得多，部分原因是为了限制功耗，一直在降低电平。为了提高带宽，人们正
在研究一种思路：在每一组存储器上重香多个访问操作。这样就为增加分组数目提供了替代方
案，同时还可以提供较高的带宽。传统的 DRAM设计在每个单元中使用电容器，通常将它们放
在一个深沟道中，这种设计在制造方面也有一些困难，从而减缓了DRAM密度的增长速度。在
本书英文版付印时，有一家制造商发布了种不需要电容器的新 DRAM，这也可能为 DRAM
技术的持续发展提供了契机。

闪存不受 DRAM 发展的限制，由于在功率和密度方面的潜在优势，很可能会扮演更重要的
角色。当然，在 PMID中，闪存已经代替了磁盘驱动器，提供了许多桌面计算机不能提供的优
势，比如“即时启动”。闪存相对于 DRAM 的潜在优势（不需要逐位晶体管来控制写操作）也
正是它自己的“阿喀琉斯脚跟”（唯一致命弱点）。闪存必须采用批擦除重写周期（其速度相当
缓慢）。因此，一些 PMD，比如 Apple iPad 将较小的SDRAM 主存储器与闪存结合使用，既充
当文件系统，也充当页存储系统，用于处理虚拟存储器。

此外，几种全新的存储器技术也正在研究之中，其中包括 MRAM，它采用磁技术存储数据，
还有相变 RAM（称为PCRAM、PCME 和 PRAM），它采用一种能够在非晶态和晶态之间进行
变化的玻璃。这两种存储器都是永久性的，其密度有可能高于 DRAM。这些并不是什么新思路，
磁阻存储器技术和相变存储器的出现已经有几十年了。任何一种技术都可以取代目前的闪存，
而取代 DRAM 的难度要大得多。尽管 DRAM的发展速度已经减缓，但由于无电容器单元的可
能性及其他潜在改进，至少在接下来的十年中不会轻易抛弃 DRAM。

几年来，人们对存储器墙的到来进行了各种预测（见前面摘引的论文），声称这可能会导
致处理器性能的降低。但是，多级缓存的扩展、更高级的填充与预取方案、编译器及程序员对
局域特性重要程度的更深入理解、使用并行机制来克服延迟的影响，所有这些都阻挡了存储器
墻的到来。在基于缓存的系统中仍然存在存储器延迟，会导致缺失的出现，而在有多处缺失尚
未解决时采用乱序流水线，就可以利用指令级并行机制来消除这些延迟的影响。而多线程及更
多线程级并行机制的引入则更进一步，提供了更多的并行机制，因此也就提供了更多克服延迟
影响的机会。指令级、线程级并行机制的应用，是对抗现代多级缓存系统中各种存储器延迟的
主要工具。

一个时不时就会冒出来的想法是使用由程序员控制的高速暂存存储器（scratchpad）或其他
高速存储器，我们后面将会看到GPU中使用了这些存储器。这些想法之所以没有成为主流，有
几个原因：第一，它们引入了具有不同行为特性的地址空间，打破了存储器模型。第二，高速
暂存存储器的存储器转换不同于基于编译器或基于程序员的缓存优化方式（比如预取），它们必
须完全处理从主存储器地址空间到高速暂存存储器地址空间的重新映射。这就增加了此类转换
的难度，限制了它的适用范围。在GPU中（见第4章）大量使用了本地高速暂存存储器，而管
理这些存储器的重担现在落在了程序员的肩上。

尽管人们在预测计算技术的未来时一定要非常小心，但历史已经证明，缓存技术是一种强
有力的、高度可扩展的思想，它有可能让我们继续建造更快速的计算机，确保存储器层次结构
能够提供系统正常运转所需要的指令和数据。

\section{历史回顾与参考文献}
附录L.3节研究了缓存、虚拟存储器和虚拟机的历史。IBM在所有这三种技术的历史上都
扮演着重要角色。这一节还包含了供扩展阅读的参考文献。

1130
［37
132
100
弔2平
案例研究与练习（Norman P. Jouppi、Naveen Muralimanohar 和
Sheng Li设计）
案例研究1：通过高级技术优化缓存性能
本案例研究说明的概念
\begin{itemize}
    \item 无阻塞缓存
    \item 缓存的编译器优化
    \item 软件和硬件预取
    \item 缓存性能对更多复杂处理器计算性能的影响
\end{itemize}
矩阵的转置就是交换它的行与列；说明如下：
「A11 A12 A13 A147
A21 A22 A23 A24
A31 A32 A33 A34
A41 A42 A43 A44」
「A11A21 A31 AAI
A12 A22 A32 A42
A13
A23
A33
A43
A14 A24 A34 A44」
下面是一段显示转置运算的简单C循环：
for （i= 0; is 3; i++）｛
for （j= 0;j≤ 3; j+t）｛
output［J］［i］ - input［i］［s］；
假定输人与输出矩阵都以行主序存储（行主序意味着行索引的变化最快速）。假定我们正在一个处
理器上执行256 ×256双精度转置，该处理器带有一个16 KB 完全相联（不用担心缓存冲突）、最近应用
最少（LRU）替换的LI 数据缓存，块大小为64字节。假定L.I缓存缺失或预取需要16个周期，而且总会
在L.2缓存中命中，L2缓存每两个处理器周期可以处理一个请求。假定数据存在于L1缓存时，上述内层
循环的每次迭代需要4个周期。假定该缓存对写缺失采用写人分派（write-allocate）、写时取（fetch-on-write）
策略。尽管不太现实，但我们假定写回脏缓存块需要0个周期。

2.1 ［10/15/15/12/20］<2.2>对于上面给出的简单实现，这种执行顺序对输人矩阵来说不是理想顺序，
但如果进行循环交换优化会为输出矩阵生成一种非理想状态。由于循环交换不足以提高其性能，
因此必然会被阻塞。

2. ［10］<2.2>为发挥分块执行的优势，缓存的最小容量应当为多少？
b.［15］ <2.2>与以上最小容量的缓存相比，分块和非分块版本的相对缺失数如何？
c.［15］<2.2>编写代码，执行BxB块的转置，其中B为块大小参数。
d. ［12］<2.2>为了保持缓存性能的一致性，使其不受两个数组在存储器中的位置影响，LI缓存
需要的最小相联度为多少？
e.［20］<2.2>在一台计算机上尝试阻塞式与非阻塞式256×256矩阵转置。根据你对计算机存储
系统的了解，结果与预期的吻合程度如何？如果可能，请解释它们之间的差异性。
2.2
［10］<2.2>假定你正在为上述非阻塞矩阵转置代码设计硬件预取器。最简单的硬件预取器仅在
发生缺失之后预取连续缓存块。更复杂的“非单位步幅”硬件预取器可以分析一个鋏失引用流，
检测并预取超过非单位步幅的块。与之相对，软件预取可以像判断单位步帽一样，轻松地判断
非单位步幅。假定预取内容直接写人缓存，所以不存在“污染”（改写在预取数据之前必须使
用的数据）。对于给定非单位步幅预取器，为了获得最佳性能，在内层循环的稳定状态中，在
给定时刻必须有多少个等待完成的预取操作？
采例物光习练 （Nonan
r7 DnengLI仪巧T/
1U1
2.3
［15/20］<2.2>在来用软件预取时，非常重要的是要保证两点，一方面要及时进行预取以供使用，
另一方面要在最大程度上减少待处理预取的数目，使之不会超出微体系结构的能力范围，并将
缓存污染降至最低。由于不同处理器的性能和局限性也都不同，从而使情况变得复杂。
2. ［15］<2.2>采用软件预取，生成矩阵转置的一个阻塞版本。
b.［20］<2.2>估计和对比阻塞与非阻塞转置代码在有无软件预取情况下的性能。
案例研究2：融会贯通：高度并行存储器系统
本案例研究说明的概念
口 交叉问题：存储器层次结构的设计
代码清单2-1中的程序可用于评估一个存储器系统的行为。其关键在于拥有精确的定时，使程序能够
穿过存储器，调用层次结构的不同层级。代码清单2-1给出了用C语言编写的代码。第一部分是一个过程，
它使用一种标准实用程序获取用户CPU 时间的准确测量值；为了能够在某些系统上运行，可能需要对这
一过程进行修改。第二部分是一个嵌套循环，用于以不同的步幅和缓存大小来读写存储器。为了获得准确
的缓存定时，这一代码要重复许多次。第三部分仅确定嵌套循环开销的时间，以便能够从总测量时间中扣
取该时间，以了解访问时间的长短。其结果以.csv 文件格式输出，便于导人电子表格中。根据我们要回答
的问题以及被测系统上存储器的大小，可能需要修改\verb|CACHE_MAX|。以单用户模式运行此程序，或者至少在
没有其他活动应用程序的情况下运行此程序，可以获得更为一致的结果。代码清单2-1 中的代码源自加州
大学伯克利分校 Andrea Dusseau编写的一段程序，在 Saavedra-Barrera ［1992］有其详尽说明。原来的程序
在现代计算机上运行时会有许多问题，为了克服这些问题并使其能够在 Microsott Visual C++环境下运行，
已经对原程序进行了一些修改。该程序可以从 \verb|www.hpl.hp.com/research/cact/aca_ch2_cs2.c|下载。
代码清单2-1，用于评估存储器系统的C程序
\begin{verbatim}
    #inc!ude "stdatx.h"
    #include sstdio.hz
    #include
    time.hz
    *define ARRAY_MIN （1024）
    #define ARRAY_MAX
    seconds（） （ /* routine to read time in seconds */
    Eime64（"&1time
    Feturn（doubie） Itime；
    text labels */
    （ixle3）
    else if
    else if Cicleg）
    printfl"&1dM，
    eise printf（"%id6，"，i/1073741824）；
    return 0：
    int
    int csize；
    Jouble steps, tsteps；
    louble ioadtime, Tastsec,sec0, secl, seci /* timing variables */
    For （stride=i: stride <= ARRAY MAX/2; stride=stride*2）
    abel （stride*sizeof（int））
    rintf（"n"）：
    for each configuration
    【csize=ARRAY MIN；
    csize A= ARRAV hAx:csize=csize*2）｛
    label（csize*sizeof（int））；/
    prinit cache size this
    for （stride=i; stride as'csize/2; stride=stride*2） ！oop w/
    x［index-stride］
    * Wait for timer to roll over *
    seconds（）：
    Se gePSaeeef!hile （secO = 1astsec）：
    /* walk through path in array for twenty seconds */
    1VL
    付厢飛
    （Fslde:1-o. 7.gt *Reeep Sampies same*，
    nextstep
    x［nextstep］；/* dependency */
    while （nextstep l- 0）；
    steps
    count 1oop iterations */
    secl -. get
    end timer */
    ｝ while （（Secr-
    sec = seci
    133
    34
    135
    repeatuntil same
    # index + stride；
    Lsteps - tsteps+
    ｝while （tstepscsteps）；
    ierations*/
    outnesultsin
    .csy format for
    printf（"84.1f，
    （loadtimes0.1） ？ 0.1 : 1oadtime）：
    ｝：/*
    end of inner for loop */
    printf（"In"）；
    ｝；/* end of outer for 1oop */
    return O；
\end{verbatim}
上述程序假定程序地址与物理地址一致，在少数使用虚拟寻址缓存的计算机上，比如 Alpha 21264，
这一假设是成立的。一般情况下，在计算机重新启动后的很短时间内，虚拟地址会与物理地址保持一致，
所以为了在结果中获得平滑曲线，可能需要重启计算机。为了回答以下问题，假定存储器层次结构中所有
组件的大小都是2的幕。假定在第二级缓存中（如果存在二级缓存的话），页的大小要远大于块的大小，
第二级缓存块的大小大于或等于第一级缓存中的块大小。在图 2-21 中绘出了该程序的一个输出示例；图
例中列出了所使用数组的大小。
2.4
［12/12/12/10/12］<2.6~使用图2-21 中的程序结果示例回答以下问题。
a. ［12］<2.6>第二级缓存的总大小和块大小为多少？
b.［12］<2.6>第二级缓存的缺失代价为多少？
c. ［12］<2.6>第二级缓存的相联度为多少？
d. ［10］<2.6>主存储器的大小是多少？
e.［12］<2.6>如果页大小为4KB，而分页时间为多少？
2.5
［12/15/15/20］<2.6>根据需要修改代码清单2-1中的代码，以测量以下特性。以y轴为经过时间，
x轴为存储器步幅，绘制试验结果曲线。两个轴均采用对象刻度，为每种缓存大小绘制一条曲线。
a.［12］<2.6>系统页大小为多少？
b. ［15］<2.6>转换旁视缓冲区（TLB）中有多少项？
c.［15］<2.6>TLB 的缺失代价为多少？
d. ［20］ <2.6>TLB 的相联度为多少？
2.6
［20/20］<2.6>在多处理器存储系统中，单个处理器也许不能填满存储器层次结构的较低层级，
但多个一同工作的处理器也许能够填满。修改代码清单 2-1中的代码，同时运行多个副本。你
能否作出以下判断。
a.［20］<2.6~你的计算机系统中实际有多少个处理器？多少系统处理器只是额外的多线程上下文？
b.［20］<2.6>你的系统有多少存储器控制器？
2.7
［20］<2.6>你能否想出一种方法，使用程序来测试指令缓存的某些特性？提示：编译器可以由一
段代码生成大量不太容易看透的指令。尝试使用你所用指令集体系结构（ISA）中一些长度已
知的简单算法指令。
宋例听光勻练4（Normanr.
r种shengLI攻
103
1000
100
10
 8K
-16K
32K
64K
128K
256K
4 512K
1M
2M
4M
8M
I6M
32M

64M
.128M
+ 512M
4B
16B
64B
256B
IK
4K
16K
64K
歩幅
256K
1M
4M
16M
64M
團2-21 代码清单2-1中程序的输出举例
256M
练习
2.8
2.9
［12/12/15］<2.2以下问题利用CACTI研究小而简单的缓存产生的影响，假定采用65 nm （0.065wm）
工艺。（CACTI 可以从网上获取：http://quid.hpl.hp.com:9081/cacti/）。
a. ［12］<2.2>对比块大小为64字节的64 KB缓存与单组存储器的访问时间。与直接映射的组织
方式相比，两路与四路组相联缓存的相对访问时间为多少？
b.［12］<2.2>对比块大小为64字节的四路组相联缓存与单组存储器的访问时间。与16 KB缀存
相比，32KB与64KB缓存的相对访问时间是多少？
c.［15］<2.2>对于64KB缓存和特定工作负载，每条指令的缺失数据如下：直接映射为 0.00664、
两路组相联为 0.00366、四路组相联为0.00987、八路组相联缓存为0.000266，求具有最短平
均存储器访问时间的缓存相联度（介于1 至8之间）。从整体来看，每条指令有0.3次数据
引用。假定缓存缺失在所有模型中均耗费10ns。为了以周期为单位计算命中时间，假定使
用CACTI输出周期时间，它对应于在流水线中没有气泡时，缓存的最高工作频率。
［12/15/15/10］<2.2>你正在研究路预测L1缓存可能带来的好处。假定64KB 四路组联单组L1 数
据缓存是某个系统中的周期时间限制器。作为一种替代缓存组织方式，你正在考虑一种路预测
缓存，其模型为一个预测准确度为80%的64KB 直接映射缓存。除非另行指出，否则假定，一
次预测错误的路访问在缓存中命中需要多消耗一个周期。假定缺失率和缺失代价如2.8题（c）部
分所示。
a. ［12］<2.2>与路预测缓存相比，当前缓存的平均存储器访问时间是多少（用周期表示）？
b.［15］ <2.2>如果所有其他组件都是最短的路预测缓存周期时间工作（包括主存储器在内），使
用路预测缓存会产生什么样的性能影响？
c.［15］<2.2>路预测缓存通常仅用于为指令队列或缓冲区提供内容的指令缓存。设想一下你希望
136
104
137
为数据缓存尝试使用路预测。假定预测精度为80%，在正确路预测时发出后续操作（例如，
其他措令的数据缓存访问、相关操作）。因此，路预测错误必然需要进行流水线刷新和重新
执行陷阱中断，这些操作将需要15个周期。在采用数据缓存路预测时，每条载入指令的平
均存储器访问时间变化是正面的，还是负面的？变化量为多少？
d.［10］<2.2>作为路预测的替代方式，许多大型相联L.2缓存对标记与数据访问进行序列化，从
而只需要激活必需的数据集数组。这样可以节省功率，但增加了访问时间。为 0.065um工艺
1 MB 四路组相联缓存使用 CACTI的详尽 Web 接口，该缓存的块大小为64字节、144位读
出，1个组、只有1个读/写端口，30位标记和来用全局布线的 ITRS-HP技术。实现标记与
数据访问序列化与并行访问的访问时间比为多少？
2.10
［10/12］<2.2>有一种新的微处理器，需要研究分组LI数据缓存与流水化L1数据缓存的相对性
能。假定有一种64 KB 两路组相联缓存，其块大小为64字节。流水化缓存由三级流水构成，
类似于 Alpha 21264数据缓存的容量。分组实现方式由两个32KB两路组相联组成。使用CACTI，
并假定采用65 m （0.065 um）工艺，回答以下问题。Web 版本的周期时间输出表明缓存可以
在什么样的频率下正常工作，不会在流水线中产生气泡。
a.［10］<2.2>该缓存的周期时间与其访问时间相比为多少？该缓存将占用多少个流水级（精确到
小数点后两位）？
b.［12］<2.2>对比流水线设计与分组设计的每次访问的面积及总动态读取能耗。说明哪种设计占
用的面积较少，哪种需要的功率较多，解释其原因。
2.11［12/15］<2.2>考虑在L2缓存缺失时使用关键字优先和提前重启动。假定L2缓存的容量为
1 MB、块大小为64字节、填充路径宽16字节。假定能够以每4个处理器周期16个字节的
速度写人L2，从存储器控制器接收前16个字节块的时间为120个周期，每从主存储器接收
另外 16个字节的块需要16个周期，也可以直接将数据传送给L.2缓存的读取端口。忽略向
L.2缓存发送缺失请求及向L1缓存传送被请求数据的周期数。
a. ［12］<2.2>在使用、不使用关键字优先和提前重启动时，为L.2缓存缺失提供服务分别需要多
少个周期？
b.［15］<2.2>你是否认为关键字优先和提前重启动对于L1缓存或L.2缓存更重要一些，哪些因素
影响着它们的相对重要性？
2.12
［12/12］<2.2>在直写L1缓存与写回L.2缓存之间设计一个写缓冲区。L2缓存写数据总线的宽度
为16B，可以每4个处理器周期向一个独立缓存地址执行一次写操作。
a. ［12］<2.2>每个写缓冲区项目应当为多少字节？
b.［15］<2.2>如果所有其他指令可以与存储指令并行发射，块存在于L.2缓存中，在通过执行64
位存储指令将存储器置零时，使用一个合并写缓冲区来代替非合并缓冲区，在稳定状态下可
以得到什么样的加速比？
c.［15］<2.2>对于采用阻塞缓存与非阻塞缓存的系统，可能出现的L.1 映失对于所需写缓冲区项
目的个数有什么样的影响？
2.13 ［10/10/10］<2.3>考虑一个桌面系统，它的处理器连接到一个采用糾错码（ECC）的2GB DRAM。
假定只有一个宽度为72位的存储器通道，其中64位用于数据，8位用于ECC。
a.［10］<2.3>如果使用1 GB DRAM芯片，DIMIM上有多少个 DRAM芯片，如果仅有一个 DRAM
连接到每个 DIMM 数据管脚，每个 DRAM必须拥有多少数据I/O？
b.［10］<2.3>为了支持32BL.2缓存块，突发（burst）长度需要为多少？
c. ［10］ <2.3>计算为了从某个活动页读取内容，DDR2-667 和 DDR2-533 DIMM 的峰值带宽为多
少？不计ECC开销。
2.14 ［10/10］ <2.3>图2-22给出一个 DDR2 SDRAM 时序图示例。tRCD 是激活一组中的一行所需要的
时间，列地址选通（CAS）延迟（CL）是从一行中读出一列所需要的周期数。假定此RAM是
栗例饼究与练（Norman
ir和 ShengLi设什）
105
\begin{verbatim}
    在具有ECC的标准 DDR2 DIMIM上，拥有72个数据行。另外假定突发长度为8，它从 DIMM
    读出8位，也就是总共64B。假定 tRCD - CAS （或CL）*clock_frequency、Clock_frequency = transfers_
    per_second/2。在发生缓存缺失时，通过第1级、第2级并返回的片上延退时间为20ms，不包
    括DRAM访问时间。
\end{verbatim}
8.［10］<2.3>对于 DDR2-667 1GB CL =5 DIMM，从出现激活命令一直到从 DRAM 请求的最后
一个数据位由有效变为无效，一共需要多少时间？假定对于每个请求，我们会自动预取同一
页中的另一个相邻缓存线。
b.［10］<2.3>在使用 DDR2-667 DIMIM 时，如果一次读取操作需要激活分组，那与读取已打开页面
相比，其相对延迟是多少？（包括在处理器内部处理该缺失所需要的时间。）
时钟
XX
CMD/
DL
BOERXY
RCD
：CAS延迟
数据
图 2-22 DDR2 SDRAM 时序图
［15］<2.3>假定 DDR2-6672 GB DIMM （CL=5）的价格为130美元，DDR2-5332GB DIMM （CL=
4）的价格为100美元。假定在一个系统中使用两个 DIMM，系统的其余组件需要800美元。对
于一个工作负载，第1000条指令出现3.33次L.2缺失，并假定所有 DRAM 读取操作中有80%
需要激活，考虑系统使用 DDR2-667和 DDR2-533 DIMM 时的性能。假定在某一时刻只有一个
L2 缺失等待处理，循序（in-order）核心的CPI为1.5，不包括L2 缓存缺失存储器访问时间，
则整个系统的性价比如何？
2.16
［12］ <2.3>你正在准备一台服务器，它采用八核 3 GHzCMP，在执行某一工作负 时的总 CPI
为2.0（假定L.2缓存缺失填充没有延迟）。L2缓存行的大小为32字节。假定该系统采用 DDR2-667
DIMIM。如果有时需要的带宽为平均带宽的2倍，那么应当提供多少个独立存储器通道才能使
系统不受存储器带宽的限制？该工作负载平均第1000条指令导致6.67次L2缺失。
2.17
［12/12］<2.3>DRAM 功率中有很大一部分是因为页面激活消耗的（超过三分之一）（请参阅
http://download.micron.com/pdf/technotes/ddr2/TN4704.pdf和 www.micron.com/systemcalc）。假定
你正在构建一个拥有2 GB存储器的系统，或者使用8组2 GB x 8 DDR2 DRAM，或者使用8
组1 GB x8 DRAM，这两者的速度相同。使用的页大小都是1KB，最后一级缓存行大小为64
字节。假定 DRAM 在未激活之前处于预充电待机状态，消耗的功率可以忽略。假定从待机状态
到活动状态的过渡时间不是很长。
2.［12］<2.3预计哪种类型的 DRAM提供的系统性能较高？解释原因。
b.［12］<2.3>由1 GBx8 DDR2 DRAM 组成的2GB DIMM 与容量相同但由1GB x4 DDR2 DRAM
组威的 DIMIM 相比，在功率方面有何差异？
2.18
［20/15/12］<2.3>为了从一个典型DRAM访问数据，必须首先激活适当的行。假定这一操作会将
大小为8 KB的整个页面发送到行缓冲区，然后从行缓冲区中选择一个特定列。如果对 DRAM
的后续访问目标也是同一页，就可以略过激活步骤；如果不是，就必须关闭当前页，对位行进
行预充电，以准备下一次激活。另一种常用DRAM策略是在访问结束之后立即主动关闭一个页，
并对位行进行预充电。假定对DRAM 的每次读取或写人都是采用64字节的大小，发送512位
的DDR总线延迟（图2-21中的数据输出）为Tddr。
138
139
106
140
T47
第2章 仔储翻除：“
2. ［20］<2.3>假定采用DDR2-667，如果它需要5个周期进行预充电、5个周期进行激活、4个周
期读取列，为了获得最短访问时间，如何根据行缓冲区命中率（r）选择策略？假定对 DRAM
的每次访问之间都有足够的时间，用以完成新的随机访问。
b.［15］<2.3>如果在对DRAM 的所有访问中有10%是一个接一个地发生，或者没有任何时间间
隙地连续发生，应当如何改变自己的决定？
c.［12］<2.3>使用上面计算的行缓冲区命中率，计算在采用两种策略时，每次访问的平均 DRAM
能耗差别。假定预充电需要2nJ，激活需要4n，从行缓冲区进行读写需要100p/位。
2.19 ［15］<2.3>每当计算机空闲时，既可以将其置于待机状态（DRAM仍然处于活动状态），也可以
让它休眠。为了使其进人休眠状态，假定必须仅将 DRAM 的内容复制到永久性介质中，比如闪
存中。如果将大小为 64字节的缓存行读写至闪存需要2.56 w，读写至 DRAM 需要0.5nJ，如
果8 GB DRAM的空闲功耗为1.6W，那么一个系统空闲多长时间后才能从休眠中获益？假定
主存储器的容量为8GB。
［10/10/10/10/10］ <2.4>虚拟机（VM）具有向计算机系统添加大量有益功能的潜力，比如降低总
拥有成本（TCO）或提高可用性。能否使用VM来提供以下功能？如果可以，如何提升？
a.［10］ <2.4>使用开发计算机测试应用程序在生产环境中的性能？
b.［10］<2.4>在发生灾难或故障时快速部署应用程序？
c. ［10］<2.4>在1/O操作密集的应用程序中获得更高性能？
d. ［10］<2.4>实现不同应用程序之间的故障隔离，提高服舒的可用性？
e.［10］<2.4>在系统上执行软件维护，同时不对正在运行的应用程序造成严重干扰？
2.21
［10/10/12/12］ <2.4>许多事件都可能导致虚拟机的性能下降，比如执行特权指令、TLB 敏失、陷
阱和 1/O。这些事件通常是在系统模式中处理的。因此，为了评估应用程序在 VM 中运行时的
减缓程度，可以计算该应用程序在系统模式下的运行时间占用户模式下执行时间的百分比。例
如，某个程序有10%的执行过程是在系统模式下完成，当它在VM 中运行时，速度可能会降低
60%。表2-9是在 Itanium 系统上使用 Xen 虚拟机，在三种不同情况下运行LMbench时，各种
系统调用的早期性能，这三种不同情况分别为：无虚拟执行、纯虚拟化和部分虚拟化，时间的
测量单位为微秒（感谢新南威尔士大学的 Matthew Chapman）。
2.［10］<2.4>预计哪种类型的程序在 VM 下运行时速度减缓较少？
b.［10］<2.4>如果速度减缓与系统时间成线性关系，给定上述减缓数据，如果一个程序有20%的
执行是在系统时间内完成，那它会减缓多少？
c.［12］<2.4>在纯虚拟化和半虚拟化条件下，上表中系统调用速度减缓的中间值为多少？
d. ［12］ <2.4>上表中哪些函数的速度减缓最严重？可能是因为什么原因？
表2-9 在无虚拟化、纯虚拟化和半虚拟化条件下各种系统调用的早期性能
\begin{verbatim}
    基准测试
    无虚拟化
    純虛拟化
    Null call
    0.04
    0.96
    Null 1/O
    0.27
    6.32
    Stat
    1.10
    10.69
    Open/close
    1.99
    20.43
    Install sighandler
    0.33
    7.34
    Handle signal
    1.69
    19.26
    Fork
    56.00
    $13.00
    Exec
    316.00
    2084.00
    Fork + exec sh
    1451.80
    7790.00
    半虚拟化
    0.5o
    2.91
    4.14
    7.71
    2.89
    2.36
    164.00
    578.00
    2360.00
    栗例究与练之（Normar ：'
    ar 和 ShengLi 攻什）
    107
\end{verbatim}
2.22 ［12］<2.4>Popek 和 Goldberg 在给出虛拟机定义时指出，性能是唯一能够将虛拟机与真实计算机
区分开来的指标。在这个问题中，我们将利用这一定义来查明是在一个处理器上以无虚拟化方
式运行，还是运行在虚拟机上。Intel VT-x技术为使用虚拟机实际提供了第二组权限级别。假定
采用VT-x技术，如果一个虚拟机要运行在另一个虚拟机的顶层，它必须做些什么？
2.23［20/25］<2.4>随着x86体系结构开始支持虚拟化，虚拟机得以快速发展，成为主流。比较 Intel Vt-x
和 AMD的 AMD-V 虚拟化技术（AMD-V 的相关信息可以参见 http://sites.amd.com/us/business/
it-solutions/virtualization/Pages/resources.aspxo）
a.［20］<2.4>对于内存印记较大、存储器操作密集的应用程序，哪一种技术的性能较高？
b.［25］<2.4>有关 AMD对虛拟化1/O的IOMMU 支持信息，可以在 http://developer.amd.com/
documentation/articles/pages/892006101.aspx 找到。虚拟化技术和输人/输出存储器管理单元
（IOMMMU）为提高虚拟化1/O性能做了哪些工作？
2.24
［30］<2.2、2.3>由于在循序超标量处理器和具有预测功能的超长指令字（VLIW）处理器上都可
以有效地开发指令级并行，所以构建乱序（000）超标量处理器的一个重要原因就是能够容忍
由于缓存缺失导致的不可预测延迟。因此，我们可以把硬件支持的000 发射看作存储器系统
的一部分！看一下图2-23 中 Alpha 21264的平面布置图，找出整数和浮点发射队列及映射器与
缓存相比的相对面积。队列调度要发射的指令，映射器重命名寄存器标识符。因此，为了支持
000发射，有必要增加这些内容。21264的芯片上只有L.1 数据与指令缓存，它们都是64KB
两路组相联。使用一种 000 超标量模拟器（比如 SimpleScalar,www.cs.wisc.edu/~mscalar/
simplescalar.html）执行存储器操作密集的基准测试，如果在循序超标量处理器中，发射队列和
映射器的区域被用于附加的L.1数据缓存区域，而不是21264模型中的000发射。确保计算机
的其他方面尽可能相似，使比较公平合理。忽略任何由于大型缓存造成访问时间或周期时间的
增长，以及大型数据缓存对芯片平面布置图的影响。（注意，这种对比不是完全公平的，因为
编译器没有为循序处理器调度这些代码。）
2.25
［20/20/20］ <2.6>Intel 性能分析器 VTune 可用于对缓存行为进行许多种测量。VTune 在 Windows 和
Limux 平台上的免费评估版本都可以从 http://software.intel.com/enus/articles/intel-vtune-amplifier-xe/
下载。它修改了案例研究 2中使用的程序\verb|（aca_ch2_cs2.c）|，使其能够在 Miorosof Visual C++上与
现成的 VTune一起工作。这个程序可以从 \verb|www.hpl.hp.com/zesearch/cact/aca._cb2_c82_vtune.c| 下
载。为了在性能分析期间扣除初始化与循环开销，已经插人了一些特的 VTune 函数。在这个
程序的 README 部分给出了详细的 VTune 安装指令。这个程序为每种配置循环20秒。在下面
的试验中，可以求出数据规模对缓存和整体处理器性能的影响。在Intel处理器上的VTune 中运
行该程序，输人数据大小分别为8KB、128 KB、4 MB 和32 MB，步幅保持为64个字节（也
就是Intel i7 处理器上的一个缓存行）。收集有关整体性能、L1数据缓存、L.2和L3缓存性能的
统计数字。
a. ［20］<2.6>对于每一种数据集大小和处理器模型与速度，列出LI 数据缓存、L2和L3 缓存中
每1000条指令的缺失数。根据结果，你可以对处理器上的L1数据缓存、12和L3缓存大小
得到什么结论？请解释你的结论。
b.［20］<2.6>对于每种数据集大小和处理器模型与速度，列出每个时钟周期执行的措令数（IPC）。
根据结果，可以对处理器上的L1、L2 和L3缺失代价得出什么结论？请解释你的结论。
c.［20］<2.6>在 Intel 000处理器上的 VTune 中运行该程序，输入数据集大小为8KB和128KB。
对于两种配置，列出每1000条指令的LI 数据缓存和L.2缓存觖失数，并给出CPI。对于高
性能000处理器上内存延迟隐藏技术的有效性，可以说些什么？提示：需要求出处理器的
L.1 数据缓存缺失代价。对于最新的 Intel i7处理器，大约为11个时钟周期。
142
108
L 仔储器会
尹单
漾衰映
徕储器
数据与捺制总线
存储爵挖制器
BIU
143
图 2-23
Alpha 21264 的平面布置图［Kessler 1999］