

\chapter{线程级并行}
计算机组织结构在 20世纪60年代中期开始偏离传统方式，
在提升计算机运行速度的努力中，回报递减定律开始发挥作
用…电子电路的运行速度最终受光速的限制⋯⋯许多电路已经
工作在纳秒级别。
一W.Jack Bouknight等人，
The Illiac IV System （1972）
我们所有未来产品的开发都专注于多核设计。我们相信这就
是这一行业的转折点。
—Intel 总裁 Paul Otellini，
在2005年度 Intel 开发人员
论坛上介绍 Intel 的未来方向

\section{引言}
正如本章开篇引语所说，很多年之前，一些研究人员就持有这样一种观点：单处理器体系
结构的发展已经接近尾声。显然，这种观点有点草率了；事实上，自20世纪50年代末、60年
代初出现第一批晶体管计算机以来，单处理器性能的增长速度在1986 年至 2003年期间达到最
高峰，其背后的推动力就是微处理器的发展。

不过，在整个20世纪90年代，多处理器的重要性一直在不断增加，设计人员试图寻求一
种构造服务器和超级计算机的方法，希望其性能优于单个微处理器。与此同时，人们还在探索
大众商品化微处理器在性价比方面的巨大优势。我们在第1章至第3章中已经讨论过，由于开
发指令级并行（IP）的回报在降低，再加上对功率因素关注程度的增加，导致了单处理器的发
展速率减缓，而这一减缓又将我们引入了计算机体系结构的一个新时代，在这个时代中，多处
理器在从低端到高端的各个领域都扮演了重要角色。本章开头的第一条引语就指出了这个明显
的转折点。

多重处理的重要性在不断提升，这反映了以下几个重要因素。

\begin{itemize}
    \item 在2000年至2005年期间，设计人员尝试寻找和开发更多的ILP，而事实表明，这种方
    法的效率很低，因功率和硅成本的增长速度要快于性能的增长速度，因此，硅与能量
    的利用效率在这一时期激剧下降。如何使性能的增长速度超过基础技术的发展速度呢？
    除了IP之外，（换个角度来看）我们所知道的唯一可伸缩、通用方式就是通过多重处理。
    \item 随着云计算和软件即服务变得越来越重要，人们对高端服务器的兴趣也在增加。
    \item 因特网上有海量数据可供利用，推动了数据密集型应用程序的发展。
    \item 人们认识到提高台式计算机性能的重要性在下降（至少图形处理功能之外的性能如此），
    要么是因为当前的性能已经可以接受，要么是因为计算高度密集、数据高度密集的应用
    程序都是在云中完成的。
    \item 人们更深入地了解到如何才能有效地利用多处理器，特别是在服务器环境中如何加以有
    效利用，这种环境中存在大量自然并行，而并行源于大型数据集、科学代码中的自然并
    行，或者大量独立请求之间的并行（请求级并行）。
    \item 通过可复用设计而不是独特设计来充分发挥设计投人的效用，所有多处理器设计都具备
    这一特点。
\end{itemize}
本章主要研究线程级并行（TLP）的开发。TLP 意味着存在多个程序计数器，因此主要通
过 MIMD 加以开发。尽管 MIMD的出现已经有几十年了，但在从嵌入式应用到高端服务器的计
算领域中，线程级并行移向前台还是最近的事情。同样，线程级并行大量用于通用应用程序而
不只是科学应用程序，也最近的事情。

这一章的重点是多处理，我们将多处理器定义为由紧耦合处理器组成的计算机，这些处
理器的协调与使用由单一处理器系统控制，通过共享地址空间来共享存储器。此类处理器通过
两种不同的软件模型来开发线程级并行。第一种模型是运行一组紧密耦合的线程，协同完成同
一项任务，这种情况通常被称为并行处理。第二种模型是执行可能由一或多位用户发起的多个
相对独立的进程，这是一种请求级并行形式，其规模要远小于将在下一章研究的内容。请求级
并行可以由单个应用程序开发（这个应用程序在多个处理器上运行，比如响应查询请求的数据
库程序），也可以由多个独立运行的应用程序开发，通常称为多重编程。

本章所研究多处理器的典型范围小到一个双处理器，大至包括数十个处理器，通过存储器
的共享进行通信与协调。尽管通过存储器进行共享隐含着对地址空间的共享，但并不一定意味
着只有一个物理存储器。这些多处理器既包括拥有多个核心的单片系统（称为多核），也包括由
多个芯片构成的计算机，每个芯片可能采用多核心设计。

除了真正的多处理器之外，我们还将再次讨论多线程主题，这一技术支持多个线程以交错
形式在单个多发射处理器上运行。许多多核处理器也包括对多线程的支持。

在下一章，我们将研究由大量处理器构建而成的超大规模计算机，这些处理器通过联网技
术连接在一起，通常称为集群；这些大规模系统通常用于云计算，在其所采用的模型中，要么
假定有大量独立请求，要么假定有高度并行、密集计算的任务。当这些集群发展到数万个服务
器时，我们就称之为仓库级计算机。

除了这里研究的多处理器和下一章的仓库级系统之外，还有一些特殊的大规模多处理器，
有时称之为多计算机，它们的耦合程度要低于本章研究的多处理器，但高于下一章的仓库级系
统。这种多计算机主要应用于高端科学计算。有许多其他书籍对这些系统进行了详细介绍，比
如Cullr、Singh 和 Gupta［1999］。由于多重处理领域不但规模庞大，而且还在不断变化之中（刚
刚提到的Culler等人撰写的参考文献有1000多页，但却只讨论了多重处理！），所以我们选择将
注意力放在我们认为是计算领域中最重要的通用部分上。附录I结合大规模科学应用程序，讨
论了在构建此类计算机时出现的一些问题。

综上所述，我们的焦点是含有中小量处理器（2~32个）的多处理器。无论是在数量方面，
还是在金额方面，此类设计都占据着主导地位。对于更大规模的多处理器设计领域（33个或更
多个处理器），我们仅给予一点点关注，主要在附录I中对其进行讨论，其中包含了此类处理器
设计的更多方面，以及并行科学工作负载的行为性能，这类科学运算是大规模多处理器的一类
主要应用。在大规模多处理器中，互连网络是设计的关键部分；附录F主要讨论了这一主题。

\subsection{多处理器体系结构：问题与方法}
为了充分利用拥有n个处理器的MIMD 多处理器，通常必须拥有至少n个要执行的线程或
进程。单个进程中的独立线程通常由程序员确认或由操作系统创建（来自多个独立请求）。在另
一极端情况下，一个线程可能由一个循环的数十次迭代组成，这些迭代是由开发该循环数据并
行的并行编译器生成的。指定给一个线程的计算量称为粒度大小，尽管这一数值在考虑如何高
效开发线程级并行时非常重要，但线程级并行与指令级并行的重要定性区别在于：线程级并行
是由软件系统或程序员在较高层级确认的；这些线程由数百条乃至数百万条可以并行执行的指
令组成。

线程还能用来开发数据级并行，当然，其开销可能高于使用SIMD处理器或GPU的情况（见
第4章）。这一开销意味着粒度大小必须大到能够足以高效开发并行。例如，尽管向量处理器或
GPU 也许能够高效地实现短向量运算的并行化，但当并行分散在许多线程之间时，粒度大小会
非常小，导致在 MIMD 中开发并行的开销过于昂贵，无法接受。

根据所包含的处理器数量，可以将现有共享存储器的多处理器分为两类，而处理器的数量
又决定了存储器的组织方式和互连策略。我们是按照存储器组织方式来称呼多处理器的，因为
处理器的数目是多还是少，是可能随时间变化的。

第一类称为对称（共享存储器）多处理（SMP），或集中式共享存储器多处理器，其特点
是核心数目较少，通常不超过8个。由于此类多处理器中的处理器数目如此之小，所以处理器
有可能共享一个集中式存储器，所有处理器能够平等地访问它，这就是对称一词的由来。在多
核芯片中，采用一种集中方式在核心之间高效地共享存储器，现有多核心都是SMP。当连接一
个以上的多核心时，每个多核心会有独立的存储器，所以存储器为分布式，而非集中式。

SMP 体系结构有时也称为一致存储器访问（UMA）多处理器，这一名称来自以下事实：所
有处理器访问存储器的延迟都是一致的，即使存储器的组织方式被分为多个组时也是如此。图
5-1 展示了这类多处理器的基本结构。SMP的体系结构将在5.2节讨论，我们将结合一种多核心
来解释这种体系结构。


图 5-1
基于多核芯片的集中式共享存储器多处理器的基本结构。多处理器缓存子系统共享同一物理存储
器，通常拥有一级共享缓存、一或多级各核心专用缓存。这一结构的关键特性是所有处理器对所
有存储器的访问时间一致。在多芯片版本中，将省略共享缓存，将处理器连接至存储器的总线或
互连网络分布在芯片之间，而不是一块芯片内部

在另一种设计方法中，多处理器采用物理分布式存储器，称为分布式共享存储器（DSM）。
图5-2展示了此类多处理器的基本结构。为了支持更多的处理器，存储器必须分散在处理器之
间，而不应当是集中式的；否则，存储器系统就无法在不大幅延长访问延迟的情况下为大量处
理器提供带宽支持。随着处理器性能的快速提高以及处理器存储器带宽需求的相应增加，越来
越小的多处理器都优选采用分布式存储器。®多核心处理器的推广意味着甚至两芯片多处理器都
采用分布式存储器。处理器数目的增大也提升了对高宽带互连的需求，在附录F可以看到一些
例子。直联网络（即交换机）和间接网络（通常是多维网络）均被用于实现这些互连。

① 这里说的“越来越小”是指多处理器中包含的处理器数目或核心数目。—译者注

图5-2
2011 年的分布式存储多处理的基本体系结构通常包括一个带有存储器的多核心多处理器芯
片，可能带有1/O 和一个接口，连向连接所有节点的互连网络。每个处理器核心共享整个存储器，
当然，在访问隶属于该核心芯片的本地存储器时，其速度要远远高于访问远端存储器的速度

将存储器分散在节点之间，既增加了带宽，也缩短了到本地存储器的延迟。DSM多处理器
也被称为 NUMA（非一致存储器访问），这是因为它的访问时间取决于数据字在存储器中的位
置。DSM 的关键缺点是在处理器之间传送数据的过程多少变得更复杂了一些，DSM需要在软
件中多花费一些力气，以充分利用分布式存储器提升的存储器带宽。因为所有多核心多处理器
（处理器芯片或插槽多于一个）都使用了分布式存储器，所以我们将从这个角度来解释分布式存
储器多处理器的工作方式。

在SMP和DSM 这两种体系结构中，线之间的通信是通过共享地址空间完成的，也就是
说，任何一个拥有正确寻址权限的处理器都可以向任意存储器位置发出存储器引用。与 SMIP 和
DSM相关联的共享存储器一词是指共享地址空间这一事实。

与之相对，下一章的集群和仓库级计算机看起来更像是由网络连接的独立计算机，如果两
个处理器之间没有运行软件协议加以辅助，那一个处理器就无法访问另一个处理器的存储器。
在此类设计中，利用消息传送协议在处理器之间传送数据。
\subsection{并行处理的挑战}
多处理器的应用范围很广，既可用于运行基本上没有通信的独立任务，也可以运行一些必
须在线程之间进行通信才能完成任务的并行程序。有两个重要的障碍使并行处理变得极富挑战
性（这两个障碍都可以用 Amdabl 定律来解释）。这些障碍的难易程度是由应用方式和体系结构
来确定的。

第一个障碍与程序中有限的可用并行相关，第二个障碍源于通信的成本较高。由于可用并
行的有限性，所以很难在所有并行处理器中都实现良好的加速比，我们的第一个示例就来展示
这一点。

例题
解答
假定希望用100个处理器获得80倍的加速比。原计算中的串行部分可以占多大比例？
回想第1章的 Amdahl定律：
加速比二一华强比例＋（1一增强比例）
增强加速比
为筒单起见，假定此程序仅以两种模式运行，一种模式是并行方式，所有处理器
都得到充分应用，这是一种增强模式；另一种模式是申行方式，仅使用一个处理
器。通过这一筒化，增强模式下的加速比就是处理器的数目，增强模式的比例就
是并行模式中花费的时间。代人上式得：
1
80=—
并行部分所占比例＋《1-并行部分所占比例）
化简得：
0.8×并行部分所占比例+80×（1-并行部分所占比例）=1
80-79.2×并行部分所占比例=1
80-1
并行部分所占比例=-
79.2
并行部分所占比例=0.9975
为了以100个处理器实现80倍的加速比，原计算中只有0.25%可以是申行的。当
然，为实现线性加速比（n个处理器的加速比为n），那整个程序就必须都是并行
的，没有串行部分。在实践中，程序不会仅在完全并行模式下运行，也不会仅在
完全申行模式下运行，而是运行在未利用全部处理器的并行模式下。

并行处理的第二个重要挑战涉及并行处理器进行远程访问所带来的长时延迟。在现有的共
享存储器多处理器中，分离核心之间的数据通信通常需要耗费35~50个时钟周期，分离芯片上
的核心之间进行数据通信可能耗费100个时钟周期到500甚至更多个时钟周期（对于大规模多
处理器而言），具体取决于通信机制、互连网络的类型以及多处理器的规模。长时间的通信延迟
显然会造成实际影响。让我们看一个简单的例子。

例题
假定有一个应用程序运行在包含32个处理器的多处理器上，它在引用远程存储器
时需要的时间为200ns。对于这一应用程序，假定除涉及通信的引用之外，其他所
有引用都会在本地存储器层次结构中命中，这一假定稍微有些乐观了。处理器会
在远程请求时停顿，处理器时钟频率为3.3 GHz。如果基础CPI（假定所有引用都
在缓存中命中）为0.5，请对比在没有通信、0.2%的指令涉及远程通信引用这两种
情況下，多处理器会快多少？
解答
首先计算每条指令占用的时钟周期数更容易一些。当涉及 0.2%的远程引用时，多
处理器的实际 CPI为：
CPI=基础 CPI+远程请求率×远程请求成本
=0.5+0.2%×远程请求成本
1349
351
262
第5章 线程級并行
远程请求成本为：
350
远程访问成本
周期时间
200 ns
-=666个周期
0.3 ns
因此，我们可以得出 CPI：
CPI=0.5+1.2=1.7
当所有引用均为本地引用时，多处理器要快出 1.7/0.5=3.4 倍。实际的性能分析要
复杂得多，因为某些非通信引用会在本地层次结构中缺失，远程访问时间也不是
一个常数值。例如，在进行远程引用时，由于许多引用试图利用全局互连从而导
致争用，增大延迟，从而使远程引用的成本大幅增加。

这些问题（并行度不足、远程通信延迟太长）是多处理器应用中最大的两个性能难题。应
用程序并行度不足的问题必须通过软件来解决，在软件中采用一些能够提供更佳并行性能的新
算法，而且软件系统应当尽量利用所有处理器来运行软件。远程延迟过长而导致的影响可以由
体系结构和程序员来降低。例如，我们可以利用硬件机制（比如缓存共享数据）或软件机制（比
如重新调整数据的结构，增加本地访问的数量）来降低远程访问的频率。我们可以利用多线程
（在本章后面讨论）或利用预取（在第2章对这一主题进行了广泛讨论）来尝试容忍这些延迟。

本章主要关注的技术是用来降低远程通信延迟过长导致的影响。例如，5.2节至5.4节讨论
了如何使用缓存来降低远程访问频率，并同时保持存储器的一致性。5.5节讨论了同步，由于同
步本来就与处理器之间的通信关联在一起，而且还可能会限制并行，所以它是一个重要的潜在
瓶颈。5.6节介绍隐藏延迟的技术和共享存储器的存储器一致性模型。在附录I中，我们主要关
注更大规模的多处理器，它们绝大多数用于科学工作。这个附录将研究此类应用的本质，以及
用数十个乃至数百个处理器来实现加速的挑战。

\section{集中式共享存储器体系结构}
人们发现，使用大型、多级缓存可以充分降低处理器对存储器带宽的需求，这一重要发现
刺激了集中式存储器多处理器的发展。最初，这些处理器都是单核心的，经常会占据整个主板，
存储器被放置在共享总线上。后来，更高性能的处理器、存储器需求超出了一般总线的能力，
最近的微处理器直接将存储器连接到单一芯片中，有时称其为后端或存储語总线，以便将它与
连接至1/O的总线区分开来。在访问一个芯片的本地存储器时，无论是为了1/O操作，还是为了
从另一个芯片进行访问，都需要通过“拥有”该存储器的芯片。因此，对存储器的访问是非对
称的：对本地存储器的访问更快一些，而对远程存储器的远程要慢一些。在多核心结构中，存
储器由一个芯片上的所有核心共享，但是从一个多核心的存储器访问另一个核心的存储器时，
仍然是非对称的。

采用对称共享存储器的计算机通常支持对共享数据与专用数据的缓存。专用数据供单个处
理器使用，而共享数据则由多个处理器使用，基本上是通过读写共享数据来实现处理器之间的
通信。在缓存专用项目时，它的位置被移往缓存，缩短平均访问时间并降低所需要的存储器带
宽。由于没有其他处理器使用数据，所以程序行为与单处理器中的行为相同。在缓存共享数据
时，可能会在多个缓存中复制共享值。除了降低访问延迟和所需要的存储器带宽之外，这一复
制过程还可以减少争用，当多个处理器同时读取共享数据项时可能会出现这种争用。不过，共
享数据的缓存也引人了一个新问题：缓存一致性。
\subsection{什么是多处理器缓存一致性}
遗憾的是，缓存共享数据会引人一个新的问题，这是因为两个不同的处理器是通过各自的
缓存来保留存储器视图的，如果不多加防范，它们最终可能会看到两个不同值。表5-1展示了
这一问题，并说明两个不同处理器如何将同一位置的内容看作两个不同值。这一难题一般被称
为缓存一致性问题。注意，存在一致性问题是因为既拥有全局状态（主要由主存储器决定），又
拥有本地状态（由各个缓存确定，它是每个处理器核心专用的）。因此，在一个多核心中，可能
会共享某一级别的缓存（比如L3），而另外一些级别的缓存则是专用的（比如L1和L2），一致
性问题仍然存在，必须加以解决。

表5-1 由两个处理器（A和B）进行读写时，单一存储位置（X）的缓存一致性问题
事件
处理器A的缀存内容
处理器B的缀存内容
时间
0
位置X的存储内容
1
1
处理器A读取X
1
2
处理器B读取×
3
处理器A将0存储到×
0

*我们最初假定两个缓存都没有包含该变量，X的值为1。我们还假定来用直写缓存，写回缝存金增加一些复杂性，但
与此类似。在处理器A写入X值后，A的缓存和存储器中都包含了新值，個B的缓存中淡有，如果B读取X的值，
将会收到数儢1！

通俗地说，如果在每次读取某一数据项时都会返回该数据项的最新写入值，那就说这个存
储器系统是一致的。尽管这一定义看起来是正确的，但它有些含混，而且过于简单；实际情况
要复杂得多。这一简单定义包含了存储器系统行为的两个方面，这两个方面对于编写正确的共
享存储器程序都至关重要。第一个方面称为一致性（coherence），它确定了读取操作可能返回什
么值。第二个方面称为连贯性（consistency），它确定了一个写人值什么时候被读取操作返回。
首先来看一致性。
如果存储器系统满足以下条件，则说它是一致的。

\begin{enumerate}
    \item 处理器P读取位置X，在此之前是由P对又进行写人，在P执行的这一写人与读取操
    作之间，没有其他处理器对位置X执行写入操作，此读取操作总是返回P写人的值。
    \item 一个处理器向位置 X 执行写入操作之后，另一个处理器读取该位置，如果读写操作的
    间隔时间足够长，而且在两次访问之间没有其他处理器向X写入，那该读取操作将返回写入值。
    \item 对同一位置执行的写人操作被串行化，也就是说，在所有处理器看来，任意两个处理器
    对相同位置执行的两次写人操作看起来都是相同顺序。例如，如果数值1、数值2被依次先后
    写到一个位置，那处理器永远不可能先从该位置读取到数值2，之后再读取到数值1。
\end{enumerate}
第一个特性只是保持了程序顺序—即使在单处理器中，我们也希望具备这一特性。第二
个特性定义了一致性存储器视图的含义：如果处理器可能持续读取到一个旧数据值，我们就能
明确地说该存储器是不一致的。

对写操作审行化的要求更加微妙，但却同等重要。假定我们没有实现写操作的申行化，而
且处理器P1 先写入地址X，然后是P2写人地址X。对写操作进行串行化可以确保每个处理器
在某一时刻看到的都是由P2写人的结果。如果没有对写人操作进行串行化，那某些处理器可能
会首先看到P2的写人结果，然后看到P1 的写人结果，并将P1写人的值无限期保存下去。避免
此类难题的最简单方法是确保对同一位置执行的所有写入操作，在所有处理器看来都是同一顺
序；这一特性被称为写入操作串行化。

尽管上述三条属性足以确保一致性了，但什么时候才能看到写人值也是一个很重要的问题。
比如，我们不能要求在某个处理器向X中写人一个取值之后，另一个读取 X的处理器能够马上
看到这个写人值。比如，如果一个处理器对X的写人操作仅比另一个处理器对X的读取操作提
前很短的一点时间，那就不可能确保该读取操作会返回这个写人值，因为写人值当时甚至可能
还没有离开处理器。写人值到底在多久之后必须能被读取操作读到？这一问题由存储器连贯性
模型回答，5.6节将讨论这一主题。

一致性和连贯性是互补的：一致性确定了向同一存储器位置的读写行为，而连贯性则确定
了有关访问其他存储器位置的读写行为。现在，作出以下两条假定。第一，只有在所有处理器
都能看到写入结果之后，写人操作才算完成（并允许进行下一次写人）。第二，处理器不能改变
有关任意其他存储器访问的任意写人顺序。这两个条件是指：如果一个处理器先写人位置A，
然后再写人位置B，那么任何能够看到B中新值的处理器也必须能够看到A的新值。这些限制
条件允许处理器调整读取操作的顺序，但强制要求处理器必须按照程序顺序来完成写入操作。
在5.6节之前将一直采用这一假定，届时将会看到这一定义的内涵以及其替代选择。

\subsection{一致性的基本实现方案}
多处理器与1/O的一致性问题尽管在起源上有些类似，但它们却有着不同的特性，会对相
应的解决方案产生影响。在I/O 情景中很少会出现存在多个数据副本的事件（这是一个应当尽
可能避免出现的事件），在多个处理器上运行的程序与此不同，它通常会在几个缓存中拥有同一
数据的多个副本。在一致性多处理器中，缓存提供了共享数据项的迁移与复制。

一致性缓存提供了迁移，可以将数据项移动到本地缓存中，并以透明方式加以使用。这种
迁移既缩短了访问远程共享数据项的延迟，也降低了对共享存储器的带宽要求。

一致性缓存还为那些被同时读取的共享数据提供复制功能，在本地缓存中制作数据项的一
个副本。复制功能既缩短了访问延迟，又减少了对被读共享数据项的争用。支持迁移与复制功
能对于共享数据的访问性能非常重要。因此，多处理器没有试图通过软件来避免这一问题的发
生，而是采用了一种硬件解决方案，通过引人协议来保持缓存的一致性。

为多个处理器保持缓存一致性的协议被称为缓存一致性协议。实现缓存一致性协议的关键
在于跟踪数据块的所有共享状态。目前使用的协议有两类，分别采用不同技术来跟踪共享状态。
\begin{itemize}
    \item 目录式一特定物理存储器块的共享状态保存的位置称为目录。共有两种不同类型的目
    录式缓存一致性，它们的差异很大。在SMP 中，可以使用一个集中目录，与存储器或其
    他某个串行化点相关联，比如多核心中的最外层缓存。在 DSM 中，使用单个目录没有
    什么意义，因为这种方法会生成单个争用点，当多核心中拥有8个或更多个核心时，由
    于其存储器要求的原因，很难扩展到许多个多核芯片。分布式目录要比单个目录更复杂，
    这些设计是5.4节讨论的主题。
    \item 监听式—如果一个缓存拥有某一物理存储器块中的数据副本，它就可以跟踪该块的共
    享状态，而不是将共享状态保存在同一个目录中。在SMP中，所有缓存通常都可以通过
    某种广播介质访问（比如将各核心的缓存连接至共享缓存或存储器的总线），所有缓存控
    制器都监听这一介质，以确定自己是否拥有该总线或交换访问上所请求块的副本。监听
    协议也可用作多芯片多处理器的一致性协议，有些设计在每个多核心内部目录协议的顶
    层支持监听协议！
\end{itemize}
采用微处理器（单核）的多处理器和缓存通过总线连接到单个共享存储器，所以监听协议
的应用越来越多。总线提供了一种非常方便的广播介质，用于实现监听协议。在多核体系结构
中，所有多核都共享芯片上的某一级缓存，所以这一状况有了大幅改变。因此，一些设计开始
转而使用目录协议，因为其开销较低。为便于读者熟悉这两种协议，我们在这里重点介绍监听
协议，在谈到 DSM 体系结构时再讨论目录协议。
\subsection{监听一致性协议}
有两种方法可以满足上一小节讨论的一致性需求。一种方法是确保处理器在写人某一数据
项之前，获取对该数据项的独占访问。这种类型的协议被称为写入失效协议（write invalid
protocol），因为它在执行写人操作时会使其他副本失效。到目前为止，这是最常用的协议。独
占式访问确保在写入某数据项时，不存在该数据项的任何其他可读或可写副本：这一数据项的
所有其他缓存副本都作废。

表5-2给出了一个失效协议的例子，它采用了写回缓存。为了了解这一协议如何确保一致
性，我们考虑在处理器执行写人操作之后由另一个处理器进行读取的情景：由于写操作需要独
占访问，所以进行读取的处理器所保留的所有副本都必须失效（这就是这一协议名称的来历）。
因此，在进行读取操作时，会在缓存中发生缺失，将被迫提取此数据的新副本。对于写人操作，
我们需要执行写入操作的处理器拥有独占访问，禁止任何其他处理器同时写人。如果两个处理
器尝试同时写人同一数据，其中一个将会在竞赛中获胜（稍后将会看到如何确定哪个处理器获
胜），从而导致另一处理器的副本失效。另一处理器要完成自己的写入操作，必须获得此数据的
新副本，其中现在必须包含更新后的取值。因此，这一协议实施了写人申行化。

表5-2 失效协议举例，该协议在单一缓存块（X）的监听总线上工作，采用写回缓存
处理器活动
总线活动
处理器A的缓存内容
处理器B的缀存内容 存储器位置X的内容
0
处理器A读取X
缓存中没有X的内容
0
0
处理器B读取X
缓存中没有X的内容
0
0
处理器A将1写至X
对于X失效
0
处理器B读取X
缓存中没有X的内容
1

*我们假定两个緩存在开始时都没有保存X的内容，存储器中的义值为口。处理器和存储器内容給出了完成处理器及总线
操作之后的取值。空格表示没有操作或没有缓存副本。当B中发生第二次缺失时，处理器 A 反馈该取值，同时取消来
自存储器的响应。此外，B缓存中的内容和X的存储嚣内容都被更新。存储器的这一更新过程是在存储器块变共享时
进行的，这种更新简化了协议，但只能在替换该块时才可能跟踪所有权，并强制进行写回。这就需要引入另外一个名为
“拥有者”的状态，表示某个块可以共事，但当拥有该块的处理器在改变或替換它时，需要更新所有其他处理器和存储
器。如果多械处理器使用了共享镶存（比如L.3），那么所有存储器都是透过这个共享缓存看到的；在这个例子中，L3
的行沟就像存储露一样，一致性必须由每个核心的专用LI 和L.2处理。正是由于这一规察结果，一些设计人贯选择在多
核心处理器中使用目录协议。使这一方法生效，L3缓存必须是包含性的（见5.7.3节）。

失效协议的一种替代方法是在写人一个数据项时更新该数据项的所有缓存副本。这种类型
的协议被称为写入更新或写入广播协议。由于写人更新协议必须将所有写人操作都广播到共享
缓存线上，所以它要占用相当多的带宽。为此，最近的多处理器已经选择实现一种写失效协议，
本章的后续部分将仅关注失效协议。
\subsection{基本实现技术}
实现失效协议的关键在于使用总线或其他广播介质来执行失效操作。在较早的多芯片多处
理器中，用于实现一致性的总线是共享存储器访问总线。在多核心处理器中，总线可能是专用
缓存（Intel Core i7中的L1和L2）和共享外部缓存（中的L3）之间的连接。为了执行一项失
效操作，处理器只获得总线访问，并在总线上广播要使其失效的地址。所有处理器持续监听该
总线，观测这些地址。处理器检查总线上的地址是否在自己的缓存中。如果在，则使缓存中的
相应数据失效。

在写人一个共享块时，执行写人操作的处理器必须获取总线访问权限来广播其失效。如果
两个处理器尝试同时写入共享块，当它们争用总线时，会串行安排它们广播其失效操作的尝试。
第一个获得总线访问权限的处理器会使它正写人块的所有其他副本失效。如果这些处理器尝试写
入同一块，则由总线实现这些写入操作的串行化。这一机制有一层隐含意思：在获得总线访问权
限之前，无法实际完成共享数据项的写人操作。所有一致性机制都需要某种方法来申行化对同一
缓存块的访问，具体方式可以是申行化对通信介质的访问，也可以是对另一共享结构访问的串行化。

除了使被写人缓存块的副本失效之外，还需要在发生缓存缺失时定位数据项。在直写缓存
中，可以很轻松地找到一个数据项的最近值，因为所有写人数据都会发回存储器，所以总是可
以从存储器中找到数据项的最新值。（对缓冲区的写人操作可能会增加一些复杂度，必须将其作
为额外的缓存项目加以有效处理。）

对于写回缓存，查找最新数据值的问题解决起来要困难一些，因为数据项的最新值可能放
在专用缓存中，而不是共享缓存或存储器中。令人开心的是，写回缓存可以为缓存缺失和写入
操作使用相同的监听机制：每个处理器都监听放在共享总线上的所有地址。如果处理器发现自
已拥有被请求缓存块的脏副本，它会提供该缓存块以回应读取请求，并中止存储器（或L3）访
问。由于必须从另一个处理器的专用缓存（LI 或 L2）提取缓存块，所以增加了复杂性，这一
提取过程花费的时间通常长于从L3进行提取的时间。由于写回缓存对存储器带宽的需求较低，
所以它们可以支持更多、更快速的处理器。结果，所有多核处理器都在缓存的最外层级别使用
写回缓存，接下来研究以写回缓存来实现缓存的方法。

通常的缓存标记可用于实施监听过程，每个块的有效位使失效操作的实施非常轻松。读取
缺失（无论是由失效操作导致，还是某一其他事件导致）的处理也非常简单，因为它们就是依
赖于监听功能的。对于写人操作，我们希望知道是否缓存了写人块的其他副本，如果不存在其
他缓存副本，那在写回缓存中就不需要将写入操作放在总线上。如果不用发送写入操作，既可
以缩短写人时间，还可以降低所需带宽。

若要跟踪缓存块是否被共享，可以为每个缓存块添加一个相关状态位，就像有效位和重写
标志位（dirty bit）一样。通过添加1个位来指示该数据块是否被共享，可以判断写人操作是否
必须生成失效操作。在对处于共享状态的块进行写入时，该缓存在总线上生成失效操作，将这
个块标记为独占。这个核心不会再发送其他有关该块的失效操作。如果一个缓存块只有唯一副
本，则拥有该唯一副本的核心通常被称为该缓存块的拥有者。

在发送失效操作时，拥有者缓存块的状态由共享改为非共享（或改独占）。如果另一个处
理器稍后请求这一缓存块，必须再次将状态改为共享。由于监听缓存也能看到所有缺失，所以
它知道另一处理器什么时候请求了独占缓存块，应当将状态改为共享。

每个总线事务都必须检查缓存地址标记，这些标记可能会干扰处理器缓存访问。减少这种
干扰的一种方法就是复制这些标记，并将监听访问引导至这些重复标记。另一种方法是在共享
的L3缓存使用一个目录，这个目录指示给定块是否被共享，哪些核心可能拥有它的副本。利用
.目录信息，可以仅将失效操作发送给拥有该缓存块副本的缓存。这就要求L3必须总是拥有L.I
或L.2 中所有数据项的副本，这一属性被称为包含，在5.7节会再次讨论它。
\subsection{示例协议}
监听一致性协议通常是通过在每个核心中整合有限状态控制器来实施的。这个控制器回应
由核心中的处理器和由总线（或其他广播介质）发出的请求，改变所选缓存块的状态，并使用
总线访问数据或使其失效。从逻辑上来说，可以看作每个块有一个相关联的独立控制器；也就
是说，对不同块的监听操作或缓存请求可以独立进行。在实际实施中，单个控制器允许交错执
行以不同块为目标的多个操作。（也就是说，即使仅允许同时执行一个缓存访问或一个总线访问，
也可以在一个操作尚未完成之前启动另一个操作。）另外别忘了，尽管我们在以下介绍中以总线
为例，但在实现监听协议时可以使用任意互连网络，只要其能够向所有一致性控制器及其相关
专用缓存进行广播即可。

我们考虑的简单协议有三种状态：无效、共享和已修改。共享状态表明专用缓存中的块可
能被共享，已修改状态表明已经在专用缓存中更新了这个块；注意，已修改状态隐含表阴这个
块是独占的。表5-3给出了由一个核心生成的请求（在表的上半部分）和来自总线的请求（在
表的下半部分）。这一协议是针对写回缓存的，但可以很轻松地将其改为针对直写缓存：对于直
写缓存，只需要将已修改状态重新解读为独占状态，并在执行写入操作时以正常方式更新缓存。
这一一基本协议的最常见扩展是添加一种独占状态，这一状态表明块未被修改，但仅有一个专用
缓存保存了这个块。我们将在5.2.6 节介绍这一扩展及其他扩展。

表5-3
缓存一致性机制接收来自核心处理器和共享总线的请求，并根据请求类型、它在本地缓存中是命
中还是缺失、请求中指定的本地缓存块状态来作出回应
所寻址缓存块
缀存操作
请求
源
功能与解释
的状态
的类型
读取命中
处理器
共享或已修改
正常命中
读取本地缓存中的数据
读取缺失
处理器
无效
正常缺失
将读取敏失放在总线上
读取觖失
处理器
共享
替换
地址冲突缺失：将读取敏失放在总线上
读取䱀失
处理器
已修改
替换
地址冲突缺失：写回块，然后将读取缺失放在总线上
写命中
处理器
已修改
正常命中
在本地缓存中写数据
写命中
处理器
共享
一致性
将失效操作放在总线上。这些操作通常称为更新或拥有者
缺失，因为它们不能提取数据，只能改变状态
写缺失
处理器
无效
正常缺失
将写缺失放在总线上
写缺失
处理器
共享
替换
地址冲突缺失：将写缺失放在总线上
写鳅失
处理器
已修改
替换
地址冲突缺失：写回块，然后将写缺失放在总线上
读取缺失
总线
共享
无操作
允许共享缓存或存储器为读取敏失提供服务
读取鉠失
总线
已修改
一致性
尝试共享数据：将缀存块放在总线上，并将状态改沩共享
失效
总线
共享
一致性
尝试写共享块，使该块失效
写缺失
总线
共享
一致性
尝试写共享块，使缓存块失效
写缺失
总线
已修改
一致性
尝试将独占块写到其他位置，写回该缓存块，并在本地缀
存中使其状态失效

*第四列将缓存操作类型操作描述为正常命中或缺失（与单处理緩存看到的情况相同）、替換（单处理器缓存替換
缺失）或一致性（保持缓存一致性所需）；正常操作或替換操作可能会根据这个块在其他缓存中的状态而产生一致
性操作。对于由总线监听到的读取缺失、写入缺失或无效操作，仅当读取或写入地址与本地缓存中的块匹配，而且
这个块有效时，才需要采取动作。

在将一个失效动作或写人峡失放在总线上时，任何一个核心，只要其专用缓存中拥有这个
缓存块的副本，就会使这些副本失效。对于写回缓存中的写人缺失，如果这个块仅在一个专用
缓存中是独占的，那么缓存也会写回这个块；否则，将从这个共享缓存或存储器中读取该数据。

图5-3显示了单个专用缓存块的有限状态转换图，它采用了写入失效协议和写回缓存。为
简单起见，我们将复制这个协议的三种状态，用以表示根据处理器请求进行的状态转换（左图，
对应于表5-3的上半部分），和根据总线请求进行的状态转换（右图，对应于表5-3的下半部分）。
图中使用黑体字来区分总线动作，与状态转换所依赖的条件相对。每个节点的状态代表着选定
专用缓存块的状态，这一状态由处理器或总线请求指定。

专用写回缓存的写入失效、缓存一致性协议，给出了缓存中每个块的状态及状态转换。缓存状态
以圆圈表示，在状态名称下面的括号中给出了本地处理器允许执行但不会产生状态转换的访问。
导致状态变换的激励以常规字体标记在转换弧上，因为状态转换而生成的总线动作以黑体标记在
转换弧上。激励操作应用于专用缓存的块，而不是缓存中的特定地址。因此，在对一个共享状态
的块产生读取缺失时，是对这个缓存块的缺失，而不是对不同地址的缺失。图形左侧显示的状态
转换是由于此缓存相关处理器的操作而发生的，右侧显示的状态转换是根据总线上的操作而发生
的。当处理器请求的地址与本地缓存块的地址不匹配时，会发生独占状态或共享状态的读取缺失
以及独占状态的写人缺失。这种敏失是标准缓存替换缺失。在尝试写人处于共享状态的块时，会
生成失效操作。每当发生总线事务时，所有包含此总线事务指定缓存块的专用缓存都会执行右图
措定的操作。此协议假定，对于在所有本地缓存中都不需要更新的数据块，存储器（或共享缓存）
会在发生对该块的读取缺失时提供数据。在实际实现中，这两部分状态图是结合在一起的。实践
中，关于失效协议还有许多非常细微的变化，包括引入独占的未修改状态，说明处理和存储器
是否会在缺失时提供数据。在多核芯片中，共享缀存（通常是 L3，但有时是L.2）充当着存储器
的角色，总线就是每个核心的专用缓存与共享缓存之间的总线，再由共享缓存与存储器进行交互

这一缓存协议的所有状态在单处理器缓存中也都是需要的，分别对应于无效状态、有效（与
清洁）状态、待清理状态。在写回单处理器缓存中会需要图5-3左半部分中孤线所表示的大多
数状态转换，但有一个例外，那就是在对共享块进行写人命中时的失效操作。图5-3 中右半部
分弧线所表示的状态转换仅对一致性有用，在单处理器缓存控制器中根本不会出现。

前面曾经提到，每个缓存只有一个有限状态机，其激励或者来自所连接的处理器，或者来
自总线。图5-4说明图5-3中右半部分的状态转换如何与图中左半部分的状态转换相结合，构成
每个缓存块的单一状态图。

缓存一致性状态圈，由本地处理器引起的状态转换用黑色表示，由总线行为引起的以灰色表示。
和图5-3中一样，有关转换的行为以粗体显示

为了理解这一协议为何能够正常工作，可以观察一个有效缓存块，它要么在一或多个专用
缓存中处于共享状态，要么就在一个缓存中处于独占状态。只要转换为独占状态（处理器写入
块时需要这一转换），就需要在总线上放置失效操作或写人缺失，从而使所有本地缓存都将这个
块记为失效。另外，如果其他某个本地缓存已经将这个块设为独占状态，这个本地缓存会生成
写回操作，提供包含期望地址的块。最后，对于处于独立状态的块，如果总线上出现对这个块
的读取缺失，拥有其独占副本的本地缓存会将其状态改变共享。

图5-4 中用灰色表示的操作用来处理总线上的读取缺失与写入缺失，它们实际上就是协议
的监听部分。在这个协议及大多数其他协议中，还保留着另外一个特性：任何处于共享状态的
存储器块在其外层共享缓存（L2或L.3，如果没有共享缓存就是指存储器）中总是最新的，这
一特性简化了实施过程。事实上，专用缓存之外的层级是共享缓存还是存储器并不重要；关键
在于来自核心的所有访问都要通过这一层级。

尽管这个简单的缓存协议是正确的，但它省略了许多复杂因素，这些因素大大增加了实施
过程的难度。其中最重要的一点是，这个协议假定这些操作具有原子性—在完成一项操作的
过程中，不会发生任何中间操作。例如，这里讨论的协议假定可以采用单个原子动作形式来检
测写人缺失、获取总线和接收响应。现实并非如此。事实上，即使读取缺失也可能不具备原子
性；在多核处理器的L2中检测到缺失时，这个核心必须进行协调，以访问连到共享L.3的总线。
非原子性操作可能会导致协议死锁，也就是进入一种无法继续执行的状态。我们将在本节后面
研究 DSM设计时讨论这些复杂内容。

对于多核处理器，处理器核心之间的一致性都在芯片上实施，或者使用监听协议，或者使
用简单的集中式目录协议。许多双处理器芯片，包括 Intel Xeon 和 AMID Opteron，都支持多芯
片多处理器，这些多处理器可能通过连接高速接口（分别称为 Quickpath 或 Hypextransport）来
构建。这些下一级别的互连并不只是共享总线的扩展，而是使用了一种不同方法来实现多核互连。
用多个多核芯片构建而成的多处理器将采用分布式存储器体系结构，而且需要一种机制来
保持芯片间的一致性，这一机制要高于、超越于芯片内部的此种机制。在大多数情况下，会使
用某种形式的目录机制。

\subsection{基本一致性协议的扩展}
我们刚刚介绍的一致性协议是一种简单的三状态协议，经常用这些状态的第一个字母来称
呼这一协议——MSI（Modified、Shared、Invalid）协议。这一基本协议有许多扩展，在本节图
形标题中提到了这些扩展。这些扩展是通过添加更多的状态和转换来创建的，这些添加内容对
特定行为进行优化，可能会使性能得到改善。下面介绍两种最常见的扩展。

（1） MESI 向基本的 MSI 协议中添加了“独占”（Exclusive）状态，用于表示缓存块仅驻存
在一个缓存中，而且是清洁的。如果块处于独占状态，就可以对其进行写人而不会产生任何失
效操作，当一个块由单个缓存读取，然后再由同一缓存写人时，可以通过这一独占状态得以优
化。当然，处于独占状态的块产生读取缺失时，必须将这个块改为共享状态，以保持一致性。
因为所有后续访问都会被监听，所以有可能保持这一状态的准确性。具体来说，如果另一个处
理器发射一个读取缺失，则状态会由独占改为共享。添加这一状态的好处在于：在由同一核心
对处于独占状态的块进行后续写人时，不需要访问总线，也不会生成失效操作，因为处理器知
道这个块在这个本地缓存中是独占的；处理器只是将状态改为已修改。添加这一状态非常简单，
只需要使用1个位对这个一致状态进行编码，表示为独占状态，并使用重写标志位表示这个块
已被修改。流行的 MESI 协议就采用了这一结构，这一协议是用它所包含的4种状态命名的，
即已修改（Modified）、独占（Exclusive）、共享（Shared）和无效（Invalid）。Intel i7使用了 MESI
协议的一种变体，称为 MESIF，它添加了一个状态（Forward），用于表示应当由哪个共享处理
器对请求作出回应。这种协议设计用来提高分布式存储器组成结构的性能。

（2） MOESI 向 MESI 协议中添加了“拥有”（Owned）状态，用于表示相关块由该缓存拥有，
在存储器中已经过时。在 MSI和 MESI 协议中，如果尝试共享处于“已修改”状态的块，会将
其状态改为“共享”（在原共享缓存和新共享缓存中都会做此修改），必须将这个块写回存储器
中。而在 MOESI协议中，会在原缓存中将这个块的状态由“已修改”改为“拥有”，不再将其
写到存储器中。（新共享这个块的）其他缓存使这个块保持共享状态；只有原缓存保持“拥有”
状态，表示主存储器副本已经过期，指定缓存成为其拥有者。这个块的拥有者必须在发生缺失
时提供该块，因为存储器中没有最新内容，如果替换了这个块，则必须将其写回存储器中。AMD
Opteron 使用了 MOESI 协议。

下一节研究这些协议在处理多重编程、并行工作负载时的性能；在研究性能时，就能清楚
地了解对基本协议进行这些扩展的价值。但是，在研究性能之前，让我们简要地看一下使用对
称存储器结构和监听一致性机制的局限性。

\subsection{对称共享存储多处理暑与监听协议的局限性}
随着多处理器中处理器数目的增长，或随着每个处理器对存储器需求的增长，系统中的任
何集中式资源都可能变成瓶颈。利用片上提供的更高带宽的连接以及共享的L3缓存（它的速度
要比存储器快），设计人员可以尝试以对称形式支持4~8个高性能核心。这种方法不太可能扩
展到远远超过8个核心的情况，一旦合并了多个多核心处理器，这种方法就无效了。

每个缓存的监听带宽也可能产生问题，因为每个缓存必须检查总线上的所有缺失。我们曾
经提到，复制标记是一种解决方案。另一种方法是在最外层缓存层级放置一个目录，这一方法
已经在最近的某些多核处理器中得到应用。这个目录明确指出哪个处理器的缓存拥有最外层缓
存中每一项的副本。这就是Intel 在i7和 Xeon 7000系统中使用的方法。注意，这个目录的使用
不会消除因为处理器之间的共享总线及L3造成的瓶颈，但它实现起来要远比将在5.4节研究的
分布式目录机制容易。

设计者如何提高存储器带宽，以支持更多或更快的处理器呢？为了提高处理器与存储器之
间的通信带宽，设计者已经采用了多根总线和互连网络，比如交叉开关或小型点对点网络。在
此类设计中，可以将存储器系统（主存储器或共享缓存）配置为多个物理组，以提升有效存储
器带宽，同时还保持存储器访问时间的一致性。图5-5展示了在使用单芯片多核心来实现系统
时，它会是什么样子。尽管利用这种方法可以在一块芯片上实现4个以上核心的互连，但它不
能很好地扩展到那些使用多核心构建模块的多芯片多处理器，因为存储器已经连接到各个多核
心芯片上，不再是集中式存储器。

图5-5 一种多核心单芯片多处理器，通过分组共享缓存实现一致存储器访问，使用互连网络而不是总线

AMD Opteron 表示监听协议与目录协议之间的另一个中间点。存储器被直接连接到每个多
核芯片，最多可以连接4个多核心芯片。其系统为 NUMA，因为本地存储器多少会更快一些。
Opteron使用点对点连接实现其一致性协议，最多向其他3个芯片进行广播。因为处理器之间的
链接未被共享，所以一个处理器要想知道失效操作何时完成，唯一的方法就是通过显式确认。
因此，一致性协议使用广播来查找可能共享的副本，这一点与监听协议类似，但它却使用确认
来确定操作，这一点与目录协议类似。由于在 Opteron 实现中，本地存储器仅比远程存储器快
一点点，所以一些软件把 Opteron 多处理器看作拥有一致存储器讶问。

监听缓存一致性协议可以在没有集中式总线的情况下使用，但仍然需要完成广播，以监听
各个缓存，获知潜在共享缓存块的所有毓失。这种缓存一致性通信是处理器规模与速度的另一
限制。由于采用较大缓存并不会影响一致性通信，所以当处理器速度很快时，肯定会超出网络
的负荷，每个缓存无法响应来自所有其他缓存的监听请求。在5.4 节，我们研究目录式协议，
在发生缺失时不需要向所有缓存进行广播。当处理器速度以及每个处理器的核心数目增大时，
更多的设计人员会选择此类协议来避免监听协议的广播限制。

\subsection{实施监听缓存一致性}
细节决定成败。
—经典谚语

在1990年编写本书第1版时，最后的“融会贯通” 是一个包含30个处理器、单根总线的
多处理器，它采用了监听式一致性；总线的容量仅略高于 50MB/S，到了2011年，这种总线带
宽恐怕连 Intel i7 中的一个核心也无法提供支持！在1995年编写本书第2版时，刚刚出现了第
一代缓存一致性多处理器，它采用了多根总线，我们当时添加了一个附录，用来描述多总线系
统中的监听实现方式。2011年，大多数仅支持单芯片多处理器的多核处理器已经选择使用共享
总线结构，连接到共享存储器或共享缓存。相反，所有支持16个或更多个核心的多核多处理器
系统都使用互连网络，而不是单根总线，设计人员必须面对一项挑战：在实现监听时，没有为
实现事件的串行化而简化总线。

前面曾经说过，在实际实现前面介绍的监听一致性协议时，最复杂的部分在于：在最近的
所有多处理器中，写人缺失与更新缺失都不是原子操作。检测写入或更新缺失、其他处理器与
存储器通信、为写人缺失获取最新值、确保所有失效操作可以正常进行、更新缓存，这些步骤
不能在单个时钟周期内完成。

在单个多核心芯片中，如果（在改变缓存状态之前）首先协调连向共享缓存或存储器的总
线，并在完成所有操作之前保持总线不被释放，那就可以有效地使上述步骤变成原子操作。处
理器怎么才能知道所有失效操作何时完成呢？在一些多核处理器中，当所有必要失效操作都已
收到并在进行处理时，会使用单根信号线发出信号。收到这一信号之后，生成缺失的处理器就
可以释放总线，因为它知道在执行与下一次缺失相关的任意行为之前，可以完成所有必要操作。
只要在执行这些步骤期间独占总线，处理器就能有效地将各个步骤变原子操作。

在没有总线的系统中，我们必须寻找其他某种方法，将缺失过程中的步骤变为原子操作。
具体来说，必须确保两个尝试同时写人同一数据块的处理器（这种情景称为竞争）保持严格排
序：首先处理一个写入操作，然后再开始执行下一个。这两次写人操作中的哪一个操作会赢得
竞争并不重要，因为只会有一个获胜者，它的一致性操作将被首先完成。在监听系统中，为了
确保一次竞争仅有一个获胜者，会广播所有缺失，并利用互连网络的一些基本性质。这些性质，
以及重启竞争失败者缺失处理的能力，是在无总线情况下实现监听缓存一致性的关键。我们将
在附录I中解释具体细节。

还可以将监听式与目录式结合在一起，有一些设计在多核处理器内部使用监听式，在多个
芯片之间使用目录式，或者反过来，在多核处理器内部使用目录式，在多个芯片之间使用监听式。

\section{对称共享存储器多处理器的性能}
在使用监听式一致性协议的多核处理器中，其性能通常由几种不同现象共同决定。具体来
说，总体缓存性能由两个因素共同决定，一个是由单处理器缓存缺失造成的流量，另一个是通
信传输导致的流量，它会导致失效及后续缓存缺失。改变处理器数目、缓存大小和块大小能够
以不同方式来影响缺失率的这两个分量，最终得到受这两种因素共同影响的总体系统性能。

附录B 对单处理器缺失率进行 3C分类，即容量（capacity）、强制（compulsory）和冲突
（contlict），并深入讨论了应用特性和对缓存设计的可能改进。与此类似，因为处理器之间的通
信而导致的缺失（经常被称为一致性缺失）可以分为两种独立源。

第一种来源是所谓的真共享缺失，源自通过缓存一致性机制进行的数据通信。在基于失效
的协议中，处理器向共享缓存块的第一次写人操作会导致失效操作，以确保对这个块的拥有关
系。此外，当另一处理器尝试修改这个缓存块中的已修改字时，要发生缺失，并传送结果块。
由于这两种缺失都是因为处理器之间的数据共享而直接导致的，所以将其划分为真共享缺失。

第二种效果称为假共享缺失，它的出现是因为使用了基于失效的一致性算法，这种算法利
用了数据块的有效位，每个缓存块只有一个有效位。如果因为写人块中的某个字（不是正被读
取的字）而导致一个块失效（而且后续引用会导致失败），就会发生假共享。如果接收到失效操
作的处理器真的正在使用要写人的字，那这个引用就是真正的共享引用，无论块大小如何都会
导致缺失。但是，如果正被写人的字和读取的字不同，那就不会因为这一失效操作而传送新值，
而只是导致一次额外的缓存缺失，所以它是假共享缺失。在假共享缺失中，块被共享，但缓存
中的字都没有被实际共享，如果块大小是单个字，那就不会发生缺失。通过下面的例子可以理
解这些共享模式。

例题
假定 x1和x2 两个字位于同一缓存块中，这个块在P1 和 P2 的缓存中均为共享状
态。假定有以下一系列事件，确认每个觖失是真共享缺失，还是假共享嵌失，或
是命中。如果块大小为一个字节，那么所发生的所有缺失都被认定为真共享缺失。
时
序
P1
P2
1
写x1
2
诶x2
3
写×1
4
写x2
5
读x2
解答
下面是按时序进行的分类。
（1）这一事件是真共享缺失，因为x1 由P2读取，需要由P2发出失效操作。

（2） 这一事件是假共享缺失，因为x2是由于P1中写人x1而导致失效的，但P2并
没有使用x1 的值。
（3） 这一事件是假共享缺失，因为包含x1的块是因为P2中的读取操作而被标记为
共享状态的，但P2并没有读取x1。在P2读取之后，包含x1 的缓存块将处于
共享状态；需要一次写入缺失才能获取对该数据块的独占访问。在一些协议中，
会将这种情况作为更新请求进行处理，它会生成总线失效，但不会传送缓存块。
（4） 这一事件是假共享缺失，原因与步骤3相同。
（5） 这一事件是真共享缺失，因为正在读取的值是由P2写人的。

尽管我们将会看到真假共享缺失在商业工作负载中的影响，但对于共享大量用户数据的紧
耦合应用程序来说，一致性缺失的角色更重要一些。我们在附录1 中详细研究它们的效果，届
时将考虑并行科学工作负载的性能。

\subsection{商业工作负载}
在这一节，我们将研究一个四处理器共享存储器多处理器在运行通用商业工作负载时的存
储器系统特性。我们讨论的这一研究是在1998年用一个四处理器 Alpha 系统完成的，但对于一
个多处理器在执行此类工作负载时的性能问题，这一研究仍然最全面、最深人。这些结果可以
在一个 AlphaServer 4100 上收集，也可以使用一种根据 AlphaServer4100建模的可配置模拟器收
集。AIphaServer 4100 中的每个处理器都是一个 Alpha 21164，它每个时钟周期最多发射4条指
令，工作频率为300MHz。尽管这个系统中 Alpha 处理器的时钟频率远低于2011年所设计系统
中的处理器，但这个系统的基本结构由一个四发射处理器和一个三级缓存层级结构构成，非常
类似于多核 Intel i7和其他处理器，如表5-4所示。具体来说，Alpha 缓存多少略小一些，但缺
失次数也低于i7。因此，Alpha系统应当能够让我们深入理解现代多核设计的特性。

表5-4 本研究所用AIpha 21164及Intel i7的缓存层级结构特性
缓存级别
特征
Alpha 21164
Intel i7
大小
8 KB I8 KB D
32 KB 1/32 KB D
栩联度
直接映射
四路 I八路 D
L1
块大小
32B
64 B
缺失代价
7
10
96 KB
256 KB
三路
八路
L2
32B
64 B
缺失代价
21
35
大小
2 MB
2 MB/核心
L3
相联度
直接映射
十六路
块大小
64 B
64 B
缺失代价
80
約100

*尽管 i7的缓存較大、相联度较离，但缺失代价也较高，所以特性可能只有微小不同。例如，根据附录B，我们可以
估计較小 AlphaLI 缓存的缺失率沟较大 i7 L.1缓存的 4.9\%和3\%，因此，对于 Alpha来说，每次引用的平均LI 缺失
代价为0.34，对于订为0.30。当需要从专用罐存进行传送时，这两种系統的代价都很离（125个周期，甚至更多）。
订还在所有核心之间共事它的L3。

这一研究中使用的工作负载包括以下3个应用程序。

\begin{enumerate}
    \item 根据TPC-B（其存储器特性类似于它的较新版本TPC-C，在第1章中介绍）建模的联机
    事务处理（OLTP）工作负载，并以 Oracle 7.3.2为底层数据库。这一工作负载由一组发出请求
    的客户端进和一组处理这些请求的服务器组成。这些服务器进程占用 85%的用户时间，剩余
    时间由客户端占用。尽管通过精心调优、利用足够的请求保持处理器繁忙可以隐藏1/O 延迟，
    但服务器进程通常会在大约25000条指令之间阻塞1O。
    \item 基于 TPC-D的决策支持系统（DSS）工作负载，（TPC-D是广泛使用的TPC-E的较早版
    本），这一工作负载也以 Oracle 7.3.2为底层数据库。这个工作负载仅包含TPC-D17个读取查询
    中的6个，不过这一基准测试中研究的6个查询包含了整个基准测试的活动范围。为了隐藏1O
    延迟，在查询内部和查询之间都开发了并行（在查询内部，会在查询形成期间检测并行）。与 OLTP
    基准测试相比，调用阻塞的频率要低得多；这6个查询平均在大约150万条指令之后产生阻塞。
    \item Web 索引搜索（AltaVista）基准测试，其基础是对 AltaVista 数据库存储器映射版本
    （200GB）的搜索。深人优化了内层循环。因为搜索结构是静态的，所以线程之间几乎不需要同
    步。在Google 出现之前，AltaVista 是最流行的 Web 搜索引擎。
\end{enumerate}

表5-5显示了在用户模式、内核和空间循环中所用时间的百分比。1/O的频率会同时增加肉
核时间和空闲时间（参见 OLTP项，它的“T/O-计算”比最大）。AltaVista 將整个搜索数据库映
射到存储器中，而且经过了广泛调优，它的内核时间或空闲时间壞少。

表5-5 商业工作负载中执行时间的分布
基准测试
用户模式时间百分比
内核时间百分比
处理锵空闲时间百分比
OLTP
71
18
11
DSS（对所有查询求平均）
87
4
9
Alta Vista
>98
<1
<l

* OLTP 基准测试中搡作系统时间和处理器空闲时间（就是1/0等待时间）所占比例最大。DSS 基准测试的操作系统时
间少得多，因沟它执行的L/O操作較少，但仍然有超过9\%的空闲时间。AltaVista 搜索引擎的全面调优通过这些测量
值表现得非常清楚。这一工作负載的数据是由 BsucroSo、Gharachorloo 和 Bugnion ［1998］在一个四处理器 AlphaServer
4100上收集的。

\subsection{商业工作负载的性能测量}
我们先来看看这些基准测试在四处理器系统中的总体处理器执行情况；如5.3.1的讨论，这
些基准测试包括大量的1/O时间，在处理器时间测试数据中忽略了这些时间。我们将6个 DSS
查询看作一个基准测试，报告其平均特性。这些基准测试的实际 CPI变化很大，从 AltaVista Web
搜索的1.3到 DSS 工作负载的平均1.6，再到 OLTP工作负载的7.0。图5-6显示了如何将执行时
分解为指令执行时间、缓存与存储器系统访问时间及其他停顿（主要是流水线资源停顿，但也
包括转换旁视缓冲区（TLB）和分支预测错误停顿）。尽管DSS与 AltaVista 工作负载的性能处于
合理范围内，但 OLTP工作负载的性能非常差，这是由于存储器层次结构的性能过差所致。

由于 OLTP 工作负载对存储器系统的要求更多，而且存在大量成本高昂的L3敏失，所以我
们主要研究L3缓存大小、处理器数目和块大小对 OLTP基准测试的影响。图5-7显示了增大缓
存大小的影响，使用两路组相联缓存，缩减大量冲突缺失。随着L3缓存的增大，执行时间会因
为L3缺失的减少而缩短。令人惊讶的是，几乎所有这些改进都是在1~2MB 范围内发生的，超
过这一范围之后，尽管当缓存为2MB 和4MB 时，缓存缺失仍然是造成大幅性能损失的原因，
但几乎没有多少改进了。问题是为什么呢？

3 个程序（OLTP、DSS 和 AltaVista）在商业工作负载中的执行时间分解。DSS数字是6个不同
查询的平均值。CPI 的变化很大，从 AltaVista较低的1.3，到DsS查询的1.61，再到 OLTP的7.0。
（分开来说，DSS 查询的CPI 范围为1.3至1.9。）“其他停顿”包括资源停顿（用21164上的重放
陷阱实现）、分支预测错误、存储器屏障和 TLB 缺失。对于这些基准测试，因为资源而导致的流
水线停顿是主要因素。这些数据结合了用户与内核访问的行为。只有 OLTP 的内核访问占有重要
比例，内核访问的表现要忧于用户访问！本节给出的所有测量数据都是由 Barroso、Gharachorlo0
和Bugnion［1998］收集

團5-7 在L3缓存大小变化时，OLTP 工作负载的相对性能，L3缓存设定为两路组相联，从1MB增大到
8MB。空闲时间随缓存大小的增大而延迟，降低了一些性能增益。这一增长是因为当存储器系统
停顿较少时，需要更多的服务器进程来隐藏1O 延迟。可以重新调整工作负载，以提高计算/通信
平衡性能，将空闲时间保持在可控范围内。PAL.代码是一组以优先模式执行的专用操作系统级指
令序列；TLB 缺失处理程序就是这样一个例子

为了更好地理解这个问题的答案，我们需要确定造成L3缺失的因素，以及当L3缓存增长
时，它们是如何变化的。图5-8给出了这些数据，显示了来自5个来源的每条指令所造成的存
储器访问周期数。当L3的大小为1MB 时，L3存储器访问周期的两个最大来源是指令和容量/
冲突缺失，当L.3较大时，这两个来源降低为次要因素。遗憾的，强制、假共享和真共享缺失
不受增大L3的影响。因此，在4 MB和8 MB 时，真共享缺失占主导地位；当L3缓存大小超
过2MB 时，由于真共享缺失没有变化，从而限制了总体缺失率的减少。

图5-8 当缓存大小增加时，占用存储器访问周期的各项因素会发生偏移。L3缓存被模拟为两路组相联

增大缓存大小可以消除大多数单处理器缺失，但多处理器缺失不受影响。增大处理器数目
如何影响各种不同类型的缺失呢？图5-9给出了这些数据，其中假定所采用的基本配置为2MB、
两路组相联L3缓存。可以预期，真共享缺失率的增加（降低单处理器鋏失不会对其有所补偿）
导致每条指令的存储器访问周期增大。

我们研究的最后一个问题是：增大块大小对这一工作负载是否有所帮助（增大块大小应当
能够降低指令和冷缺失率，在限度范围内，还会降低容量/冲突嵌失率，并可能降低真共享嵌失
率）。图5-10显示了当块大小由32字节变化到256字节时，每千条指令的缺失数目。将块大小
由32字节变化到256字节会影响到4个缺失率分量。

\begin{itemize}
    \item 真共享缺失率的降低因数大于2，表示真共享模式中存在某种局域性。
    \item 强制缺失率显著降低，与我们的预期一致。
    \item 冲突/容量缺失有小幅降低（降低因数1.26，而在增大块大小时的降低因数为8），表示
    当L3缓存大于2MB 时所发生的单处理器缺失没有太高的空间局域性。
    \item 假共享缺失率接近翻番，尽管其绝对数值较小。
\end{itemize}

对指令缺失率缺乏显著影响，这一事实是令人惊讶的。如果有一个仅包含指令的缓存具备
这一特性，那就可以得出结论：其空间局域性非常差。在采用混合L2缓存时，诸如指令数据冲
突之类的其他影响也可能会导致较大块中产生较高的指令缓存缺失。其他研究已经表明，在大
型数据库和 OLTP 工作负载（它们有许多小的基本块和专用代码序列）的指令流中，空间局域
性较低。根据这些数据，可以将块大小为32字节的L3的缺失代价作为基准，将块大小较大的
L3 的缺失代价表示为前者的倍数。

图 5-9 当处理数目增大时，存储器访问周期的各项导致因素因为真共享缺失的增加而增加。由于每个
处理器现在必须处理更多的强制缺失，所以强制缺失会稍有增加

当L3 缓存的大小增加时，每千条指令的缺失数目稳定下降，所以L3块大小至少应当为128字
节。L.3缓存的大小为2 MB，两路组相联

由于现代 DDR SDRAM加快了块访问速度，所以这些数字看起来是可以实现的，特别是块
大小为128字节的情况。当然，我们还必须考虑增加存储器通信量以及与其他核心争用存储器
的影响。后一效果可能很轻松地抵销通过提高单个处理器性能而获得的增益。

\subsection{多重编程和操作系统工作负载}
我们的下一项研究是包括用户行为和操作系统行为的多重编程工作负载。所使用的工作负
载是 Andrew 基准测试编译阶段的独立副本，这一基准测试模拟了软件开发环境。其编译阶段
使用8个处理器执行 Unix “make”命令的一个并行版本。这一工作负载在8个处理器上运行
5.24秒，生成203个进程，对3个不同文件系统执行 787次磁盘请求。运行此工作负载使用了
128 MB 存储器，没有发生分页行为。

此工作负载有3个截然不同的阶段：编译基准测试，涉及大量计算行为；将目标文件安装
到一个库中；删除目标文件。最后一个阶段完全由 1/O 操作主导；只有两个进程是活动的（每
个运行实例有一个进程）。在中间阶段，1/O 也扮演着重要角色，处理器大多处于空闲状态。与
经过仔细调优的商业工作负载相比，这个总体工作负载涉及的系统操作和IO操作要多得多。
为进行工作负载的测量，我们假定有以下存储器和I/O 系统。

\begin{itemize}
    \item 第一级指令缓存——32KB，两路组相联，块大小为64字节，命中时间为1个时钟周期。
    \item 第一级数据缀存—-32KB，两路组相联，块大小为32字节，命中时间为1个时钟周期。
    我们改变L1数据缓存，以研究它对缓存特性的影响。
    \item 第二级缓存——1MB 一致缓存，两路组相联，块大小为128字节，命中时间为1个时钟
    周期。
    \item 主存储器—一总线上的唯一存储器，访问时间为100个时钟周期。
    \item 磁盘系统—一固定访问延迟为3ms（小于正常值，以缩短空闲时间）。
\end{itemize}
表5-6显示如何针对使用上述参数的8个处理器来分解其执行时间。执行时间被分解为以
下4个分量。

（1） 空闲-
一在内核模式空闲循环中执行。
（2）用户—以用户模式执行。
（3） 同歩-
执行或等待同步变量。
（4） 内核-
一在既未处于空闲状态也没有进行同步访问的操作系统中执行。

表5-6 多重编程井行“make”工作负载中执行时间的分布
用户执行
内核执行
同步等待
处理器空闲（等待V/O）
所执行的指令
27%
3%
1%
69%
执行时间
27%
7%
2%
64%

*当8个处理器中仅有1个处于活动状态时，空闲时间之所以占很大比例是因为磁盘延迟的原因。这些教据及这一工
作负載的后续测量由 SimOS 系統收集［Rosenblum 等人，1995］。实际执行及数据收集由斯坦福大学的 M. Rosenbhum、
S. Herrod 和E.Bugnion 完成。

这一多重编程工作负载的指令缓存性能损失非常明显，至少对操作系统如此。当块大小为64
字节、采用两路组相联缓存时，操作系统中的指令缓存缺失率由32KB缓存的1.7\%变为256 KB
缓存的0.2\%。对于各种缓存大小，用户级指令缓存缺失大体为操作系统缺失率的六分之一。这
一点部分解释了如下事实：尽管用户代码执行的指令数为内核的9倍，但这些指令占用的时间
仅为内核所执行少量指令的4倍。

\subsection{多重编程和操作系统工作负载的性能}
在这一小节，我们研究多重编程工作负载在缓存大小、块大小发生变化时的缓存性能。由
于内核特性与用户进程性能之间的差异，我们将这两个分量保持分离。别忘了，用户进程执行
的指令是内核的8倍，所以整体缺失率部分由用户代码中的缺失率决定，后面将会看到，这一
缺失率通常是内核缺失率的五分之一。

尽管用户代码执行更多的指令，但与用户进程相比，操作系统的特性可能导致更多的缓存
缺失，除了代码规模较大和缺少局域性之外，还有两个原因。第一，内核在将页面分配给用户
之前，会对所有页面进行初始化。第二，内核实际上是共享数据的，因此其一致性缺失率不可
忽视。与之相对，用户进程只有在不同处理器上调度进程时才会导致一致性鋏失，而这一部分
缺失率是很小的。

图5-11给出了当数据缓存大小、块大小变化时，数据缺失率的内核及用户部分。增大数据
缓存大小对用户缺失率的影响要大于对内容缺失率的影响。增大块大小对于两种缺失率都有正
面影响，这是因为很大一部分缺失是因为强制和容量导致，这两者都可能通过增大块大小加以
改进。由于一致性觖失相对来说更为罕见，所以增大块大小的负面影响很小。为了了解内核与
用户进程的行什么会不同，我们可以看看内核缺失是如何表现的。

在增大L1数据缓存大小（左图）及增大L2数据缓存块大小时（右图），数据缺失率的用户分量
及内核分量表现不同。将L1数据缓存由32 KB增大到256 KB（块大小为32字节）导致用户缺
失率的降低大于内核觖失率：用户级缺失率的下降因数大约为3，而内核级缺失率的下降因数仅
为1.3。当L1 块大小增大时（保持L1缓存为32KB），缺失率的用户分量及内核分量都会稳定下
降。与增大缓存大小的影响相对，增大块大小会显著降低内核缺失率（当块大小由 16 字节变
128 字节时，内核引用的下降因数仅略低于4，而用户引用则略低于3）

图5-12显示了缓存大小及块大小增大时，内核缺失的变化。这些缺失被分为三类：强制缺
失、一致性觖失（由真、假共享导致）和容量/冲突觖失（包括由于操作系统与用户进程之间和
多个用户进程之间的干扰所导致的缺失）。图S-12证明：对于内核引用，增大缓存大小只会降
低单处理器容量/神突觖失率。与之相对，增大块大小会导致强制缺失率的降低。当块大小增大
时，一致性缺失率没有大幅增大，这意味着假共享效率可能不是很明显，尽管此类缺失可能会
抵消通过降低真共享缺失所带来的增益。

在8个处理器上运行多重编程工作负载，当L1 数据缓存大小由32KB 变化为 256 KB时，内核
数据缺失率分量的变化。强制缺失率分量保持不变，因它不受缓存大小的影响。容量分量的
下降因数大于2，而一致性分量几乎翻番。一致性缺失增大的原因在于：发生冲突的项目会由于
容量原因而变少，所以失效操作导致发生缺失的可能性会随着缓存大小的增大而增大。可以预
料，L1 数据缓存的块大小增加会大幅降低内核引用中的强制缺失率。它对容量缺失率也有显著
影响，在块大小的变化范围中，这一缺失率的降低因数为2.4。增加块大小只能少量减少一致性
通信流量，它在64字节时稳定下来，在变为128字节时，一致性缺失率没有变化。由于当块大
小增加时一致性缺失率没有显著降低，所以因为一致性所导致的缺失率部分由大约7%增长到大
约 15%

如果我们研究每次数据引用所需要的字节数，如图5-13所示，可以看到内核的通信流量比
较高，会随着块大小的增加而增加。很容易就能看出其原因：当块大小由16字节变为128字节
时，缺失率大约下降3.7，但每次缺失传送的字节数增大8倍，所以总缺失通信量仅提高2倍多
一点。当块大小由 16字节变为128字节时，用户程序的增大也会超过2倍，但它的起始水平要
低得多。

对于多重编程工作负载，操作系统对存储器系统的要求要严格得多。如果工作负载中包含
了更多的操作系统行为或类似于操作系统的行为，而且其特性类似于这一工作负载测量的结果，
那就很难构建具有足够功能的存储器系统。一种可能是提高性能的方法是让操作系统更多地了
解缓存，可能是通过更好的编程环境，也可能通过程序员的帮助来实现。例如，操作系统会为
不同系统调用发出的请求而重复利用存储器。尽管被重复利用的存储器将被完全改写，但硬件
并没有意识到这一点，它会尝试保持一致性，即使缓存块不会被读取，也会坚持认为存在这一
可能性。这一行为类似于在过程调用时重复利用栈位置。IBM Power 系列就已经允许编译器在
过程调用时指示这种行为类型，最新的 AMD处理器也提供类似支持。系统是很难检测这种行
为的，所以可能需要程序员提供帮助，其回报可能要大得多。

操作系统与商业工作负载对多处理器存储器系统提出了非常严酷的挑战，而且它们与科学
应用程序不同（在附录1中进行研究），不太适合进行算法或编译器重构。随着核心数目的增加，
预测此类应用程序的行为也会变得更为困难。一些模拟或仿真技术可以用大型应用程序（包括
操作系统）对数百个核心进行仿真，它们对于坚持设计的分析与量化方法至关重要。

图5-13 当块大小增加时，对于内核分量与用户分量，每次数据引用所需要的字节数据会增加。將这个图
表与附录1中的科学程序数据进行对比是很有意义的

\section{分布式共享存储器和目录式一致性}
我们在S.2节讨论过，监听协议在每次发生缓存缺失时都需要与所有缓存进行通信，包括对
共享数据进行的写人操作。监听式机制没有任何用于跟踪缀存状态的集中式数据结构，这是它的
一个基本优点（因为这样可以降低成本），但考虑到可伸缩性时，这也成了它的“阿基里斯脚跟”。

例如，考虑一个由四核多核心组成的多处理器，它能够保持每个时钟周期一次数据引用的速
率，时钟速率为4GHz。从附录I中的I.5节的数据可以看出，这些应用程序需要4GB/s ~170GB/8
的总线带宽。尽管这些实验中的缓存很小，但大多数通信量都是一致性通信流量，不受缓存大
小的影响。尽管现代总线可以达到4GB/s的带宽，但170GB/s还是远远超过了任何总线式系统
的能力。在最近几年中，多核处理器的发展迫使所有设计人员转向某种分布式存储器，以支持
各个处理器的带宽要求。

我们可以通过分布式存储器来提高存储器带宽和互连带宽，如图5-2所示；这样会立刻将
本地存储器通信与远程存储器通信隔离开来，降低了对存储器系统和互连网络的带宽要求。除
非不再需要一致性协议在每次缓存缺失时都进行广播，否则通过分布式存储器不会有太大收益。

如前所述，监听式一致性协议的替代方法是目录式协议。目录中保存了每个可缓存块的状
态。这个目录中的信息包括哪些缓存（或缓存集合）拥有这个块的副本，它是否需要更新，等
等。在一个拥有共享最外层缓存（即L3）的多核心中，实现目录机制比较容易：只需要为每个
L3块保存一个位向量，其大小等于核心的数目。这个位向量表示哪些专用缓存的L3 中可能拥
有一个块的副本，失效操作仅会发送给这些缓存。如果L3是包含性的，那这一方法对于单个多
核心是非常有效的，在Intel i7 中就是采用了这一机制。

在多核心中使用单个目录时，即使它能避免广播，这种解决方案也是不可能扩展的。这个
目录必须是分布式的，但其分布方式必须能够让一致性协议知道去哪里寻找存储器所有缓存块
的目录信息。一个容易想到的解决方案是将这个目录与存储器分布在一起，使不同一致性请求
可以进入不同目录，就像不同存储器请求进人不同存储器一样。分布式目录保留了如下特性：
块的共享状态总是放在单个已知位置。利用这一性质，再另外维护一些信息，指出其他哪些节
点可能缓存这个块，就可以让一致性协议避免进行广播操作。图5-14显示了在向每个节点添加
目录时，分布式存储器多处理器的样子。

向每个节点添加一个目录，以在分布式存储器多处理器中实施缓存一致性。在本例中，节点被显
示为单个多核芯片，相关存储器的目录信息可能驻存在多核心处理器的内部，也可能在其外部。
每个目录负责限踪一些缓存，这些缓存共享该节点内部部分存储器的存储器地址。一致性机制可
能会维护多核心节点内部的目录信息，并处理所需要的一致性操作

最简单的目录实现方法是将每个存储器块与目录中的一项相关联。在这种实现方式中，信
息量与存储器块数（每个块的大小与L.2或L.3缓存块相同）和节点数的乘积成正比，其中一个
节点就是在内部实施一致性的单个多核心处理器或一小组处理器。对于处理器少于数百个的多
处理器而言（每个处理器可能是多核的），这一开销不会导致问题，因为当块大小比较合理时，
目录开销是可以忍受的。对于大型多处理器，需要一些方法来高效地扩展目录结构，不过，只1379
有超级计算机规模的系统才需要操心这一点。

\subsection{目录式缓存一致性协议：基础知识}
和监听式协议一样，目录式协议也必须实现两种主要操作：处理读取缺失和处理共享、清洁
缓存块的写入操作。（对于当前正被共享的块，其写入缺失的处理就是上述两种操作的组合。）为
实现这些操作，目录必须跟踪每个缓存块的状态。在简单协议中，这些状态可能为下列各项之一。
\begin{itemize}
    \item 共享-一或多个节点缓存了这个块，存储器的值是最新的（所有缓存中也是如此）。
    \item 未缓存——所有节点都没有这个缓存块的副本。
    \item 已修改——只有一个节点有这个缓存块的副本，它已经对这个块进行了写操作，所以存
    储器副本已经过期。这个处理器被称为这个块的拥有者。
\end{itemize}

除了跟踪每个潜在共享存储器块的状态之外，我们还必须跟踪哪些节点拥有这个块的副本，
在进行写人操作时需要使这些副本失效。最简单的方法是为每个存储器块保存一个位向量，当
这个块被共享时，这个向量的每一位指明相应的原处理器芯片（它可能是一个多核心）是否拥
有这个块的副本。当存储器块处于独占状态时，我们还可以使用这个位向量来跟踪块的拥有者。
为了提高效率，还会跟踪各个缓存中每个缓存块的状态。

每个缓存中状态机的状态与转换都和监听缓存中使用的状态机相同，只不过在发生转换时
的操作稍有不同。用于定位一个数据项独占副本并使其失效的过程有所不同，因为它们需要在
发出请求的节点与目录之间进行通信，在目录与一或多个远程节点进行通信。在监听式协议中，
这两个步骤通过向所有节点进行广播而结合在一起。

在查看这种协议的状态图之前，先来研究一下为了处理缺失和保持一致性而可能在处理器
和目录之间传送的消息类型，这样会有所帮助。表5-7给出了节点之间发送的消息类型。本地
节点是发出请求的节点。主节点（home node）就是一个地址的存储器位置及目录项所在的节点。
物理地址空间是静态分布的，所以事先知道哪个节点中包含给定物理地址的存储器与目录。例如，
高阶位可以提供节点编号，低阶位提供节点上存储器内的偏移。本地节点也可能是主节点。当主
节点是本地节点时，由于副本可能存储于第三节点上（称为远程节点），所以必须访问该目录。

表5-7 在节点之间为保证一致性而发送的可能消息，以及源节点和目标节点、消总内容（p=发出请求的
节点编号，A=所请求的地址，D=数据内容），和消息的功能

消忠类型
来源目标
消息内容
读取鉠失
本地缓存 主目录
P,A
写人缺失
本地缀存
主目录
P，
A
失效
本地缓存
主目录
A
失效
主目录
运程缓存
A
取数据
主目录
运程缓存
A
消息的功能
节点P在地址A发生读取缺失，请示数据井将P设置为读取共享者
节点P在地址A发生写人敏失，请求数据并使用P成为独占拥有者
向所有缓存了地址A块的远程緩存发送失败请求
使地址A处数据的共享副本失效
取回地址A的块，并发送到它的主目录，把远程缓存中A的
状态改为共享
取数据/失效
主目录
运程缓存
A
取回地址A的块，并发送到它的主目录，使缓存中的块失效
数据值应答
主目录
本地缀存
D
从主存储器返回数据值
数据写回
运程缓存
主目录
A,D
写回地址A的数据值
*前3条消息是由本地节点发送到主节点的请求。第四个到第六个是当主节点需要數据来满足读取缺失或写入缺失请
求时，向远程节点发送的消息。数据应答消息用于由主节点向发出请求的节，点传送一个取值。在两种情况下需要对
数据值执行写回操作：一种情况是，如果替换了缓存中的一个教据块，且必须写回到它的主存储器中；另一种情况
是，对来自主节点的取数据消息或取数据/失效消怒做应答时。只要数据块处于关事状态就执行写回標作，这样能简
化协议中的状态数目，这是因沟任何胜数据块必须处于独占状态并且任何共享块总是可以在主存储器中获取。

远程节点是拥有缓存块副本的节点，这一副本可能独占（在此情况下只有一个副本），也可
能共享。远程节点也可能与本地节点或主节点相同。在此类情况下，基本协议不会改变，但处
理器之间的消息可能会被处理器内部的消息代替。

在这一节，我们采用存储器一致性的一种简单模型。为了在最大程度上减少这种类型的消
息及协议的复杂性，我们假定这些消息的接受及处理顺序与其发送顺序相同。这一假定在实际
中并不成立，可能会导致额外的复杂性，在5.6 节讨论存储器一致性模型时会讨论其中一部分内
容。在这一节，我们利用这一假定来确保在传送新消息之前先处理节点发送的失效操作，就像
在讨论监听式协议时的假设一样。和在监听情景中一样，我们省略了一些实现一致性协议所必
需的细节。具体来说，要想实现写人操作的串行化，并获知某写人的失效操作已经完成，并不
像广播式监听机制那样轻松，而是需要采用明确的确认方法来回应写人缺失和失效请求。附录
I中更详细地讨论了这些问题。
\subsection{目录式协议举例}
目录式协议中缓存块的基本状态与监听式协议中完全相同。目录中的状态也与我们前面展
示的状态类似。因此，我们首先看一个简单的状态图，它给出了一个具体缓存块的状态转换，
然后再研究与存储器中每一个块相对应的目录项的状态图。和监听情景中一样，这些状态转换
图并没有给出一致性协议的所有细节；但是，实际控制器调度依赖于多处理器的大量细节（消
息发送特性、缓冲结构，等等）。在这一节，我们给出了基本的协议状态图。附录I中研究了在
实现这些状态转换图中的一些棘手问题。

图5-15显示了一个具体缓存对应的协议操作。所使用的符号与上一节相同，来自节点外部
的请求用灰色表示，操作用黑色表示。一个具体缓存的状态转换由读取缺失、写人缺失和状态
提取请求导致；图5-15显示了这些操作。一个具体缓存也会生成这些读取缺失、写人缺失和失
效消息，它们会被发送给主目录。读取缺失与写入鲈失要求数值回复，这些事件在改变状态之
前会等待回复。如何知道失效操作何时完成，那是另一个问题，另行处理。

目录式系统中一个具体缓存块的状态转换图。本地处理器的请求用黑色表示，来自主目录的请
求用灰色表示。这些状态与监听式系统中相同，而且事务非常类似，用显式失效与写回请求来
代替向总线正式广播的写人缺失。与监听控制器中一样，我们仍然假定在尝试写人共享缓存块
时将被作为缺失而进行处理；在实践中，这样的事务可以看作拥有权请求或升级请求，可以在
未请求所提取缓存块的同时提交拥有权

图5-IS 中缓存块状态转换图的操作基本上与监听情景中一样：状态是相同的，激励也几乎
相同。写入缺失操作由数据提取和失效操作替代，失效操作由目录控制器选择性地发送，而在
监听机制中，写人缺失操作是在总线（或其他网络）上广播的。与监听协议一样，在写人缓存
块时，它必须处于独占状态，所有共享块都必须在存储器中进行更新。在许多多核处理器中，
在核心之间共享处理器缓存的最外层级（比如 Intel i7、AMDD Opteron 和IBMPower7中的L3），
处于这一级别的硬件将在同一芯片上每个核心的专用缓存之间保持一致性，或者使用内部目录
实现，或者使用监听实现。因此，只需要与最外层共享缓存进行交互，就能使用芯上多核一致
性机制在大量处理器之间扩展一致性。因为这一交互是在L3层级进行的，所以处理器与一致性
请求之间的争用就不会导致问题，也可以避免标签的复制。

在目录式协议中，目录实现了一致性协议的另一半。发向目录的一条消息会导致两种不同
类型的操作：更新目录状态；发送附加消息以满足请求。目标中的状态表示一个块的三种标准
状态；但与监听机制中不同的是，目录状态表示一个存储器块所有缓存副本的状态，而不是表
示单个缓存块的相应信息。

存储器块可能未由任何节点缓存，可能缓存于多个节点中并可读（共享），也可能仅在一个
节点中独占缓存并可写。除了每个块的状态之外，目录还会跟踪拥有某一缓存块副本的节点集
合；我们使用名为共享器的集合来执行这一功能。在节点数少于64的多处理器中（每个节点可
能表示4~8倍的处理器），这一集合通常表示为位向量。目录请求需要更新这个共享器集合，
还会读取这个集合，以执行失效操作。

图5-16给出了在目录中为回应所接收消息而采取的操作。目录接收三种不同请求：读取缺
失、写人缺失和数据写回。目录发送的回应消息用粗体表示，而集合“共享器”的更新用黑色
表示。因为所有激励消息都来自外部，所以所有操作都以灰色表示。我们的简化协议假定一些
操作是原子操作，比如请求某个值并将其发送给另一个节点；实际实现时不能采用这一假定。

为了理解这些目录操作，让我们逐个状态查看所接收的请求和所采取的操作。当块处于未
缓存状态时，存储器中的副本就是当前值，所以对这个块的请求只能是以下两种。

\begin{itemize}
    \item 读取缺失——从存储器向发出请求的节点发送其请求的数据，请求者成为唯一的共享节
    点。块的状态变为共享。
    \item 写入缺失——向发送请求的节点传送取值，该节点变为共享节点。这个块变为独占状态，
    表明缓存了唯一有效副本。共享器指明拥有者的身份。
    当块处于共享状态时，存储器值是最新的，所以可能出现相同的两个请求。
    \item 读取缺失——从存储器向发出请求的节点发送其请求的数据，请求者被添加到共享集合
    中。
    \item 写入缺失——向请求节点发送取值。向共享者集合中的所有节点发送失效消息，共享者
    集合将包含发出请求的节点的身份。这个块的状态变为独占状态。
\end{itemize}

当块处于独占状态时，这个块的值保存在一个节点的缓存中，这个节点由共享者（拥有者）
集合识别，所以共有3种可能的目录请求。

\begin{itemize}
    \item 读取缺失—一向拥有者发送数据提取消息，它会将拥有者缓存中这个块的状态转变为共
    享，拥有者将数据发送给目录，再在这里将其写到存储器中，并发给提出请求的处理器。
    将发出请求的节点的身份添加到共享者集合中，这个集合中仍然包含拥有者处理器的身
    份（因为这个处理器仍然拥有可读副本）。
    \item 数据写回—拥有者正在替换这个块，因此必须将其写回。这个写回操作会更新存储器
    副本（主目录实际上变为拥有者），这个块现在未被缓存，共享者集合为空。
    \item 写入缺失—这个块有一个新的拥有者。向旧日拥有者发送一条消息，将其缓存中的这个
    块失效，并将值发送给目录，从目录中发送给提出请求的节点，这个节点现在变成新的
    拥有者。共享者被设定为新拥有者的身份，这个块仍然保持独占状态。
\end{itemize}

图5-16 目录的状态转移图与独立缓存的转移图具有相同的状态和结构。由于所有操作都是由外部导致
的，所以均以灰色表示。粗体表示该目录回应请求所采取的操作

图5-16中的状态转换图是一种简化图，与监听式缓存的情景相同。在采用目录式协议时，
以及用网络而非总线来实现监听机制时，协议需要处理非原子化存储器转换。附录1深入探讨
了这些问题。

实际多处理器中使用的目录协议还进行了其他一些优化。具体来说，在这种协议中，在针
对独占块发生读取缺失或写入缺失时，会首先将这个块发送到主节点上的目录中。再从这里将
其存储到主存储器中，并发送给原来发现请求的节点。商用多处理器使用的许多协议都会将数
据从拥有者节点直接转发给发出请求的节点（同时对主节点执行写回操作）。由于这些优化方法
增大了死锁可能，并增加了必须处理的消息类型，所以通常会提高复杂性。

从5.2.8节开始，我们讨论了实现监听式协议的一些挑战，在实施目录式机制时，也要应对
大部分此类挑战，而且还增加了一些新的问题，在附录I中介绍了这些新问题。在5.8节，我们
将简要介绍现代多核处理器是如何将一致性扩展到单个芯片之外的。多芯片一致性和多核一致
性有4种组合方式：监听/监听（AMD Opteron）、监听/目录、目录/监听和目录/目录！

\section{同步：基础知识}
同步机制通常是以用户级软件例程实现的，这些例程依赖于硬件提供的同步指令。对于较
小型的多处理器或低争用解决方案，一种很关键的硬件功能是拥有不可中断的指令或指令序列，
它们能以原子方式提取和改变一个值。软件同步机制就是利用这一功能实现的。这一节的重点
是锁定及非锁定同步操作的实现。可以非常轻松地利用锁定和非锁定来创建互斥，并实现更复
杂的同步机制。

在高争用情景中，同步可能会成为性能瓶颈，因为争用会引人更多延迟，在此种多处理器
中，延迟可能更大一些。附录I 将会讨论如何将这一节讨论的基本同步机制扩展到处理器数目
很大的情况。
\subsection{基本硬件原语}
在多处理器中实施同步时所需要的关键功能就是一组能够以原子方式读取和修改存储器位
置的硬件原语。没有这一功能，构建基本同步原语的成本就会过高，并随着处理器数目的增大
而增大。基本硬件原语有许多替代方式，所有这些方式都能够以原子形式读取和修改一个位置，
还有某种方法可以判断读取和写人是否以原子形式执行。这些硬件原语是一些基本构建模块，
用于构建各种用户级别的同步操作，包括诸如锁和屏障之类的内容。一般情况下，架构师不希
望用户利用基本硬件原语，而是希望这些原语由系统程序员用来构建同步库，这个过程通常比
较复杂，需要一些技巧。我们首先来看这样一个硬件原语，说明如何用它来构建某些基本的同
步操作。

一种用于构建同步操作的典型操作就是原子交换，它会将寄存器中的一个值与存储器的一
个值进行交换。为了明白如何利用这一操作来构建基本的同步操作，假定我们希望构建一个简
单锁，数值0表示这个锁可以占用，数值1表示这个锁不可用。处理器尝试对这个锁进行置位，
具体做法是将寄存器中的1与这个锁的相应存储器地址进行交换。如果其他某个处理器已经申
请了访问权，则这一交换指令将返回1，否则返回0。在后一种情况下，这个值也被改变为1，
以防止任意进行竞争的交换指令也返回0。

例如，考虑两个处理器，每个处理器都尝试同时进行交换：只有一个处理器将会首先执行
交换操作，并返回数值0，第二个处理器进行交换时将会返回1，所以不会存在竞争问题。使用
交换原语来实现同步的关键是这个操作具有原子性：这一交换是不可分的，两个同时交换将由
写人申行化机制进行排序。如果两个处理器尝试以这种方式对同步变量进行置位，它们不可能
认为自己同时对这个变量进行了置位。

还有大量其他原子原语可用于实现同步。它们都拥有一个关键特性：它们读取和更新存储
器值的方式可以让我们判断这两种操作是不是以原子形式执行的。在许多较旧的多处理器中存
在一种名为测试并置位的操作，它会测试—个值，如果这个值通过测试则对其进行置位。比如，
我们可以定义一个操作，它会检测0，并将其值设定为1，其使用方式与使用原子交换的方式类
似。另一个原子同步原语是提取并递增：它返回存储器位置的值，并以原子方式使其递增。我
们用0值来表示同步变量未被声明，可以像使用交换一样使用提取与递增。稍后将会看到，还
有其他一些类似于“提取并递增”的操作用法。

实现单个原子存储器操作会引入一些挑战，因为它需要在单个不可中断的指令中进行存储
器读取与写人操作。这一要求增加了一致性实施的复杂性，因为硬件不允许在读取与写人之间
插人任何其他操作，而且不能死锁。

替代方法是利用一对指令，其中第二条指令可以返回一个值，根据这个值，可以判断这一
对指令是否以原子形式执行。如果任一处理器执行的所有其他指令要么在这对指令之前执行，
可么在这对指令之后执行，那就可以认为这对指令具有原子性。因此，如果一个指令对具有原
子特性，那所有其他处理器都不能在这个指令对之间改变取值。

这种指令对包含一种名为链接载入或锁定载入的特殊载人指令和一种名为条件存储的特
存储指令。这些指令是按顺序使用的：对于链接载人指令指定的存储器位置，如果其内容在对
同一位置执行条件存储之前发生了改变，那条件存储就会失败。如果在两条指令之间进行了上
下文切换，那么存储条件也会失败。条件存储的定义是在成功时返回1，失败时返回0。由于链
接载人返回了初始值，而条件存储仅在成功时才会返回1，所以以下序列对RI 内容指定的存储
器位置实现了一次原子交换：

\begin{verbatim}
    try：
    MOV
    R3,R4；移动交換值
    LL
    R2.0（R1）；链接載入
    SC
    R3,0（R1）；条件存储
    BEQZR3,try；分支存储失败
    MOV R4,RZ ；将載入值放入R4 中
\end{verbatim}

在这个序列的末尾，R4 的内容和R1指定存储器位置的内容已经实现了原子交换（忽略了延退分
支的影响）。在任意时间，如果处理器介入LL和SC指令之间，修改了存储器中的取值，那么SC
在R3中返回0，导致此代码序列再次尝试。

链接载人/条件存储机制的益处之一就是它能用于构建其他同步原语。例如，下面是原子的
“提取并递增”：
\begin{verbatim}
    try：
    LL
    R2,0（R1）；链接載入
    DADDUIR3,R2，#1；递增
    SC
    R3,0（R1）；条件存储
    BEQZ R3,try；条件存储失敗
\end{verbatim}
这些指令通常是通过在寄存器中跟踪LL指令指定的地址来实现的，这个寄存器称为链接寄存
器。如果发生了中断，或者与链接寄存器中地址匹配的缓存块失效（比如，另一条SC使其失效），
链接寄存器将被清除。SC指令只是核查它的地址与链接寄存器中的地址是否匹配。如果匹配，
SC 将会成功；否则就会失败。在再次尝试向链接载入地址进行存储之后，或者在任何异常之后，
条件存储将会失败，所以在选择向两条指令之间插人的指令时必须非常小心。具体来说，只有
寄存器-寄存器指令才是安全的；否则，就有可能造成死锁情景，处理器永远无法完成SC。此
外，链接载入和条件存储之间的指令数应当很小，以尽可能减少无关事件或竞争处理器导致条
件存储频繁失败的情景。

\subsection{使用一致性实现锁}
在拥有原子操作之后，就可以使用多处理器的一致性机制来实施自旋锁（spin lock）—
处理器持续用循环来尝试获取锁，直到成功为止。在两种情况下会用到自旋锁，一种是程序员
希望短时间拥有这个锁，另一种情况是程序员希望当这个锁可用时，锁定过程的延迟较低。因
为自旋锁会阻塞处理器，在循环中等待锁被释放，所以在某些情况下不适合采用。

最简单的实施方法是在存储器中保存锁变量，在没有缓存一致性时将会使用这种实施方式。
处理器可能使用原子操作（比如5.5.1 节所述的原子交换）持续尝试获得锁，测试这一交换过程
是否返回了可用锁。为释放锁，处理器只需要在锁中存储数值0即可。下面的代码序列使用原
子交换来锁定自旅锁，其地址在R1 中：
\begin{verbatim}
    DADDUTR2,RO，#1
    lockit：
    EXCHR2,0（R1）
    ；原子交换
    BNEZR2,1ockit
    ；已经锁定？
\end{verbatim}
如果多处理器支持缓存一致性，就可以使用一致性机制将锁放在缓存中，保持锁值的一致
性。将锁放在缓存中有两个好处。第一，它允许采用一种实施方式，允许针对本地缓存副本完
成“自旋”过程（在一个紧凑循环中尝试测试和获取锁），不需要在每次尝试获取锁时都请求全
局存储器访问。第二个好处来自以下观察结果：锁访问中经常存在局域性；也就是说，上次使
用了一个锁的处理器，很可能会在不远的将来再次用到它。在此类情况下，锁值可以驻存在这
个处理器的缓存中，大幅缩短获取锁所需要的时间。

要实现第一个好处（能够针对本地缓存副本进行循环，不需要在每次尝试获取锁时都生成
存储器请求），需要对这个简单的自旋过程进行一点修改。上述循环中每次尝试进行交换时都需
要一次写人操作。如果多个处理器尝试获取这个锁，会分别生成这一写人操作。这些写入操作
大多会导致写人缺失，因为每个处理器都是尝试获取处于独占状态的锁变量。

因此，应当修改自旋锁过程，使其在自旋过程中读取这个锁的本地副本，直到看到该锁可
用为止。然后它尝试通过交换操作来获取这个锁。处理器首先读取锁变量，以检测其状态。处
理器不断地读取和检测，直到读取的值表明这个锁未锁定为止。这个处理器随后与所有其他正
在进行“自旋等待”的处理器展开竞赛，看谁能首先锁定这个变量。所有进程都使用一条交换
指令，这条指令读取旧值，并将数值1存储到锁变量中。唯一的获胜者将会看到0，而失败者
将会看到由获胜者放在里面的1。（失败者会继续将这个变量设置到锁定值，但这已经无关紧要
了。）获胜的处理器在锁定之后执行代码，完成后将0存储到锁定变量中，以释放这个锁，然后
再从头开始竞赛。下面的代码执行这一自旋锁（别忘了，0是未锁定，1是锁定）：
\begin{verbatim}
    lockit：
    LDR2,0（R1）
    ；载入锁
    BNEZR2,lockit
    ；不可用——自旋
    DADDUIR2,RO，#1
    ；戴入锁定值
    EXCHR2,0（R1）
    ；交换
    BNEZR2,lockit
\end{verbatim}
；如果锁不为Q，则跳转
让我们看看这一“自旋锁”机制是如何使用缓存一致性机制的。表5-8显示了当多个进程
尝试使用原子交换来锁定一个变量时的处理器和总线（或目录）操作。一旦拥有锁的处理器将
0存储到锁中，所有其他缓存都将失效，必须提取新值以更新它们保存的锁副本。这种缓存首
先获取未定值（0）的一个副本，并执行交换。在满足其他处理器的缓存缺失之后，它们发现这
个变量已经被锁定，所以必须回过头来进行检测和自旋。

这个例子显示了链接载入/条件存储原语的另一个好处：读取操作与写人操作是明确独立
的。链接载入不一定导致任何总线通信。这一事实允许采用以下简单代码序列，它的特性与使
用交换的优化版本一样（R1 拥有锁的地址，LL代替了 LD，SC 代替了 EXCH）：
\begin{verbatim}
    lockit：
    LLR2,0（R1）
    ；链接載入
    BNEZR2,1ockit
    ；不可用——自棘
    DADDUIR2,RO，#1
    ；锁定值
    SCR2,0（R1）
    ；存储
    BEQZR2,lockit
    ；如果失败则跳转
\end{verbatim}
第一个分支构成了自旋循环，第二个分支化解当两个处理器同时看到锁可用时的竞赛。

步骤
1
PO
拥有锁
表5-8 三个处理器PO、P1、P2的缓存一致性步骤和总线通信
步骤结束时锁
的一致性状态
共享
2
3
将锁设为0
P1
开始自旋、判断锁是
否为0
（接收到失效操作）
缀存缺失
P2
开始自旋、判断锁
是否为0
（接收到失效操作〉
缓存缺失
独占 （PO）
共享
4
（当总线/目录忙时
通过锁为0的检测
等待）
锁为0
共享
总线/目录操作
以任意顺序满足P1和P2的缓存
缺失。锁状态变为共享
来自PO销变量的写入失效操作
总线/目录为P2缓存缺失提供服
务，从P0写回，状态为共享
溝足P2的缓存缺失
5
6
执行交换、获得缓存
䱀失
交换完成、返回1，
将锁设置为1
执行交换，获得缓
存觖失
完成交换：返回0，
井将锁置沩1
进人关键部分
共享
满足P1的緩存缺失
独占 （P2）
总线/目录为P2缓存觖失提供服
多，生成失效，锁为独占
独占（P1）
总线/目录为P1綴存缺失提供服
务，发送失效操作，从P2生成
写回操作
8
自旋、检测锁是否为0
无

*本表假定采用写入失效一致性。在开始时，PO拥有这个锁（步骤1），锁的值为1（即被锁定）；它最初为独占的，
在步骤1开始之前由PO拥有。PO退出并解锁（步骤2）。P1 和P2竞赛，看看谁能在交換期间看到未锁定值（步骤
3 至步骤5）。P2赢得竞賽，进入关键部分（步骤6与步骤7），而P！ 的尝试失败，所以它开始自旋等待（步骤7
和步骤8）。在实际系统中，这些事件将耗费更多时间，远多于8次时钟嘀嗒，因为获取总线和回复敏失所需要的时
间要长得多。一旦到了步骤8，这一过程就可以从 P2开始重复，它最终获得独占访问，并特锁设置为0。

\section{存储器连贯性模型：简介}
缓存一致性保证了多个处理器看到的存储器内容是一致的。但它并没有回答这些存储器内
容应当保持何种程度的一致性。我们问“何种程度的一致性”时，实际是在问一个处理器必须
在什么时候看到另一个处理器更新过的值？由于处理器通过共享变量进行通信（用于数据值和
同步两种目的），于是这个问题便简化次：处理器必须以何种顺序来观测另-一个处理器的数据写
入？由于“观测另一处理器写人操作”的唯一方法就是通过读取操作，所以问题现在变为：在
不同处理器对不同位置执行读取和写入操作时，必须保持哪些特性？

“保持何种程度的一致性”这一问题看起来非常简单，实际上却非常复杂，我们通过一个简
单例子来了解一下。下面是来自处理器PI 和P2的两段代码，并排列出如下：

\begin{verbatim}
    P1：
    A= 0；
    P2: B=0；
    ⋯⋯
    A= 1；
    B = 1；
    L1：
    if （B == 0）. L2：
    if （A == 0）...
\end{verbatim}
假定这些进程运行在不同处理器上，位置A和B最初由两个处理器进行缓存，初始值为0。如果
写入操作总是立刻生效，而且马上会被其他处理器看到，那两个 语句（标有L1和L2）不可
能将其条件计算为真，因为能够到达 IF 语句，说明A或B中必然已经被指定了数值1。我们假
定写人失效被延迟，处理器可以在这一延迟期间继续执行。因此，PI 和 P2在它们尝试读取数值
之前，可能还没有（分别）看到B和A的失效。现在的问题是，是否应当允许这一行为？如果
应当允许，在何种条件下允许？

存储器连贯性的最简单模型称为顺序连贯性模型。顺序连贯性要求任何程序每次执行的结
果都是一样的，就像每个处理器是按顺序执行存储器访问操作的，而且不同处理器之间的访问
任意交错在一起。有了顺序连贯性，就不可能再出现上述示例中的某些不明执行情况，因为必
须完成赋值操作之后才能启动下F语句。

实现顺序连贯性模型的最简单方法是要求处理器推迟完成所有存储器访问，直到该访问操
作所导致的全部失效均告完成为止。当然，如果推迟下一个存储器访问操作，直到前一访问操
作完成为止，这种做法同样有效。别忘了，存储器连贯性涉及不同变量之间的操作：两个必须
保持顺序的访问操作实际上访问的是不同的存储器位置。在我们的例子中，必须延迟对A或B
的读取（A 0或B-0），直到上一次写人操作完成为止（B=1或A=1）。比如，根据顺序连贯性，
我们不能简单地将写入操作放在写入缓冲区中，然后继续执行读取操作。

尽管顺序连贯性模型给出了一种简单的编程范例，但它可能会降低性能，特别是当多处理
器的处理器数目很大或者互连延迟很长时尤为如此，如下例所示。

例题
假定有一个处理器，一次写人缺失需要50个时钟周期来确定拥有权，在确定拥有
权之后发射每个失效操作需要10个时钟周期，在发射之后，失效操作的完成与确
认需要80个时钟周期。假定其他4个处理器共享一个缓存块，如果处理器保持顺
序连贯性，一次写人缺失会使执行写人操作的处理器停顿多长时间？假定必须明
确确认失效操作之后，一致性控制器才能知道它们已经完成。假定在为写人鋏失
获得拥有者之后可以继续执行，不需要等待失效；该写人操作需要多长时间？

解答
在等待失效时，每个写人操作花费的时间等于拥有时间再加上完成失效所需要的
时间之和。由于失效操作可以重叠，所以只需要为最后一项操心，它是在确定拥
有权之后开始的10+10+10+10=40个时钟周期。因此，写入操作的总时间为
50+40+80=170个时钟周期。与之相比，拥有时间只有50个时钟周期。通过实现
适当的写人缓冲区，甚至有可能在确定拥有权之前继续进行。

为了提供更好的性能，研究人员和架构师已经研究了两种不同路径。第一，他们开发了强
大的实施方式，能够保持顺序连贯性，但使用延迟隐藏技术来降低代价；我们将在5.7 节讨论
这些内容。第二，他们开发了限制条件较低的存储器一致性模型，支持采用更快速的硬件。这
些模型可能会影响程序员看到多处理器的方式，所以在讨论这些低限制模型之前，先来看看程
序员有什么期望。

\subsection{程序员的观点}
尽管顺序连贯性模型有性能方面的不足，但从程序员的角度来看，它拥有简单性这一优点。
挑战在于，要开发一种编程模型，既便于解释，又支持高性能实施方式。

有这样一种支持更高效实施方式的编程模型，它假定程序是同步的。如果对共享数据的所
有访问都由同步操作进行排序，那就说这个程序同步的。如果满足以下条件，就说数据引用是
由同步操作进行排序的：在所有可能的执行情景中，一个处理器对某一变量的写人操作与另一
个处理器对这个变量的访问（或者为读取，或者为写人）之间由一对同步操作隔离开来，其中
一个同步操作在写人处理器执行写人操作之后执行，另一个同步操作在第二个处理器执行访问
操作之前执行。如果变量可以在未由同步操作进行排序的情况下更新，此类情景称为数据竞赛，
因为操作的执行结果取决于处理器的相对速度，和硬件设计中的竞赛相似，其输出是不可预测
的，由此给出另一种同步程序的名字：无数据竞赛。

给出一个简单的例子，变量由两个不同处理器读取和更新。每个处理器用锁定和解锁操作
将读取和更新操作包围起来，这两种操作是为了确保更新的互斥和读取操作的连贯性。显然，
每个写人操作与另一个处理器的读取操作之间现在都由一对同步操作隔离开来：一个是解锁（在
写入操作之后），一个是锁定（在读取操作之前）。当然，如果两个处理器正在写人一个变量，
中间没有插入读取操作，那这些写人操作之间也必须由同步操作隔离开。

人们普遍认同“大多数程序都是同步的”这一事实。这一观察结果之所以正确，主要是因
为：如果这些访问是非同步的，那么到底哪个处理器赢得数据竞赛就由执行速度决定，从而会影
响到程序结果，那程序的行为就可能是不可预测的。即使有了顺序连贯性，也很难理清此类程序。

程序员可能尝试通过构造自己的同步机制来确保排序，但这种做法需要很强的技巧性，可
能导致充满漏洞的程序，而且在体系结构上可能不受支持，也就是说在以后各代多处理器中可
能无法工作。因此，几乎所有的程序员都选择使用同步库，这些库正确无误，而且针对多处理
器和同步类型进行了优化。

最后，标准同步原语的使用可以确保：即使体系结构实现了一种比顺序连贯性模型更宽松
的连贯性模型，同步程序也会像硬件实施了顺序连贯性一样运行。

\subsection{宽松连贯性模型：基础知识}
宽松连贯性模型的关键思想是允许乱序执行读取和写入操作，但使用同步操作来实施排序，
因此，同步程序的表现就像处理器具备顺序连贯性一样。这些宽松模型是多种多样的，可以根
据它们放松了哪种读取和写入顺序来进行分类。我们利用一组规则来指定顺序，其形式为X Y，
也就是说必须在完成操作 X之后才能执行操作Y。顺序连贯性模型需要保持所有4种可能顺序：
RW、R一R、W-R 和W-W。宽松模型的确定是看它们放松了这4种顺序中的哪一种。

\begin{enumerate}
    \item 放松 W一R顺序，将会得到一种称为完全存储排序或处理连贯性的模型。由于这种排
    序保持了写人操作之间的顺序，所以许多根据顺序连贯性运行的程序也能在这一模型下运行，
    不用添加同步。
    \item 放松 W-W顺序，将会得到一种称为部分存储顺序的模型。
    \item 放松 R-*W和R-次R顺序，将会得到许多模型，包括弱排序、PowerPC连贯性模型和释
    放连贯性，具体取决于排序约束条件的细节和同步操作实施排序的方式。
\end{enumerate}
通过放松这些排序，处理器有可能获得显著的性能提升。但是，在描述宽松连贯性模型时
存在许多复杂性，包括放松不同顺序的好处与复杂性、准确定义写人完成的含义、决定处理器
什么时候看到它自己写人的值。有关宽松模型的复杂性、实施问题以及性能潜力的更多信息，
我们强烈推荐 Adve 和 Gharachorloo［1996］的优秀教程。

\subsection{关于连贯性模型的最后说明}
目前，许多正在开发的多处理器都支持某种宽松连贯性模型，既有处理器连贯性，又有释
放连贯性。由于同步过程与处理器特性密切相关，而且容易导致错误，所以人们希望大多数程序
员使用标准同步库，编写同步程序，使程序员感觉不到弱连贯性模型的选择，并得到更高的性能。

还有一种观点认为，利用推测功能，宽松连贯性模型的大多数性能优势都能通过顺序连贯
性或处理器连贯性实现。下一节将更全面地讨论这一内容。

这一观点中关于宽松连贯性的关键部分涉及编译器的角色，以及针对潜在共享变量来优化
存储器访问的能力；这一主题也将在5.7节讨论。

\section{交叉问题}
由于多处理器重新定义了许多系统特性（例如，性能评估、存储器延迟和可伸缩性的重要
性），所以它们引入了一些贯穿整个领域的重要设计问题，对硬件和软件都产生影响。在这一节，
我们将给出一些与存储器连贯性问题有关的示例。随后研究在向多重处理中添加多线程时所能
获得的性能。

\subsection{编译器优化与连贯性模型}
定义存储器连贯性模型的另一个原因是指定合法的编译器优化范围，可以针对共享数据来
执行这些优化。在显式并行程序中，除非明确定义了同步点，而且程序被同步，否则编译器不
能交换对两个不同共享数据项的读取操作和写人操作，因为这种转换可能会影响程序的本来语
义。因此，一些相对简单的优化方式也无法实施，比如共享数据的寄存器分配，因为这种转换
通常会交换读取和写人操作。在隐式并行程序中（比如，用“高性能FORTRAN（HPF）” 编写的
程序），程序必须被同步，而且同步点已知，所以不会出现这一问题。编译器能否从更宽松的连
贯性模型中获得明显好处，无论是从研究的角度来看还是从实践的角度来看，这都依然是一个
开放性的问题，由于缺乏统一模型，可能会妨碍编译器的部署进程。

\subsection{利用推测来隐藏严格连贯性模型中的延迟}
在第3章曾经看到，可以利用推测来隐藏存储器延迟。还可用来隐藏因为严格连贯性模型
导致的延迟，获得宽松存储器模型的大多数好处。其关键思想是：处理器使用动态调度来重新
安排存储器引用的顺序，让它们有可能乱序执行。乱序执行存储器引用可能会违犯顺序连贯性，
从而影响程序的执行。利用推测处理器的延迟提交功能，可以避免这种可能性。假定一致性协
议是以失效操作为基础的：如果处理器在提交存储器引用之前，收到该存储器引用的失效操作，
处理器会使用推测恢复来回退计算，并利用失效地址的存储器引用重新开始。

如果处理器对存储器请求进行重新排序后，新执行顺序的结果不同于在顺序连贯性下看到
的结果，处理器将会撤消此次执行。使用这一方法的关键在于：处理器只需要确保其结果与所
有访问完全循序完成时是一样的，通过检测两种结果可能在什么时候出现不同，就可以做到这
一点。由于很少会触发推测重启，所以这种方法很有吸引力。只有当非同步访问实际导致竞赛
时，才会触发推测重启［Gharachorloo、Gupta 和 Hennessy 1992］o

Hil ［1998］提倡将顺序连贯性或处理器连贯性与推测执行结合起来，作为一种连贯性模型。
他的观点包括三个部分。第一，积极实现顺序连贯性或处理器连贯性，可以获得更宽松模型的
大多数好处。第二，这种实施方式仅对推测处理器增加了非常少的实施成本。第三，这种方法
允许程序员考虑使用顺序连贯性或处理器连贯性的更简单编程模型。MIPS R10000设计团队在
20世纪90年代中期就深刻认识到这一点，使用R10000的乱序功能来支持顺序连贯性的这种积
极实施方式。

一个尚未解决的问题是，在优化对共享变量的存储器引用时，编译器技术会取得怎样的成
功？共享数据通常是通过指针和数组索引进行访问的，这一事实再加上优化技术的现状，已经
限制了此种优化技术的使用。如果这一技术进入实用状态，而且能够带来显著的性能优势，编
译器编写人员可能会希望使用更宽松的编程模型。

\subsection{包含性及其实现}
所有多处理器都使用多级缓存层级结构来减少对全局互连的要求和缓存缺失延迟。如果缓
存还提供了多级包含性（缓存层次结构的每一级都是距处理器更远一层的子集），所以我们可以
使用多级结构来减少一致性通信与处理器通信之间的争用，当监听与处理器缓存访问必须竞争
缓存时，就会出现这些争用。许多具有多层缓存的多处理器都具备这种包含性，不过，最近有
些多处理器采用较小的L1缓存和不同的块大小，有时会选择不实施这种包含特性。这一限制有
时也称为子集特性，因为每个缓存都是它下一级缓存的子集。

乍看起来，保持多级包含特性是件很简单的事情。考虑一个两级示例：L.1 中的所有缺失要
么在L2命中，要么在L2 中产生缺失，无论是哪一种情况，缺失块都会进入L1和L.2 两级缓存。
与此类似，任何在L2命中的失效都必然被发送给LI，如果L1 中存在这个块，将会使其失效。

难以理解的地方在于当L1 和L2 的块大小不同时会发生什么。选择不同块大小是非常合情
合理的，因为L2通常要大得多，其缺失代价中的延迟分量也要长得多，因此希望使用较大的块
大小。当块大小不同时，对于包含性的“自动”实施有什么影响呢？L.2中的一个块对应于L.1
中的多个块，L2的一次映失所导致的数据替换对应于多个 L1 块。例如，如果L.2 的块大小是
L1 的4倍，那么L.2中的一次缺失将替换相当于4个L1块的内容。下面考虑一个详细示例。

例题
假定L.2的块大小为L1块的4倍。说明一次导致L1和L2产生替换的地址鳅失将
如何违犯包含特性。
解答

假定L1 和L.2是直接映射的，L1 的块大小为 个字节，L2的块大小为48个字节。
假定L1 包含两个块，起始地址为x和x+b，且x mod.46=0，也就是说，x也是L.2
中一个块的起始地址；因此，L2中的单个块包含着L1块 、 + 、 +28和 +3b。
假定处理器生成一个对块y的引用，这个块对应于在两个缓存中都包含x的块，从
而会产生缺失。由于L2产生缺失，所以它会提取 4b个字节，并替换包含2、8+b、
x+26和x+36的块，而L1取得B个字节，并替换包含x的块。由于L.1仍然包含x+，
但L2不再包含，因此不再保持包含特性。

为了在采用多个块大小时仍然保持包含性，在较低级别完成替换时，必须上溯到层次结构
的较高级别，以确保较低级别中替换的所有字在较高级别的缓存中都已失效；相联度的不同级
别也会产生同类问题。2011年，设计人员在实施包含性方面仍然存在分歧。Baer 和Wang［1988］
详细描述了包含性的优势与挑战。Intel i7 为L3应用了包含性，也就是说L3总是包含L.2和L.1
的内容。这样就可以在L3 实施一种简单的目录机制，在最大程度上降低因为监听L.1和L2而
对这些情景造成的干扰，目录中指出L1或L2中含有一个缓存副本。AMID Opteron 与之相对，
使L.2包含L.1 的内容，但对L.3没有这一限制。它们使用了监听协议，但除非存在命中情况，
否则仅需要在L2 进行监听，在这种情况下，会向LI 发送监听。

\subsection{利用多重处理和多线程的性能增益}
这一节将考察两项研究，了解在多核处理器上使用多线程的有效性；在下一节还会讨论这
一主题，届时将研究 Intel i7的性能。我们的这两项研究是以Sun T1（在第3章介绍）和 IBM Power5
处理器为基础的。

我们利用第3章研究过的三个面向服务器的基准测试来研究T1多核处理器的性能，这三个
基准测试为 TPC-C、SPECJBB（SPEC Java业务基准测试）和SPECWeb99。SPECWeb99 基准
测试仅在 T1 的一个四核版本上运行，因为它不能扩展到利用八核处理器的全部32个线程；另
两个基准测试以八核心处理器运行，每个核心有4个线程，总共有32个线程。表5-9给出了八
核心T1的每线程 CPFI和每核心CFI，以及实际 CFT 和每个时钟周期的指令数（IPC）。

表5-9 八核心Sun T1处理器的每线程CPI、每核心CPI、实际八核心CPI和实际IPC（CPI的倒数）
基准测试
每线程的CPI
每核心的CPI
八个核心的实际CPI
八个核心的实际IPC
TPC-C
7.2
1.8
0.225
4.4
SPECJBB
5.6
1.40
0.175
5.7
SPECWeb99
6.6
1.65
0.206
4.8
wupwise
swim
mgrid
applu
mesa
galgcl
art
equake
facerec
ammp
lucas
fma3d
sixtrack
apptu
vpr
gcC
图 5-17
con
gap
vortex
bzip2
twolf
0.9
1.0
12
15
1i4
1L5
加速比

在八处理醬 IBM eServer P5575 上对比SMT 和单线程（ST）性能。注意，x轴的起始加速比为
0.9，表明有性能损失。每个 Power'5 核心中仅有一个处理器是活动的，通过降低存储器的破坏性
干扰，应当可以稍稍改善 SMT的结果。SMIT结果是通过创建16个用户线程获得的，而ST结果
仅使用了8个线程；由于每个处理器仅有一个线程，所以操作系统将 PowerS 切换为单线程模式。
这些结果是由IBM 公司的 MoCalpin 收集的。从数据中可以看出，SPECfpRate结果的标准偏差
略高于 SPECintRate （0.13比0.07），表明浮点程序的SMT提升可能会有很大的变化范围

IBM Power5是一种支持同时多线程（SMT）的双核处理器。为了研究多线程在多处理器中
的性能，我们对一个拥有8个Power 5处理器的IBM 系统进行了测试，仅使用了每个处理器上
的一个核心。图 5-17给出了一个八处理器 Power5 多处理器在有、无 SMT 时执行 SPECRate2000
基准测试的加速比，见图题中所述。平均来说，SPECintRate 的速度为 1.23倍，而 SPECfpRate
的速度1.16 倍。注意，一些浮点基准测试在SMT模式中的性能会稍有下降，加速比最多会降
低0.93。尽管人们希望 SMI可能会更好地隐藏 SPECFP基准测试的高缺失率，但看起来，在以
SMT模式运行这些基准测试时，会遇到存储器系统中的一些限制。

\section{融会贯通：多核处理器及其性能}
2011年，多核心成为所有新处理器的主旋律。各种实现方式的变化很大，它们对大型多芯
片多处理器的支持也同样有很多不同。这一节研究4种不同多核处理器的设计和一些性能特征。

表5-10给出了4种为服务器应用设计的多核处理器。Intel Xeon 的设计基础与i7相同，但
它的核心更多、时钟频率稍慢（功率限制了其时钟频率）、L.3缓存较大。AMID Opteron 和桌面
Phenom 共享相同的基础核心，而 SUNT2与在第3章遇到的SUN TI 相关。Power7 是 Power5
的扩展，核心更多，缓存更大。

表5-10 4种最近为服务器设计的高端多核处理（2010年发布）的特征汇总
特征
晶体管数
功率（标称）
每个芯片的最大核心数
多线程
每个核心的线程数
每个时钟周期发射的
指令数
时钟频率
最外层缓存
AMD Opteron 8439
IBM Power 7
Intel Xenon 7560
Sun T2
9.04亿
137 W
6
无
1
12亿
140 W
8
SMT
4
23亿
130w
8
SIMT
2
5亿
95 w
8
细粒度
8
一个线程发射3条
一个线程发射6条
一个线程发射4条
两个线程发射2条
2.8 GHz
L3、6MB、共享
2.7 GHz
L3.24MB、共享
1.6 GHz
L2、4 MB、共享
包含
多核-~致性协议
无、尽管12是L1的超集
MOESI
4.1 GHz
L3、32 MB（采用嵌入
DRAM）、共享或由各个
核心专用
有、L3超集
扩展MESI，具有行为
和局域性暗示（13-状
态协议〉
设在L3的目录
使用SMP链接可以将最
多32个处理器芯片连接
起来。采用动态分布式目
录结构。8核心芯片之外
的存储器访问是对称的
有、L3超集
MESIF
有
MOESI
多核一致性实现
对扩展一较性的支持
监听式
利用 HyperTransport
可以将最多8个处理
器连接为一个环，采
用目录式或监听式
协议。系统为NUMA
设在L3的目录
通过Quickpath互
连可以连接最多8
个处理器核心。以
外部逻辑支持
目录
设在L2的目录
通过每个处理器的
四个一致性链接实
现，可用于进行监
听。最多两个芯片直
接相连，最多四个使
用外部ASIC的连接

*表中包含了这些处理器中核心数最多的版本；其中一些处理器还有核心教较低、时钟频率较高的版本。IEM Powet7
中的L3可以全部类享，也可以划分为各个核心专用的更快速专用区域。我们仅包含了这些多核心处理器的单芯片实
现方式。

首先，我们将对比其中三种多核处理器在配置为多芯片多处理器时的性能和性能扩展性
（由于 AMID Opteron 的数据不足，所以省略了这一处理器）。

这三种多处理器对 IP和 TLP 的侧重点不尽相同，除此之外，它们的市场定位也有明显不
同。因此，我们没有过多地关注绝对性能的对比，而将重点放在增加处理器时的性能扩展能力
上。在研究这一数据之后，将会更详细地研究 Intel Core i7 的多核性能。

我们将展示三组基准测试的性能：SPECintRate、SPECfpRate 和 SPECjbb2005。我们将
SPECRate 基准测试聚集在一起，展示这些多处理器在请求级并行方面的性能，这种并行的特征
就在于独立程序的并行与重叠执行。具体来说，除系统服务之外的所有内容都未被共享。

SPECjbb2005是一种可扩展Java 业务基准测试，它对一种三层客户端/服务器系统进行建模，主
要关注服务器端，类似于我们在第1章研究的SPECPower 中所使用的基准测试。这些基准测试
对Java 虚拟机、即时编译器、垃圾收集、线程及操作系统其他方面的实现进行了测试；还测试
了多处理器系统的可伸缩性。

图 5-18给出了 SPECRate CPU 基准测试在核心数目增加时的性能变化。随着处理器芯片数
及核心数的增加，可以获得近似线性的加速比。

当处理芯片数增大时，三种多核处理嚣运行 SPECRate 基准测试的性能。注意，对于这个高
度并行的基准测试，得到了近似线性的加速比。这两个曲线都采用对数-对数刻度，所以线性加
速比表现为一条直线

图5-19 给出了 SPECjbb2005 基准测试的类似数据。要在开发更多 ILP 和仅关注 TLP 之间
实现平衡是很复杂的，它与具体的工作负载高度相关。SPECjbb2005 工作负载能够在增加更多
处理器时进行扩展，使运行时间（而非问题规模）保持恒定。在这种情况下，会有足够的并行，
可以通过64个核心来实现线性加速比。我们将在结语部分再次讨论这一主题，但现在先让我们
仔细地研究一下 Intel Core i7 在单芯片、四核心模式下的性能。

8192K
2048K
- UItraSPARCT2
Xeon X7560
— Power7
512K
128K
8
16
32
64
核心数

图5-19 当处理器芯片数目增加时，三种多核心处理器运行 SPECjbb2005 基准测试的性能。注意，对于
这一并行基准测试，得到了近似线性的加速比

\subsubsection{Intel Core i7 多核的性能与能耗效率}
在这一节，我们利用第3章考虑过的两组基准测试来研究 i7的性能，即并行 Java 基准测试
和并行 PARSEC 基准测试（在表3-18中有详细介绍）。我们首先来看一下在没有使用 SMT时多
核心性能、扩展能力与单核心的对比。然后将多核心和SMT功能结合起来。这一节的所有数据
与前面i7 SMT评估中的数据（3.13节）一样，都来自 Esmaeilzadeh 等人［2011］。数据集也与前
面使用的相同（见表3-18），只是去除了 Java 基准测试 tradebeans 和 pibb2005（仅留下了5个可
伸缩Java 基准测试）：即使采用4个核心、总共8个线程，tradebeans 和 pibb2006的加速比也不
会超过1.5，因此不适于评估更多核心。

图5-20绘制了在没有使用SMT时 Java和 PARSEC基准测试的加速比和能量效率曲线。给
出能耗效率曲线意味着我们绘制的是两核心或四核心运行消耗能量与单核心运行消耗能量的比
值；因此，能耗效率越高越好，取值为1.0时为其平衡点。在所有情景中，没有使用的核心都
处于尝试睡眠模式，基本上相当于将这些核心关闭，使其功耗降至最低。在对比单核心和多核
心基准测试的数据时，一定要记住，在单核心（及多核心）情景中，L3缓存和存储器接口的全
部能耗成本都是物有所值的。因为这一事实，对于那些能够很好扩展的应用程序，有可能进一
步改善其能耗指标。在汇总这些结果时使用了调和均值，其隐含意义见图题。

本国给出了未采用SMT 时，两核和四核处理器执行并行 Java 与 PARSEC 工作负载时的加速比。
这些数据由 Esmaeilzadeh 等人［2011］收集，使用的设置与第3章所述设置相同。Turbo Boost 功能
被关闭。加速比与能耗效率数据使用调和均值汇总，其隐含含义就是在这种工作负载中，运行每
个2p 基准测试所花费的时间是等价的

如图 5-20所示，PARSEC基准测试的加速比要优于Java基准测试，在四核心处理器上的加
速比效率为76\%（即实际加速比除以处理器数目），而Java 基准测试在四核心处理器上的加速
比效率为 67\%。尽管从数据中可以很清楚地看出这一结果，但要分析存在这种差异的原因要麻
烦一些。例如，很有可能是 Amdahl定律降低了Java 工作负载的加速比。此外，处理器体系结
构与应用程序之间的交互也可能在其中产生影响（它会影响到同步成本或通信成本等问题）。具
体来说，并行化程度很高的应用程序（比如 PARSEC中的程序）有时可能因为计算与通信之间
的有利比值而获益，这种比值可以降低对通信成本的依赖性（见附录I）。

这种加速比的差异性可以转换为能耗效率的差异性。例如，相对于单核心版本，PARSEC
基准测试实际上只是稍微提高了能耗效率；这一结果可能受到以下事实的显著影响：L3缓存在
多核运行版本中的使用效率要高于单核情景，而两种情景中的能耗成本是相同的。因此，对于
PARSEC 基准测试，多核方法达到了设计人员从关注ILP的设计转向多核设计的目的，即：其
性能的增长速度不低于功率的增长速率，从而使能耗效率保持不变，甚至还有所提高。在Java
情景中我们看到，由于 Java 工作负载的加速比级别较低，（尽管在2p运行中，Java 能耗效率与
PARSEC 相同！）所以两核和四核运行版本都没有达到能耗效率的平衡点。四核 Java 情景中的
能耗效率相当高（0.94）。对于 PARSEC或Java 工作负载，以ILP为中心的处理器很可能需要更
多的功率才能实现相似的加速比。因此，在提高这些应用程序的性能方面，以TLP 为中心的方
法当然也会优于以ILP为中心的方法。

\subsubsection{将多核与SMT结合起来}
最后，我们通过测量两组基准测试在2~4个处理器、1~2个线程（总共4个数据点、最
多8个线程）情况下的结果，来研究多核与多线程的组合方式。图5-21给出了在处理器数目为
2或4、使用和未使用SMT时，在 Intel i7上获得的加速比和能耗效率，采用调和均值来汇总两
组基准测试的结果。显然，如果在多核情景下也有足够的线程级并行，SMT是可以提高性能的。
例如，在四核无 SMT 情景中，Java 和 PARSEC的加速比效率分别为 67\%和76\%。在采用 SMT、
四个核心时，这些比值达到了令人惊讶的 83\%和97\%！

本图给出了在有、无SMT 时，以两核和四核处理执行并行Java 和 PARSEC工作负载的加速
比。注意，以上结果是在线程数由2变为8时获得的，反映了体系结构的影响和应用程序的特征。
汇总结果时采用了调和均值，如图5-20的图题所述

能耗效率给出了一幅稍有不同的画面。对于 PARSEC，加速比在四核 SMIT情景中（8个线
程）基本上为线性，功率的增长要更慢一些，从而使这种情景中的能耗效率达到1.1。Java情景
要更复杂一些；两核心SMT（四线程）运行时的能耗效率峰值达到 0.97，在四核心 SMT（8线
程）运行时下降到 0.89。在部署4个以上的线程时，Java 基准测试非常有可能遭遇 Amdabl 定律
效应。一些架构师已经观察到，多核处理器将提高性能（从而提高能耗效率）的更多责任转嫁
给程序员，Java 工作负载的结果显然证实了这一点。

\section{谬论与易犯错误}
由于对并行计算的理解不够成熟，所以存在大量的隐藏易犯错误，这些错误要么会被一些
细心的设计人员发现，要么会被一些倒霉的设计人员碰上。由于近年来围绕多处理器进行了大
量夸大宣传，所以也存在着很多常见谬论。我们选择其中一些列出如下。

\textbf{易犯错误 通过随执行时间线性变化的加速比来测量多处理器的性能。}
“迫击炮射击”曲线长期用于判断并行处理器的成功与否。（这种曲线显示了线性加速、稳
定和最后的下降过程。）尽管加速比是并行程序的一个方面，但它并不是性能的直接度量。第一
个问题是所打展处理器的功率：一个程序的性能线性提高到相当于100个 Intel Atom 处理器（在
上网本上使用的低端处理器），它的速度可能慢于在一个八核 Xeon上运行的版本。对于浮点计
算密集的程序一定要尤其小心，没有硬件辅助的处理元件也许能够很好地扩展，但整体性能却
可能很差。

只有在对比每种计算机上的最佳算法时，对比执行时间才是公平的。在两个计算机上对比
相同代码，看起来可能是公平的，但实际并非如此；并行程序在单处理器上的运行速度可能比
顺序执行版本要慢一些。开发并行程序有时可能会导致算法方面的改进，所以将过去人们熟知
的顺序程序与并行代码进行对比（这看起来是公平的）时，比较的并不是等价算法。为了反映
这一问题，有时会使用相对加速比（同一程序）和真实加速比（最佳程序）等术语。

如果结果中呈现超线性性能，也就是说一个程序在n个处理器上运行时，其速度要比在等
同单处理器上的运行速度快n倍以上，那就表明这种对比可能是不公平的，尽管在某些情况下
已经遇到了“真实的” 超线性加速比。例如，一些科学应用程序通常会在小幅增加处理器数目
时（2或4增加到8或16），实现超线性加速比。这些结果的出现通常是因为一些关键性的数据
结构无法放入拥有2个或4个处理器的多处理器中的聚合缓存，但却可以放入拥有8个或16个
处理器的多处理器的聚合缓存中。

总而音之，要通过对比加速比来对比性能，运气好时也需要很强的技巧性，运气不好时可
能会造成误导。对比两种不同多处理器的加速比并不一定能够告诉我们有关这些多处理器相对
性能的相关信息。甚至在相同处理器上对比两种不同算法也需要一些技巧，这是因为我们必须
使用真正的加速比而不是相对加速比来获得有效的对比结果。

\textbf{谬论 Amdahl定律不适用于并行计算机。}
1987年，某个研究组织的领导人宣布，Amdabl 定律（见1.9节）已经被 MIIMD多处理器打
破。但是，这一声明并不意味着这一定律已经被并行计算机推翻；程序中一些被忽略的部分仍
然会对性能产生限制。为了理解这些媒体报道的基础，让我们看看 Amdahl［1967］最初是怎么
说的。

\begin{tcolorbox}
    在这里，我们可以得出一个相当明确的结论：如果不能以近乎相同的幅度来提高串行
    处理速率，那在提高并行处理速率方面所做的努力都是徒劳无功的。［P483］
\end{tcolorbox}

对这一定律的一种解释就是：由于每个程序都有申行执行的部分，所以对于经济合理的处
理器数目会有一个上限，比如说100个。如果在使用1000个处理器时仍然呈现线性加速比，那
就证明对Amdahl定律的这一解释是错误的。

“Amdahl 定律已被‘推翻’”这一表述的基础就是使用扩展加速比（scaled speedup），也称
为弱扩展（weak scaling）。研究人员对基准测试进行了扩展，使数据大小增大到1000倍，
并对比了扩展后基准测试的单处理器与并行执行时间。对于这一特定算法，程序的顺序执行部
分恒定，与输人的大小无关，而其余部分是完全并行的，因此在采用1000个处理器时的加速比
是线性的。因为运行时间的增长速度要长于线性增长速率，所以这个程序在扩展之后的运行时
间要长一些，即使采用1000个处理器也是如此。

在对输人进行扩展的情况下测得的加速比不同于真正的加速比，将其当作真正加速比会造
成误导。由于并行基准测试经常在不同规模的多处理器上运行，所以明确指出允许何种应用程
序扩展以及如何完成扩展是很重要的。让数据规模随处理器数目的增大而扩展，在大多数情况
下并不恰当，但是当处理器数目大量增加时（称为强扩展），如果继续采用固定规模的问题，通
常也是不恰当的，这是因为当我们向用户提供一个大得多的多处理器时，他们通常会选择运行
应用程序的更大、更详细版本。关于这一重要主题的更多讨论，请参阅附录I。

\textbf{谬论 需要线性加速比才能提高多处理器的成本效率。}

人们普遍认同，并行计算的主要优点之一是：即使与最快速的单处理器相比，也能在更短
的时间内给出计算结果。但是，许多人也持有这样的观点：并行处理器不可能实现与单处理器
一样的成本效率，除非它们能够实现完美的线性加速比。这种观点认为，由于多处理器的成本
是处理器数目的线性函数，所以只要低于线性加速比，就意味着性能/价格比下降，使并行处理
器的成本效率低于使用单处理器的情况。

这种观点的问题在于：成本不仅是处理器数目的函数，也依赖于系统的存储器、1/O和开销
（机箱、电源、互连，等等）。在多核时代里，每个芯片上有多个处理器，这一观点就更没有什
么意义了。

在系统成本中包含存储器的影响是由Wood 和Hil［1995］指出的。我们使用的例子将以最近
使用TPC-C和 SPECRate 基准测试获得的数据为基础，但利用并行科学应用程序工作负载也可
以得出同一结论，甚至可以更强烈地表达这一观点。

图5-22 给出了 TPC-C、SPECintRate 和 SPECfpRate 在 IBM eServerp5 多处理器上的加速比，
此多处理器配有4~64个处理器。图中显示，只有TPC-C获得了优于线性加速比的结果。对于
SPECintRate 和 SPECfpRate，加速比低于线性加速比，成本也是如此，这是因为它们与 TPC-C
不同，主存储器和所需磁盘数目的扩展也都低于线性扩展。

如图5-23所示，与四处理器配置相比，更多的处理器数目实际上可能更具成本效率。在对
比两个计算机的性价比时，必须确保准确评估了总系统成本和可以达到的性能。对于许多具有
重高存储器需求的应用程序，这种对比可能极大地增加了使用多处理器的吸引力。

三种基准测试在IBM eServer p5 多处理上的加速比，这个多处理器分别配有4、

图5-23 三个基准测试在包含4~64个处理器的 IBM eServer p5多处理器上执行时，其相对于四处理霧
系统的性能价格比表朋：处理器数据较大时，也可能实现与四处理器配置一样的成本效率。对
于 TPC-C，这一配置与官方运行时使用的配置相同，这就意味着磁盘与存储器随处理器数目线
性扩展，包含64 个处理器的机器，其成本大约是32处理器版本的两倍。与之相对，磁盘和存
储器的扩展速度更慢一些（不过，仍然快于在64个处理器上实现最佳SPECRate 所需要的速度）。
具体来说，磁盘配置从4处理器版本的一个驱动器扩展为64处理器版本的四个驱动器（140GB）。
存储器从4处理器版本的8GB 发展到64处理器系统的20 GB

\textbf{易犯错误 不要开发利用多处理器体系结构或针对此种结构进行优化的软件。}
软件开发长期滞后于多处理的发展，可能是因为软件问题要困难得多。我们给出一个例子
来说明这些问题的微妙之处，但实际有许多例子可供我们选择。

在将.一个为单处理器设计的软件应用于多处理器环境时，经常会遇到一个问题。例如，2000
年的SGI操作系统最初是用单个锁来保护页表数据结构的，当时认为页面分配的出现频率较低。
在单处理器中，这种做法并不会带来性能问题。但在多处理器中，可能会成为某些程序的主要
性能瓶颈。考虑一个程序，它使用了大量在启动时初始化的页面，UNIX 对于静态分配页面就
是这样做的。假定实现了这个程序的并行化，由多个进程来分配这些页面。由于页面分配需要
使用页表数据结构，而这种结构只要处于使用状态就会被锁定，如果这些进程都试图同时分配
它们的页面（这正是我们希望在初始化时做的工作！），即使是允许在操作系统中存在多个线程
的操作系统内核也会被串行化。

这种页表申行化清除了初始化过程中的并行，对整体并行性能有非常严重的影响。这一性
能瓶颈即使在多重编程时也仍然存在。例如，假定我们将并行程序分散到独立进程中，然后在
每个处理器上运行一个进程，从而在进程之间不存在共享。（这就是一位用户所做的工作，因为
他合情合理地相信这一性能问题是由于其应用程序中的意外共享或干扰造成的。）遗憾的是，那
个唯一锁仍然会使所有进程串行化，所以即便是多重编程性能也非常糟糕。这个易犯错误指明，
在多处理器上运行软件时，可能会出现一些虽很微小但却影响巨大的性能觖陷。和所有其他关
键的软件组件一样，操作系统算法和数据结构在多处理器上下文中也都必须进行重新考虑。在
页表的较小部分上设置锁，可以有效地消除这一问题。在存储器结构中也存在类似问题，在没
有实际发生共享时，会增大一致性通信流量。

由于多核心处理器已经成为从桌面计算机到服务器等各个领域的主旋律，所以并行软件中
的投人不足已经变得非常明显。由于重视不够，所以可能还要等待多年之后，我们使用的软件
系统才能充分利用这些不断增加的核心。

\section{结语}
30多年来，研究人员和设计人员一直在预测单处理器会终结，会被多处理器超越。但直
到本世纪的前几年，这一预测还总被证实是错误的。在第3章已经看到，尝试寻找和利用更多
ILP 的成本在效率上是难以承受的（在硅面积和功率方面都是如此）。当然，多核并没有解决
功率问题，因为它显然增加了晶体管数和晶体管开关的活动数目，而这正是功率消耗的两个主
要因素。

但是，多核的确改变了这场游戏。因为允许将空闲核心置于功率节省模式，所以也可以在
功率效率方面进行一些改进，本章的结论已经证明了这一点。更重要的是，多核技术将保持处
理器繁忙的重担更多地交给了 TLP，而不再是依靠 TLP，TLP 由应用程序和程序员负责确认，
而ILP 则由硬件负责。我们已经看到，这些差别显然在Java 与PARSEC 基准测试的多核性能与
能耗效率方面进一步扩大。

尽管多核技术为克服能耗效率方面的问题提供了.些直接帮助，并将大部分重担移交给软
件系统，但仍然存在一些难度很大的挑战和尚未解决的问题。例如，人们也在尝试利用积极推
测的线程级版本，但到目前为止，这些努力的结果与尝试利用 ILP的努力面临着相同的命运。
也就是说，性能有所提高，但不是特别明显，可能低于能耗的增大幅度，所以诸如推测线程或
硬件先行 （run-ahead）之类的思想都没有成功地整合到处理器中。和ILP 的推测一样，除非推
测结果总是正确的，否则其成本就会超过其收益。

除了编程语言和编译器技术的重要问题之外，多核技术已经重新开放了计算机体系结构中
另一个长期存在的问题：是否值得考虑异构处理器？尽管现在还没有提交这种多核处理器，而
且异构多处理器仅在专用计算机或嵌人式系统中取得了有限的成功，但它在多核心环境中的可
能性要高得多。和多重处理中的许多问题一样，其答案可能依赖于软件模型和编程系统。如果
编译器和操作系统可以有效地使用同构处理器，它们将会变得更加主流。目前，对于许多应用
程序来说，现有编译器的能力还不足以有效地应对中等数量的同构核心所带来的压力，但有一
类拥有异构核心的多处理器变得越来越常见，它们在功能方面有明显不同，而且有一些用于分
解应用程序的明确方法。这些多处理器包括诸如 GPU 和媒体处理器之类的专用处理单元。对能
耗效率的重视还可能促使多处理器中包含一些具有不同性能功率比的核心。

在本书的1995年版本中，我们在这一章的末尾讨论了当时极具争议的两个问题。
（1） 基于微处理器的超大规模多处理器将会使用哪种体系结构？
（2） 多重处理在未来的微处理器体系结构中扮演何种角色？

这些年来，这两个同题已经大体得到解决。

因为超大规模多处理器没有变成一个不断发展的主流市场，所以目前构建这种大规模多处
理器的唯一具有成本效率的方法就是使用集群，在集群中，各个节点或者是单个多核微处理器，
或者是小规模、共享存储器的多处理器（通常有2~4个多核处理器），互连网络采用标准网络
技术。这些集群已经扩展到数万个处理器，并且安装在专门设计的“仓库”计算机中，下一章
将讨论这一主题。

第二个问题的答案在最近六七年前已经变得非常清晰：多处理器未来性能的增长将源于多
核处理器对线程级并行的开发，而不是通过开发更多的ILP。

因此，核心已经成为芯片的新构建模块，供应商提供了各种不同的芯片，它们以单核设计
为基础，采用了不同数目的核心和L3缓存。例如，表S-11 给出了仅使用 Nehalem 核心（用于
Xeon 7560和i中）构建的 Intel处理器系列。

表5-11 各称基于Nehalem微体系结构的Intel处理暴的特性参数
处理鬋
系列
核心
L3绶存
功率（典型）
时钟频率（GHz）
Xeon
7500
8
18~24 MB
130 W
2~2.3
Xeon
5600
4~6（有、无SMT）
12 MB
40~130 W
1.86~333
Xeon
3400~3500
4（有、无SMT）
8 MB
45~130 W
1.86~33
Xeon
5500
2~4
4~8MB
80~130 W
1.86~3.3
辽7
860~975
4
8 MB
82~130 W
2.53~3.33
i7 mobile
720~970
4
6~8 MB
45~55 W
1.6~2.1
i5
750-760
4（无SMT）
8 MB
80 W
2.4-2.8
i3
330~350
2（有、无SMT）
3 MB
35W
2.1~2.3

*本表仍然在每一行中包含了许多项（2~8项）。其中所列价格是每单达到1000个时的单价。
价格（美元）
2837~3692
440~1663
189~999
80~1600
284~999
364~378
196~209

在20世纪80年代和90年代，随着IP的诞生与发展，一些软件通过优化编译器而天ILP，
成为其成功的关键。与此类似，要想成功开发线程级并行，也要依赖于适当软件系统的发愈
就像依赖于计算机体系结构一样。在过去的30多年里，并行软件发展迟缓，所以在未来几军里
广泛开发线程级并行仍然富有挑战性。此外，作者相信，出现最佳多核体系结构的机会非常大。
为了设计此类体系结构，架构师需要定量设计训练，并能够准确地为数千个运行数万亿条指令
的核心建立模型，包括大规模应用程序和操作系统。如果没有这种能力和方法，架构师就像是
在黑暗中射击。有时可能会碰上好运，但更常见的还是锴过目标。
\section{历史回顾与参考文献}
附录 L.7节考察了多处理器和并行处理的历史。根据时间段和体系结构的划分，附录1.7
节讨论了早期的试验性多处理器和在并行处理中的一些著名争论。其中也介绍了最近的进展，
并给出了供扩展阅读的参考文献。

案例研究与练习（Amr Zaky 和 David A.Wood 设计）
案例研究 1：单芯片多核多处理
本案例研究说明的概念
\begin{itemize}
    \item 监听式一致性协议转换
    \item 一致性协议的性能
    \item 一致性协议的优化
    \item 同歩
    \item 存储器连贯性模型的性能
\end{itemize}
图5-24所示的简单多核多处理器给出了一种经常实现的对称共享存储器体系结构。每个处理器拥有
单个专用缓存，使用图5-4的监听式一致性协议来保持一致性。每个缓存都是直接映射的，共有四个块，
每个块保存两个字。为了简化说明，缓存地址标签中包含了完整的地址，每个字仅显示两个十六进制字符，
最低有效字位于右侧。一致性状态表示为 M、S和I（已修改、共享和无效）。
5.1 ［10/10/10/10/10/10/10］<5.2>对于这一练习的每一部分，假定初始缓存与存储器状态如图5-24所
示。这个练可的每一部分以如下形式指定由一或多个 CPU操作组成的序列：
\begin{verbatim}
    P#：sop> saddress> ［svalue>］
\end{verbatim}
其中，指明 CPU（例如，PO）、s0p>是CPU操作（例如，读取或写入）、saddress>表示存
储器地址、<value~指示在写人操作时指定的新字。将以下每个操作看作是独立应用于图S-24
给定的初始状态。在给定操作之后，缓存和存储器的结果状态是怎样的（即，一致性状态、标
签和数据）？仅给出发生变化的块，例如，PO.B0：（I,120.00 01）表示 CPU PO 的BO 块最終状态
为I，标签为120，数据字为00和01。另外，每个读取操作返回什么样的值？
\begin{verbatim}
    8. ［10］ <5.2>
    PO: read 120
    b.［10］<5.2>
    PO:write 120 <-- 80
    C.［101 ≤5.2>
    P3:write 120 <-- 80
    d. ［10］ <5.2>
    P1:read 110
    e. 110］<5.2>
    PO: write 108 <-- 48
    f.［101<5.2>
    PO: write 130 ≤-- 78
    8［10］<5.2>
    P3:write 130 <--78
\end{verbatim}
案例研究与练习（Amr Zaky 和 David A.Wood 设计）
307
PO
P1
P3
存储器
图5-24 多核（点对点）多处理
5.2 ［20/20/20/20］<5.3>监听式缓存一致性多处理器的性能取决于许多具体的实施问题，它们决定了
缓存能够以何种速度作出回应，提供处于独占或 M状态的块中数据。在一些实施方式中，当一
个缓存块在另一个处理器的缓存中处于独占状态时，对这个块的CPU读取缺失要快于存储器中
一个块的缺失。这是因为缓存要小于主存储器，所以速度也就更快一些。与之相反，在某些实
施中，由存储器提供数据的缺失要快于由缓存提供数据的缺失，这是因为缓存通常是针对“前
端”或 CPU引用进行优化的，而不是针对“后端”或监听式访问进行优化的。对于图5-24所
示的多处理器，考虑在单个 CPU上执行一系列操作，其中：
\begin{itemize}
    \item CPU读取和写入命中不会产生停顿时钟周期；
    \item CPU 读取和写人缺失在分别由存储器和缓存提供数据时，生成N存條油和 N *个停顿周期；
    \item 生成失效操作的 CPU写入命中导致 N**个停顿周期；
    \item 由于冲突或另一个处理器请求独占块而造成写回块时，会另外增加Nz个停顿周期。
\end{itemize}
考虑两种实现方式，它们的性能参数不同，汇总于表5-12中。考虑以下操作序列，假定其
初始缓存状态如图 5-24所示。为简便起见，假定第二个操作在第—个操作完成之后开始（即使
它们由不同处理器执行时也是如此）：

P1: read 110
P3: read 110

对于实现方式1，由于第一次读取是由 PO 的缓存提供数据，所以它产生50个停顿周期。
P1 在等待这个块时停顿40个周期，PO在回应 PI 的请求将其写回存储器时，停顿10个周期。
之后，P3的第二次读取生成100个停顿周期，因为它的缺失是由存储器提供数据，这个序列总
共生成150个停顿周期。对于以下操作序列，每个实现方式生成多少个停顿周期？
表5-12 监听一致性延迟
参
数
实现方式1
100
N 林
N失*
Nsm
40
I5
10
实现方式2
100
130
15
10
5.3
5.4
a.［20］<5.3>
PO:read 120
PO: read 128
PO:read 130
b.120］<5.3>
PO:read 100
PO:write 108 <--48
PO:write 130 <-- 78
c. ［201 <5.3>
P1:read 120
P1:read 128
P1: read 130
d. ［20］ <5.3>
P1:read 100
P1:write
108 ~-- 48
Pl: write 130
≤- 78
［20］<5.2>许多监听一致性协议拥有更多的状态、状态转换或总线事务，以减少保持缓存一致性
的开销。在练习 5.2的实现方式1中，当峡失数据由缓存提供时，缺失导致的停顿周期要少于
由存储器提供数据时的停顿周期。一些一致性协议尝试通过提高这一情况的出现频率来提高性
能。一种常见的协议优化方法是引入被拥有状态（通常表示为 O）。在那些可能仅读取“被拥
有”块的节点中，“被拥有”状态的表现类似于“共享”状态，但在某些节点中，必须在其他
节点发生对“被拥有”块的读取和写人缺失时提供数据，“被拥有”状态的表现类似于“已修
改”状态。对处于“已修改”状态或“被拥有”状态的块发生读取续失时，将向发出请求的块
提供数据，并转换为“被拥有”状态。对处于“已修改”或“被拥有”状态的块发生写人敏失
时，将向发出请求的节点提供数据，并转换为“失效”状态。仅当某个节点替换了处于“已修
改”或“被拥有”状态的块时，这种经过优化的MOSI 协议才会更新存储器。画出带有附加状
态和转换的新协议图。
［20/20/20/20］<5.2>对于以下代码序列及表5-12中两种实现方式的定时参数，计算基本MSI协
议和练习5.3中优化 MOSI 协议的总停顿周期。假定不需要总线事务的状态转换不会导致额外
的停顿周期。
a. ［201<5.2>
PO: read 110
P3：
read 110
PO: read 110
b. ［201 <5.2>
P1:read 120
P3:read 120
PO:read 120
c.［201<5.22
PO:write 120 <--80
P3:read 120
PO:read 120
d.［201<5.2>
PO:write 108 <--88
P3:read 108
PO: write 108 <-- 98
5.5 ［201 <5.2>一些应用程序首先读取大型数据集，然后修改其中的大多数或全部数据。基本 MSI
一致性协议将提供所有处于共享状态的缓存块，然后被迫执行失效操作，将它们升级为“已修
5.6
5.7
5.8
案例研究与练习（Amr Zaky 和 David A.Wood设计）
309
改”状态。额外的延迟会对一些工作负载产生严重影响。通过添加一项协议优化，不再需要对
那些由同一处理器先读后写的数据块进行更新。这一优化向协议中添加了 “独占”（E）状态，
表示所有其他节点都没有这个块的副本，但它还没有被修改。当存储器为读取缺失提供数据，
而且其他节点都没有有效副本时，缓存块即进入“独占”状态。CPU对这个块的读取和写人会
继续进行，但没有进一步的总线通信流量，但CPU写人操作会将一致性状态转换为“已修改”。
“独占”状态不同于“已修改”状态，节点可能会悄无声息地替换“独占”块（而“已修改”块
则必须写回存储器）。另外，对“独占”块的读取缺失会导致该块转换为“共享”状态，但不
会要求节点以数据作出回应（因为存储器中拥有最新副本）。绘制MESI 协议的新协议图，在
其中添加“独占”状态，以及向基本 MIS 协议的“已修改”、“共享”和“无效”状态的转换。
［20/20/20/20/20］<5.2>假定图5-24的缓存内容和表5-12 中实现方式1的定时参数。以下代码序
列在基本协议和练习5.5的新 MESI 协议中的总停顿周期为多少？假定不需要互连事务的状态
转换不会导致额外停顿周期。
\begin{verbatim}
    a.［20］ <5.2> PO: read 100
    PO: write 100 <-- 40
    b. 1201 <5.2>
    PO:read 120
    PO: write 120 <-- 60
    c. ［20］ <5.2>
    PO: read 100
    PO: read 120
    d. ［20）<5.2>
    PO: read 100
    P1:write 100 <-- 60
    €. ［20］<5.2>
    PO: read 100
    PO: write 100 <-- 60
    P1: write 100 <--40
\end{verbatim}
［20/20/20/20］ ＜S.5>在大多数商用共享存储器机器中，自旋锁可能是最简单的同步机制。这个自
旋锁依靠交换原语来自动载入旧值和存储新值。锁定例程重复执行此交换操作，直到它发现未
锁定的锁为止（即返回值为0）：
\begin{verbatim}
    DADDUI R2,RO，#1
    1ockit：
    EXCH R2,0（R1）
    BNEZ R2, lockit
\end{verbatim}
要解决一个自旋锁，只需要存储数值0即可：
unlock: SW RO,0（R1）
如5.5节中的讨论，经过更多优化的自旋锁利用缓存一致性，并使用载人操作来检查这个锁，
允许它以缓存中的共享变量进行自旋：
\begin{verbatim}
    lockit：
    LD
    R2, 0（R1）
    BNEZ
    R2, lockit
    DADDUI RZ,RO，#1
    EXCH
    R2,0（R1）
    BNEZ
    R2, lockit
\end{verbatim}
假定处理器PO、P1 和P3都尝试获取位于地址Ox100的一个锁（即：寄存器R1 保存着数值0x100）。
假定缓存内容如图 5-24所示，定时参数如表5-12 中的实现方式1所示。为简便起见，假定关
键部分的长度为1000个时钟周期。
2.［20］<5.5>使用简单自旋锁，判断每个处理器在获取该锁之前大约导致多少个存储器停顿周期。
b.［20］<S.5>使用优化自旋锁，判断每个处理器在获取该锁之前大约导致多少个存储器停顿周期。
c.［20］<5.5>使用简单自旋锁，大约导致多少个互连事务？
d.［20］<5.5>使用“测试、测试并置位”自旋锁，大约导致多少个互连事务？
［20/20/20/20］<5.6>顺序一致性（SC）要求所有读取和写人都是按某一总体顺序执行的。这就需
要处理器在某些特定情况下，在提交读取或写人指令时停顿下来。考虑以下代码序列：
write A
read B
其中 write A 导致一次缓存缺失，read B导致一次缓存命中。根据SC，处理器必须暂停 read B，
直到它可以排定 write A（从而可以执行该操作为止）。SC的简单实现将使处理器停顿，直到
缓存接收到数据，并可以执行写人操作为止。较弱的一致性模型放松了对读取和写人的排序约
束条件，减少了处理器必须停顿的情况。总体存储顺序（TSO）一致性模型要求所有写入操作
都按某一总体顺序执行，但允许处理器的读取操作越过自己的写入操作。这就允许处理器实现
写缓冲区，其中包含已经提交的写人操作，但这些写人操作还没有针对其他处理器的写入操作
进行排序。在 TSO 中允许读取操作眺过写缓冲区（这在SC中是不允许的。）假定每个时钟周
期可以执行一次存储器操作，而且那些在缓存中命名或者可以由写人缓冲区提供数据的操作不
会导致停顿周期。未能命中的操作将导致表5-12所列的延迟。假定有图5-24所列的缓存内容。
对于SC和TSO一致性模型，在每个操作之前有多少个停顿周期？
a. ［20］ <5.6>
PO: write 110 <--80
PO: read 108
b. ［20］<5.6>
PO: write 100
-- 80
PO:read 108
C. ［20］ <5.6>
PO:write 110
＜-- 80
PO:write 100
-- 90
d. ［20］<5.6>
PO: write 100 <-- 80
PO: write 110 --90
案例研究 2：简单的目录式一致性
本案例研究说明的概念
\begin{itemize}
    \item 目录式一致性协议转换
    \item 一致性协议的性能
    \item 一致性协议的优化
\end{itemize}
考虑图 5-25所示的分布式共享存储器系统。它由两个四核芯片组成。每个芯片中的处理器共享L.2
缀存（L2\$），两个芯片通过一个点对点互连连接在一起。系统存储器分散在两个芯片上。图5-26放大了
这个系统的一部分。Pi.j 表示芯片j上的处理器1。每个处理器有单个直接映射的LI缓存，其中包括两
个块，每个块包括两个字。每个芯片有单个直接映射的L.2缓存，其中包括两个块，每个块包括两个字。
为了简化图示，缓存地址标签包含完整地址，每个字仅给出两个十六进制字符，最低有效字在右侧。L.1
缓存状态用M、S和I标识，分别表示“已修改”、“共享”和“失效”。L.2缓存和存储器都有目录。目
录状态用 DM、DS和 DI标识，分别表示“已修改目录”、“共享目录”和“失效目录”。这一简单目录
式协议在图5-15和图5-16中描述。L.2目录列出了本地共享者/拥有者，如果某一行在另一芯片中进行外部
共享，则还会增加一些记录；例如，P1.0:E 表示某一行由本地处理器P1.0 共享，在某一其他芯片上进行
外部共享。存储器目录中有一个清单，列出了某一行的芯片共享者/拥有者；例如CO，C1 表示某一行在芯片
0和芯片1中共享。
5.9
［10/10/10/10/15/15/15/15］<5.4>对于本练习的每一部分，假定初始缓存状态与存储器状态如图
5-26所示。本练习的每一部分指定了一或多个 CPU操作组成的序列，其形式如下：
\begin{verbatim}
    P#：<op> caddress>［<-- svalue>］
\end{verbatim}
其中，指明CPU（例如，PO,0）、<0p~是 CPU操作（例如，读取或写人）、saddress>表示存
储器地址、<value~指示在写人操作时指定的新字。在给定的 CPU操作序列完成之后，缓存和
存储器的最终状态如何（即，一致性状态、共享者/拥有者、标签和数据）？还有，每个读取操
作返回什么值？
a. ［10］<5.4> P0,0: read 100
b. ［10］ <5.4>
c.［101 <S.4>
d. ［01 <5.4>
e. ［157 ¢5.4>
f.［15］<5.4>
8.1151 <5.4>
h.［1S1<5.4>
紫例研究与练习（Amr Zaky 和DavidA.Wood设计）
311
PO.0:read 128
PO,0:write 128 <-- 78
PO,0: read 120
PO.0:read 120
P1,0: read 120
PO,0:read 120
P1,0:write 120 <-- 80
PO,0:write 120 <-- 80
P1,0:read 120
PO,0: write 120 <-- 80
P1.0: write 120 <-- 90
芯片0
芯片1
PO
P1
PO
PI
P3
P3
L2$
L2$
P0,0
MO
MI
留 5-25
带有 DSM 的多芯片、多核多处理醬
PO,1
P3,1
L2$，0
L2$，1
MO
MI
图 5-26 多芯片多核处理話中的缓存与存储器状态
5.10 ［10/10/10/10］<5.4~目录式协议比监听式协议的可扩展性更强，因为它们会向那些拥有块副本的
节点发送显式请求和失效消息，而监听式协议则向所有节点广播所有请求和失效消息。考虑图
5-25所示的八处理器系统，假定所有未显示缓存拥有失效块。对于下面的每个序列，确认些
420
312
第5章 线程級并行
节点（芯片/处理器）接收每个请求和失效消息。
a.［10］<5.4>
PO,0:write 100 <-- 80
b. 1101 <5.4>
PO,O:write 108 <--88
c.［101 <5.4>
PO,0:write 118 <-- 90
d. ［101≤5.4>
P1,0: write 128 <-- 98
5.11 ［25］<5.4练习5.3要求向简单的 MSI 监听协议中添加了“被拥有”状态。重复这一问题，但采
用以上简单的目录协议。
5.12 ［25］<5.4>试讨论为什么利用简单目录协议添加“独占”状态要比采用监听协议时困难得多。举
出一个例于。
案例研究3：高级目录式协议
本案例研究说明的概念
口 目录式一致性协议实施方式
口一致性协议的性能
口 一致性协议的优化
案例研究 2中的目录一致性协议抽象地描述了目录式一致性，但假定采用与简单监听系统非常一致
的原子转换。高性能目录系统采用流水化、交换互连，极大地提高了带宽，但也引入了过渡状态和非原子
化转换。目录式缓存一致性协议的可扩展性高于监听式缓存一致性协议，原因有两个。第一，监听式缓存
一致性协议向所有节点广播请求，限制了其可伸缩性。目录式协议使用一个间接层级（向目录中发送的消
息）来确保仅向拥有块副本的节点发送这些请求。第二，监听式系统的地址网络必须按总体顺序提供请求，
而目录式协议可以放松这一约束条件。一些目录式协议假定没有网络排序，这样就允许采用自适应路由技
术来提高网络带宽，所以是有益的。其他协议依靠点对点排序（即，从节点 PO发到节点PI 的消息将按顺
序到达）。即使有这样一个排序约束条件，目录式协议的过渡状态通常也要多于监听式协议。表5-13给
出了一种简化目录式协议的缓存控制器状态转换，这种协议依赖于点对点网络排序。表5-14 给出了目录
控制器的状态转换。
\begin{verbatim}
    状态
    I
    s
    M
    读取
    发送
    GetS/ISD
    进行
    读取
    进行
    读取
    写入
    发送
    GetM/IMAD
    发送
    GeiN/IM D
    进行写入
\end{verbatim}
表5-13 广播监听式缓存控制暑转换
\begin{verbatim}
    Forwarded Forwarded
    替换
    错误
    _Gets
    _GetM
    错误
    锴误
    I
    发送
    Put/MI^
    失效
    发送
    Ack/I
    发送
    Ack/
    错误
    PutM_
    Ack
    错误
    Data
    锴误
    Last
    Ack
    错误
    错误
    错误
    错误
    错误
    错误
    ISP
    z
    z
    z
    发送
    Ackvisi”
    发送数
    据，发送
    PaMS/MIS*
    错误
    发送数据！
    错误
    错误
    错误
    错误
    错误
    Isie
    z
    z
    z
    发送
    Ack
    错误
    错误
    错误
    IMA
    z
    Z
    z
    IM^
    Z
    发送
    Ack
    错误
    镨误
    错误
    错误
    IMS^
    IMI^
    错误
    保存数
    据，进行
    读取/S
    保存数
    据，进行
    读取/
    保存数
    据，IMA
    错误
    错误
    错误
    错误
    进行读
    取/
    ．.
\end{verbatim}
案例研究与练习（Amr Zaky 和DavidA.Wood设计）
\begin{verbatim}
    313
    状态
    IMI
    读取
    写入
    替换
    z
    Z
    Z
    失效
    错误
    Forwarded Forwarded
    _GetS
    _GetM
    错误
    错误
    PUtM_
    Ack
    错误
    Data
    错误
    IMS^
    Z
    Z
    Z
    发送
    Ack/IMIY^
    z
    错误
    错误
    MS^
    z
    Z
    锴误
    发送数据
    错误
    （续）
    Last
    Ack
    ‘进行读
    取，发送
    数据几
    进行写
    人/发送
    数据/S
    错误
    MI^
    进行
    读取
    Z
    Z
    Z
    错误
    发送数据
    发送数据
    ^S
    MI^
    发送数据11I
    错误
    错误
    状态
    DI
    DS
    DM
    GetS
    发送数据、
    加至共享者
    /DS
    发送数据、
    加至共享者
    /DS
    转发GetS、
    加至共享者
    DMS”
    DMS"
    转发GetS.
    添加到共
    享者
    GetM
    发送数据、清除
    共享者，发送拥
    有者/DM
    向找享者发送
    IV，设置拥有
    者，发送数据/DM
    转发GetM，向共
    享者发送INV，清
    除共享者，设置
    拥有者
    转发GetM，向共
    享者发送INV，潰
    除共享者，设置
    拥有者/DM
\end{verbatim}
表5-14 目录控制器转换
\begin{verbatim}
    Put M
    （拥有者）
    错误
    PutMS
    （非拥有者）
    发送PutM_Ack
    PutM
    （拥有者）
    错误
    PutS
    （非拥有者）
    发送PutM_Ack
    错误
\end{verbatim}
\begin{verbatim}
    发送PutM_Ack错误
    发送FutM_Ack
    保存数据、发送
    PutM_Ack/DI
    发送PutM_Ack
    保存数据，添加
    到共享者，发送
    PutM_Ack/DS
    发送PutM_Ack
    保存数据.发送
    PuttM_AckDS
    发送PutM_Ack
    保存数据，添加
    到共享者，发送
    PutM_Ack/DS
    发送PutM_Ack
\end{verbatim}
对于每个块，目录中都保存着一个状态和当前拥有者字段，或者当前共享者列表（如果有的话）。
为了以下讨论及后继问题，假定L2缓存被禁用。假定存储器目录以处理器为单位列出了共享者/拥有者。
例如，在图5-26中，第108行的存储器目录为“PO， 0:P3.0”，而不是“CO.CI”。另外，假定这些消
息可以通过透明方式跨越芯片边界（如果需要的话）。
各行按当前状态索引，各列按事件索引，决定了<操作/后续状态>元组。如果仅列出一个后续状态，则
不需要操作。不可能出现的情景标以“错误“，表示错误条件；“z意味着当前不能处理所请求的事件。
以下示例说明了这一协议的基本操作。假定处理器尝试对处于1状态（失效状态）的块进行写人操作。
相应的元组为 “发送 GetM/IMAD”
，表示缓存控制器应当向目录发送 GetM （GetModified.）请求，并转换为状
态IAD。在最简单的情最中，请求消息找到处于 DI（目录失效）状态的目录，表示所有其他缓存都没有副本。
目录以 Data 消息作出回应，其中也包含了所希望 Acks 的数目（在此情景中为0）。在这一简化协议中，缀存
控制器将这一条消息看作两条消息：一个 Data 消息，后面跟着一个 Last Ack 事件。首先处理 Data 消息，保存
数据并转换到 IV。然后处理Last Ack事件，转换为状态 M。最后，可以以状态M执行写人操作。
如果 GetM发现处于 DS（目录共享）状态的目录，则该目录向共享者列表中的所有节点发送
Iavalidate（INV）消息，向拥有共享者编号的请求者发送数据，并转换为M状态。当 INV消息到达共享者时，
它们会找到处于S状态或I状态（如果它们已经悄悄地使这个块失效）的块。在任何一种情况下，共享者
都会直接向发出请求的节点发送一条 Ack。请求者将会计算它接收到的Ack数，并与随 Data 消息发回的
数字进行对比。当所有 Ack都已收到后，发生 Last Ack 事件，触发缓存向 M状态的转换，并允许写人操
作继续进行。注意，所有Ack 有可能在 Data 消息之前已经到达，但Last Ack 事件并不会发生。这是因为
Data 消息中包含了Ack数目。因此，此协议假定 Data 消息是在 Last Ack 事件之前进行处理的。
5.13 ［10/10/10/10/10/10］<5.4>考虑上面介绍的高级目录式协议和图5-26中的缓存内容。在以下每种
情况下，哪一个过渡状态序列会影响到缓存块的移动？
a. ［10］<5.4>
PO,0: read 100
b.［10］<5.4>
PO,0:read 120
c. ［101<3.4>
PO,0:write 120 <--80
d. ［101<5.4>
P3,1:write 120 <-- 80
c. ［10］<5.4>
P1,0: read 110
f，
110<5.4>
PO,0:write 108 -- 48
5.14
［15/15/15/15/15/15/15］<5.4>考虑上面介绍的高级目录式协议和图5-26中的缓存内容。在以下每
种情况下，哪一个过渡状态序列会影响到缓存块的移动？在所有情况下，假定处理器在同一周
期中发出自己的请求，但目录按照自上而下的顾序对这些请求进行排序。假定控制器的操作表
现为原子操作（例如，目录控制器将执行 DS-->DM转换所需要的全部操作，然后才会处理对同
一块的另一请求）。
a. ［15］<5.4>
PO,0：
read 120
P1,0:read 120
b. 115］<5.4>
PO,0：
read 120
P1,0: write 120 ≤--80
C. ［15］<5.4>
PO,0: write 120
P1,0：
read 120
d. ［15］<5.4>
PO,0:write 120 ≤--80
P1,0: write 120 4-- 90
c. ［15］<5.4>
PO,0: replace 110
P1,0: read 110
f. ［151 <5.4>
P1,0: write 110 <-- 80
PO,0: replace 110
8［15］<5.4>
P1,0: read 110
PO,0: replace 110
5.15 ［20/20/20/20/20］ <5.4>图5-25 中所示多处理器（禁用了L2）实现了表5-13及表5-14所述的协
议，对此多处理器假定有以下延迟。
\begin{itemize}
    \item CPU 读取与写人命中不产生停顿时钟周期。
    \item 要完成一次敏失（例如，执行读取或执行写人），仅当它是为回应Last Ack事件而执行时，
    才会占用Lacd个时钟周期（否则，它会在向缓存中复制数据时完成）。
    \item 生成替换事件的CPU 读取或写人操作会在发射 PutModified 消息之前（比如使用写回缓冲区）
    发射相应的 GetShared或GetModified消息。
    \item 发送请求或确认消息（例如 GetShared）的缓存控制器事件会延迟 Lsezd.mag 个时钟周期。
    \item 读取缓存并发送数据消息的缓存控制器事件会延迟 L send.dat 个时钟周期。
    \item 接收数据消息并更新缓存的缓存控制器事件会延迟Lrevdats 个时钟周期。
    \item 存储器控制器在转发请求消息时会导致Lsmd.mg个时钟周期的延迟。
    \item 存储器控制器对于它必须发送的每个失效消息会另外导致Linv个时钟周期的延迟。
    \item 缓存控制器为它接收到的每个失效消息产生\verb|Lstnd_msg| 个时钟周期的延迟（一直延迟到它发送
    Ack 消息为止）。
    \item 存储器控制器读取存储器和发送数据消息的延迟为Iread memory个时钟周期。
    \item 存储器控制器向存储器写入数据消息的延迟为Lwrie.asmory 个时钟周期（一直延迟到它发送
    Ack 消息为止）。
    \item 非数据消息（例如，请求、失效、Ack）的网络延迟为 \verb|Lrea_msg| 个时钟周期。
    \item 数据消息（例如，请求、失效、Ack）的网络延迟为Lsutn.mg 个时钟周期。
    \item 任何一条从芯片0传送到芯片1的消息都会增加20个时钟周期的延迟（反之亦然）。
    考虑一种实现方式，其具有表5-15汇总的性能指标。
\end{itemize}
针对以下操作序列、图5-26中的缓存内容以及上述目录协议，每个处理器节点观测到的延迟为
多少？
\begin{verbatim}
    8.［201<5.4>
    b.1201<5.4>
    C. ［20］<5.4>
    d. ［20］<5.4>
    e. ［20］<5.4>
    P0,0: read 100
    PO,0:read 128
    PO,0: write 128 <--68
    PO,0: write 120
    ＜-- 50
    PO,0: write 108
    ≤-- 80
    表5-15 目录式一致性协议的延迟
    操
    作
    Send_nsg
    Send_data
    Rcv_data
    Read-memory
    Write-menory
    iny
    ack
    Req_msg
    Data_msg
    延迟
    6
    20
    15
    100
    20
    1
    4
    15
    30
\end{verbatim}
5.16 ［20］<5.4>在出现缓存缺失时，前面介绍的交换监听协议和本案例研究中的目录式协议都会尽快
执行读取或写人操作。具体来说，它们将这一操作看作是向稳定状态转换过程的一部分，而不
是先转换到稳定状态，再简单地重试这一操作。这并非一种优化。而是为了确保转发过程的成
功，各种协议实施方式必须确保它们在放弃一个块之前至少执行了一个CPU操作。假定一致性
协议实施方式没有实现这一点。试解释这种做法可能如何导致活锁（livelock）。给出可以模拟
这一行为特性的简单代码示例。
5.17 ［20/30］<5.4-一些目录式协议向协议中添加了“被拥有”（O）状态，类似于前面讨论的对监听
式协议的优化。在那些可能仅读取“被拥有”块的节点中，“被拥有”状态的表现类似于“共
享”状态，但在某些节点中，必须根据其他节点对“被拥有”块的 Get 请求提供数据，
“被拥
有”状态的表现类似于“已修改”状态。在对一个“已修改” 块发出 GetSbared 请求时，需要
节点向发出请求的处理器和存储器发送数据，而“被拥有”状态消除了这一情景。在 MOSI 目
录式协议中，在对处于“已修改”或“被拥有”状态的块发出 Getshared 请求时，会向发出请
求的节点提供数据，并转换为“被拥有”状态。在处理“被拥有” 状态的 GetModified 请求时，
类似于对“已修改”请求的处理。这种经过优化的 MOSI 协议仅在节点替换处于“已修改”或
“被拥有”状态的块时才会更新存储器。
a.［20］<5.4>试解释为什么说此协议中的MSA状态本质上就是一个“过渡”的“被拥有”状态。
b.［30］<5.4>请修改缓存与目录协议表，以支持一种稳定的“被拥有”状态。
5.18
［25/25］<5.4-前面所述的高级目录式协议依靠一种点对点有序互连来确保操作正确。假定有图
5-38的初始缓存内容和以下操作序列，试解释，当此互连不能维持点对点排序时，可能会发生
什么问题。假定处理器同时执行这些请求，而目录则是按所示顺序进行处理。
［424］
425
［426
316
第5章 线程级并行
a. ［251<5.4>
b.［25］<5.4>
P1,0: read 110
P3,1:write 110 ≤-- 90
PI,0:read 110
PO,0: replace 110
练习
5.19 ［15］ <5.1>假定有一个关于应用程序的函数，其形式为F（ip），表示在总共提供p个处理器的情
况下，恰好有：个处理器可供使用的时间比例。即：
立FP=1
假定在使用：个处理器时，应用程序的运行速度加快i倍。请改写 Amdahl 定律，将某一应用程
序的加速比表示为p的函数。
5.20 ［15/20/10］ <5.1>在这个练习中，我们研究互连网络拓扑对程序的每条指令时钟周期数（CPI）的
影响（这些程序运行在包含64 个处理器的分布式存储器多处理器上）。处理器的时钟频率为
3.3 GHz，应用程序的所有引用都在缓存中命中，其基础 CPI 为0.5。假定有0.2%的指令涉及远
程通信引用。远程通信引用的成本为（100+10h）ns，其中h是指一次远程引用为到达远程处理
器存储器并返回时必须在通信网络进行跳转的次数。假定所有通信网络都是双向的。
a. ［1S］ <5.1>当64个处理器分别排列为一个环、一个8x8处理器网格或者超立方体时，计算在
最糟情況下的远程通信成本。（提示：在一个2”超立方体中，最长的通信路径有n个链接。）
b. ［20］ <5.1>将此应用程序在没有远程通信时的基本CPI 与分别采用（a）部分三种拓扑时获得的
CPI进行对比。
c［10］<5.1>与分别采用（a）部分三种拓扑进行远程通信的性能相比，没有远程通信的应用程序可
以加快多少倍？
5.21 ［15］ <5.2>说明可以如何针对直写缓存来修改图5-4所示的基本监听协议。与写回缓存相比，在
采用直写缓存时不需要哪一项主要硬件功能？
5.22 ［20］ <5.2>向基本监听缓存一致性协议（见图5-4）中添加一个洁净的独占状态。以图5-4的形
式给出这一协议。
5.23
［I5］<5.2>关于伪共享问题，有人提出一种解决方案：为每个字添加一个有效位。添加之后，不
需要删除𤦂个块就能使一个字失效，这样处理器可以在其缓存中保存块的某一部分，而另一个
处理器可以写人这个块的不同部分。如果包含这一功能，需要向基本监听式缓存一致性协议（见
图5-4）中增加什么样的复杂性？注意考虑所有可能出现的协议操作。
5.24 ［15/20］＜5.3>本练习研究的是在共享存储器多处理器系统设计中应用一些非常积极的技术时，
对处理器中开发指令级并行的影响。考虑两种除处理器之外完全相同的系统。系统 A使用的处
理器采用简单的单发射循序流水线，而系统B使用的处理器具有4路发射、乱序执行以及拥有
64项的重新排序缓冲区。
2.［1S］ <5.3>按照图5-6中的约定，我们将执行时间划分为指令执行、缓存访问、存储器访问和
其他停顿。预测系统A与系统B中这些组件有什么何种不同？
b.［10］<5.3>基于5.3 节对联机事务处理（OLTP）工作负载行为特性的讨论，OLTP 工作负载与
其他基准之间的哪种重要区别限制了它从更积极处理器设计中获得的好处？
5.25 ［15］<5.3>如何改变应用程序的代码，以避免伪共享？编译器做些什么？哪里需要有程序员的
指示？
5.26 ［15］<5.4>假定有一个目录式缓存一致性协议。目录中当前拥有的信息表明处理器P1 拥有“独
占”模式的数据。如果这个目录现在收到处理器P1对同一缓存块的请求，这可能意味着什么？
目录控制器应当怎么做？（此类情况被称为竞赛情景，正是难以设计和验证一致性协议的原因
所在。）
案例研究与练习（Amr Zaky 和David A.Wood设计）
317
5.27［20］<5.4>目录控制器可以为那些已经被本地缓存控制器替换的行发送失效消息。为了避免此类
消息，并保持目录的一致性，人们采用了替换提示。此类消息告诉控制器：某个块已经被替换。
修改5.4节的目录式一致性协议，以利用此类替换提示。
5.28
［20/30］<5.4>利用全部填充的位向量来直接实现目录时，一个不利因素是目录信息的总大小会
以乘积规模扩大（即：处理器数×存储器块数）。如果存储器随处理器数目线性增大，那目录
的总大小就会以处理器数目的4次方增长。在实践中，由于目录只需要为每个存储器块保存一
个位（一个存储器块通常为32~128个字节），所以当处理器数目处于中小规模时，这一问题
并不严重。例如，假定有一个大小为128字节的块，与主存储器相比，目录存储的大小为处理
器数目/1024，或者说当有100个处理器时，大约增加10%的存储量。如果发现需要保存的信息
量与每个处理器的缓存大小成比例，那就可以避免上述问题。我们将在这些练习中探讨一些解
决方案。
2.［20］<5.4＞—种可以获得可扩展目录协议的方法是将多处理器组织为逻辑层次结构，以处理器
作为这个层次结构的叶子，目录位于每个子树的根部。每个子树的目录记录哪些后代缓存了
哪些存储器块，以及哪些以该子树中的节点为主节点的存储器块被缓存在该子树的外部。俊
定每个目录都是完全相联的，计算记录这些目录的处理器信息所需要的存储数量。答案中还
应当包括该层次结构每一级中的节点数目以及节点的总数。
b.［30］<5.4>实现目录机制的一种替代方法是实现一些非密集位向量。共有两种策略；一种策略
是减少所需要的位向量数目，另一种策略是减少每个向量的位数。可以通过跟踪来对比这些
机制。首先，将这个目录实现为一个4路组相联缓存，其中存储全部填满的位向量，但仅针
对那些被缓存在主节点外部的块。如果发生了目录缓存缺失，选择一个目录项，并使该项失
效。第二，将目录实现为每个项目包含8个位。如果某个块仅被缓存在其主节点之外的唯一
节点中，那这个宇段将包含这个节点的编号。如果这个块被缓存在主节点之外的多个节点中，
那这个字段就是一个位问量，每一位指示一组8个处理器，其中至少有一个处理器缓存了这
个块。利用64 处理器执行过程的跟踪功能，模拟这些机制的行为。假定非共享引用的缓存
非常完美，从而只需要关注一致性行为特性。确定当目录缓存大小增大时的外来失效数目。
5.29
［10］ <5.5>使用链接载入/条件存储指令对实现经典的“测试并置位”指令。
5.30
［15］<5.5>一种常用的性能优化方法是在填充同步变量时，将同步变量所在缓存行中的所有其他
有用数据全部排除在外。构造一个可能造成性能损失的反例。假定采用监听写人失效协议。
5.31
［30］<5.5>为多核处理器实现链接载入/条件存储对的一种可能方式是将对这些指令设置约東条
件，使其使用末缓存的存储器操作。监听单元负责解读所有核心对该存储器的所有读取与写人
操作。它跟踪链接载入指令的来源，以及在链接载入及其相应的条件存储指令之间是否发生了
任何中间存储操作。监听单元可以防止任何发生失败的条件存储操作写入任何数据，并可以使
用互连信号通知处理器：此次存储失败。请为支持四核心对称多处理器（SMP）的存储系统
设计这样一个监听器。考虑以下因素：读取请求和写人请求的数据大小通常是不同的（4、8、
16、32字节）。任何存储器位置都可能是链接载入/条件存储对的目标，存储器监听器应当假定：
对任意位置进行的链接载入/条件存储引用都可能与同一位置的常规访问交错在一起。监听器的
复杂性应当与存储器大小无关。
5.32 ［10/12/10/12］<5.6>如5.6 节中的讨论内容，存储器一致性模型提供了一个技术规格，说明存储
器系统应当如何展现给程序员。考虑以下代码段，其中，初始值为：
A=flag=C=0.
P1
P2
A= 2000
while （flag ==1）｛：｝
f1ag=1
C=A
a.［10］<5.6>在代码段结束时，预测C中的取值。
427
428
318
429
第5章线程级并行
b.［12］<5.6>系统采用通用互连网络、目录式缓存一致性协议，支持非阻塞式载人，结果C等于
0。描述一种可能出现这一结果的情景。
c.［10］<5.6>如果希望使该系统保持顺序一致性，需要设置哪些关键的约束条件？
假定处理器支持宽松存储器一致性模型。宽松一致性模型需要明确确认同步。假定此处理器
支持“屏障”指令，能够确保此承障指令之前的所有存储器操作都完成之后，才允许开始屏
障之后的存储器操作。为了确保能够得到顺序一致性的“直观结果”，应当在以上代码段的
什么位置包含屏障措令？
5.33
［25］<5.7>—个两级缓存层次结构，L1更接近处理器。请证明，如果L.2的相联度至少与L.1相
同，两种缓存都使用行可替换单元（LRU）进行替换，两个缓存的块大小相同。
5.34［讨论］<5.7>在尝试对多处理器系统进行详细的性能评估时，系统设计人员使用以下三种工具之
一：分析模型、由跟踪驱动的模拟和由执行驱动的模拟。分析模型使用数学表达式对程序的行
为进行建模。由跟踪驱动的模拟在实际计算机上运行应用程序，并生成服踪轨迹，通常是存储
器操作的跟踪轨迹。这些跟踪轨迹可以通过缓存模拟器进行回放，也可以用具有简单处理器模
型的模拟器进行回放，在在参数发生变化时预测系统的性能。由执行驱动的模拟器模拟整个执行
过程，为处理器状态等保持等价结构。这些方法之间的准确性与速度均衡如何？
5.35
［40］ <5.7、5.9>在增加处理器的数目时，多处理器和集群的性能通常也会提高，理想情况下应
当是n个处理器提高n倍。这一有偏基准测试的目标是让程序在增加处理器时的性能恶化。例
如，这就意味着当多处理器或集群仅有一个处理器时，程序的运行速度最快，有2个处理器时，
速度较慢，4个处理器时要比两个处理器还慢，以此类推。在每种组织结构中，是哪些关键的
性能特性导致逆线性加速比的？