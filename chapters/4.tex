

\chapter{向量、SIMD和 GPU体系结构中的数据级并行}
我们将这些算法称沟数据并行算法，是因为它们的并行源于
对大型数据集的同时操作，而不是来自多个控制线程。
—W.Daniel Hillis 和 Guy L. Steele，
"Data Parallel Algorithms”， Comm. ACM （1986）
如果要耕种一块地，那你会选择两头强壮的公牛呢，还是1024
只小鸡？
—Seymour Cray，超级计算机之父
（争论的是选择两个功能强大的向量处理器，还是选择许多简
单的处理器。）

\section{引言}
关于单指令多数据（SMID）体系结构（在第1章介绍），人们总是会问一个问题：有多少
应用程序拥有大量的数据级并行（DLP）。50年后，这个问题的答案不只包含科学运算中的矩阵
计算，还包括媒体图像和声音处理。此外，由于单条指令可以启动许多数据运算，所以 SIMD
在能耗效率方面可能要比多指令多数据（MIMD）更高效一些，MIMD 每进行一次数据运算都
需要提取和执行一条指令。这两个答案使SIMID 对于个人移动设备极具吸引力。最后，SMID
与 MIMD 相比的最大优势可能就是：由于数据操作是并行的，所以程序员可以采用顺序思维方
式但却能获得并行加速比。

本章介绍SIMD的3种变体：向量体系结构、多媒体 SIMD指令集扩展和图形处理单元
（GPU）。①

第一种变体的出现要比其他两个早30年以上，它实际上就是以流水线形式来执行许多数据
操作。与其他SIMID变体相比，这些向量体系结构更容易理解和编译，但过去一直认为它们对
于微处理器来说太过昂贵了，这一看法直到最近才有所改变。这种体系结构的成本，一部分用
在晶体管上，另一部分用于提供足够的DRAM带宽，因为它广泛依赖于缓存来满足传统微处理
器的存储器性能要求。

第二种 SIMD变体借用SIMID 名称来表示基本同时进行的并行数据操作，在今天支持多媒
体应用程序的大多数指令集体系结构中都可以找到这种变体。x86体系结构的 SIMD指令扩展
是在1996年以MIMX（多媒体扩展）开始的，在接下来的10年间出现了几个 SSE（流式 SIMD
扩展）版本，一直发展到今天的AVX（高级向量扩展）。为了使x86计算机达到最高计算速度，
通常需要使用这些 SIMD指令，特别是对于浮点序。

SIMID 的第三种变体来自 GPU社区，它的潜在性能要高于当今传统多核计算机的性能。尽
管GPU的一些特征与向量体系结构相同，但它们有自己的一些独特特征，部分原因在于它们的
发展生态系统。在GPU 的发展环境中，除了 GPU及其图形存储器之外，还有系统处理器和系
统存储器。事实上，为了辨识这些差别，GPU社区将这种体系结构称为异类。

对于拥有大量数据并行的问题，所有这三种 SIMD变体都有一个共同的好处：与经典的并
行 MIMD编程相比，程序员的工作更轻松一些。为了对比SIMID 与 MIIMID的重要性，图4-1绘
制了 x86计算机中 MIIMD 的核心数与 SIMD 模式中每个时钟周期的32位及 64位运算数随时间
的变化曲线。

对于x86计算机，我们预期每个芯片上每两年增加两个核心，SIMD的宽度每四年翻一番。
给定这些假设，在接下来的10年里，由SIMID并行获得的潜在加速比为MIMID 并行的两倍。
因此，尽管 MIMD 并行最近受到的关注要多得多，但理解 SIMD并行至少与理解 MIMID并行一
样重要。对于同时具有数据级并行和线程级并行的应用程序，2020年的潜在加速比将比今天的
加速比高一个数量级。

① 本章的基础材料包括：本书第4版的附录下 “向量处理器”（Krste Asanovic 编写）和附录 G "VLIW和 EPIC的
硬件与软件”；Computer Organication and Design第4版的附录 A “图形与计算 GPU"（John Nickolls 和 David Kirk
编写）；还参考了 Joe Gebis 和 David Patterson 于2007年4月在JEEB Computer 杂志上发表的文章 “Embracing and
Extending 20th-Century Instruction Set Architectures"。

4. 回里件尔给們
1000
-米~- MIMID*SIMD（32b）
X~ MIMD*SIMD（64b）
SIMD（32b）
SIMD（64b）
中 MIMD
100
潜在并行加速比
10
2003
2007
2011
2015
2019
2023
图 4-1

x86 计算机通过 MIMD、SIMD、MIMD 与SIMD组合中的并行而在不同时间获得的潜在加速比。
这一曲线假定 MIIMD的每个芯片上每两年增加两个核心，SIMID的运算数每四年翻番
本章的目的是让架构师理解向量为什么比多媒体SIMDD更具一般性，以及向量与 GPU体系
结构之间的相似与区别。由于向量体系结构是多媒体 SIMID指令的超集（包括一个更好的编译
模型），而且由于GPU与向量体系结构有一些相似性，所以我们首先从向量体系结构人手，为
以下两节奠定基础。下一节介绍向量体系结构，而附录G对这一主题的讨论要深入得多。
\section{向量体系结构}
执行可向量化应用程序的最高效方法就是向量处理器。
—Jim Smith
计算机体系结构国际研讨会（1994年）

向量体系结构获得在存储器中散布的数据元素集，将它们放在一些大型的顺序寄存器堆中，
对这些寄存器堆中的数据进行操作，然后将结果放回存储器中。一条指令对数据向量执行操作，
从而会对独立数据元素进行数十个“寄存器-寄存器” 操作。

这些大型寄存器堆相当于由编译器控制的缓冲区，一方面用于隐藏存储器延迟，另一方
面用于充分利用存储器带宽。由于向量载人和存储是尝试流水化的，所以这个程序仅在每个
向量载入或存储操作中付出较长的存储器延迟时间，而不需要在载人或存储每个元素时耗费
这一时间，从而将这一延迟时间分散在比如64个元素上。事实上，向量程序会尽力使存储器
保持繁忙状态。

\subsection{VMIPS}

我们首先看一个向量处理器，它由图4-2所示的主要组件组成。这个处理器大体以Cray-1
为基础，它是本节讨论的基础。我们将这种指令集体系结构称为 VMIPS；它的标量部分为 MIIPS，
它的向量部分是MIPS的逻辑向量扩展。这一小节的其他部分研究 VMIPS 的基本体系结构与其
他处理器有什么关系。

VMIPS指令集体系结构的主要组件如下所示。

\begin{itemize}
    \item 向量寄存器—每个向量寄存器都是一个固定长度的寄存器组，保存一个向量。VMIPS
    有8个向量寄存器，每个向量寄存器保留64个元素，每个元素的宽度为64位。向量寄
    存器堆需要提供足够的端口，向所有向量功能单元馈送数据。这些端口允许将向量操作
    高度重叠，发送到不同向量寄存器。利用一对交叉交换器将读写端口（至少共有16个读
    取端口和8个写入端口）连接到功能单元的输入或输出。
    
    \item 向量功能单元—每个单元都完全实现流水化，它可以在每个时钟周期开始一个新的操作。
    需要有个控制单元来检测冒险，既包括功能单元的结构性冒险，又包括关于寄存器访问的
    数据冒险。图42显示 VMIPS 有5个功能单元。为简单起见，我们仅关注浮点功能单元。
\end{itemize}

王存储恐
向量载人/
向量
寄存器
浮点加/减
深点乘
浮点除
整教运
逻辑逶笰
标量
寄存器
图 4-2

向量体系结构 VMIPS的基本结构。这一处理器拥有类似于 MIIS 的标量体系结构。它还有8个
64元素向量寄存器，所有功能单元都是向量功能单元。这一章为算术和存储器访问定义了特殊的
向量指令。图中显示了用于逻辑运算与整数运算的向量单元，所以 VMIPS 看起来像是一种通常
包含此类单元的标准向量处理器；但是，我们不会讨论这些单元。这些向量与标量寄存器有大量
读写端口，允许同时进行多个向量运算。一组交叉交换器（粗灰线）将这些端口连接到向量功能
单元的输人和输出

\begin{itemize}
    \item 向量载入/存储单元——这个向量存储器单元从存储器中载人向量或者将向量存储到存
    储器中。VMIPS 向量载入与存储操作是完全流水化的，所以在初始延迟之后，可以在向
    量寄存器与存储器之间以每个时钟周期一个字的带宽移动字。这个单元通常还会处理标
    量载入和存储。
    
    \item 标量寄存器集合—标量寄存器还可以提供数据，作为量功能单元的输人，还可以计
    算传送给向量载人/存储单元的地址。它们通常是MIPS的32个通用寄存器和32个浮点
    寄存器。在从标量寄存器堆读取标量值时，向量功能单元的一个输入会闩锁住这些值。
\end{itemize}

表4-1列出了 VMIPS 向量指令。在 VMIPS 中，向量运算使用的名字与标量 MIIPS 指令的名
字相同，但后面追加了字母“VV”。因此，ADDVV.D 就是两个双精度向量的加法。向量指令的输
人或者为一对向量寄存器（ADDVV.D），或者为一个向量寄存器和一个标量寄存器，通过附加“VS”
来标识（ADDVS.D）。在后—种情况下，所有操作使用标量寄存器的相同值来作为一个输人：运
算 ADDVS.D将向量寄存器中的每个元素都加上标量寄存器的内容。向量功能单元在发射时获得标
量值的一个副本。大多数向量运算有一个向量目标寄存器，尽管其中一些（比如人口计数）会
产生标量值，这个值将存储在标量寄存器中。

表4-1 VMIPS向量指令，仅给出双精度浮点操作
指令
ADDVV.D
ADDVS.D
SUBVV.D
SUBVS.D
SUBSV.D
MULVV.D
MULVS.D
DIVVV.D
DIVVS.D
DIVSV.D
LV
SV
LVWS
SVWS
LVI
SVI
CVI
S--V.D
S--VS.D
操作数
功能
V1，¥2，¥3
将V2与V3中的元素相加，然后将结果放在V1中
V1.V2,F0
将FO加到V2中的每个元素，然后将结果放在V1中
V1,V2.V3
从V2中减去V3的元素，然后将结果放在V1中
V1.V2.FO
从V2的元素中减去FO，然后将结果放在V1中
V1.F0，¥2
从FO中减去V2的元素，然后将结果放在V1中
V1.¥2.V3
将V2和V3中的元素相乘，然后将结果放在V1中
V1,V2,F0
将V2中的每个元素乘以FO，然后将结果放在V1中
V1.V2.V3
将V2中的元素除以V3，然后将结果放在V1中
V1.V2,FO
将V2中的元素除以FO，然后将结果放在V1中
V1.F0.¥2
将FO除以V2的元素，然后将结果放在V1中
V1.R1
将从地址R1开始的存储器内容载入向量寄存器V1中
R1.V1
将向量寄存器V1的内容存储到从地址R1开始的存储嚣中
V1.（R1,R2）
从地址R1开始载人VI，步蝠为R2（即，R1 +1 × R2）
（R1,R2），V1
将V存储到地址RI处，步幅为R2（即，RL + 1 ×R2）
V1.（R1+V2）
将向量内容载人V1，向量的元素位于R1+V2（1）（即，V2 索引）
（R1+V2）.V1
将V存储到向量中，它的元素位于RI+ V2（1）（即，V2 索引）
V1,RI
创建索引向量，将值0.1 × R1,2 × RL....63 × RI存储到V2中
V1,V2
对比V1和V2中的元素（EQ、NE、GT、LT、GE、LE）。如果条件为真，则将在相应
V1.FO
的位向量中入1，否则放人0。将所得到的位向量放在向量遮罩寄存器中（W）。
指令S--VS.D执行同样的对比操作，但它的一个操作数为标量值
POP
CVM
MTC1
MFC1
MVIM
MVFM
RI.VM
计算向量遮罩寄存器VM.1的个数，并将计数值存储到R1中
将向量遮罩寄存器设为全1
VLR,RI
将R1的内容移到向量长度寄存器VL中
R1,VLR
将向量长度VL的内容移到R1中
VM, FO
将F0的内容移动到向量遮罩寄存器VM中
FO, VM
将向量遮罩寄存器VM的内容移动到F0中
*除了向量寄存器外，还有两个特珠寄存器 VLR和VM，下面将进行讨论。这些特殊寄存器假定存在于 MIPS协处理器！
空间中，与FPU 寄存器位于一起。后面将解释带有步幅的运算以及索引创建及索引载入/存储操作的应用。

名字LV和SV 表示向量载人和向量存储，它们载人或存储整个双精度数据向量。一个操
作数是要载人或存储的向量寄存器，另一个操作数是MIIPS 通用寄存器，它是该向量在存储
器中的起始地址。后面将会看到，除了这些向量寄存器之外，我们还需要两个通用寄存器：
向量长度寄存器和向量遮罩寄存器。当向量长度不是64时使用前者，当循环中涉及 IF语句
时使用后者。

功率瓶颈使架构师非常看重具有以下特点的体系结构：一方面能够提供高性能，另一方面
又不需要高度乱序超标量处理器的能耗与设计复杂度。向量指令天生就与这一趋势吻合，架构
师可以用它们来提高简单循序标量处理器的性能，而又不会显著增大能耗要求和设计复杂度。
在实践中，开发人员可以采用向量指令的方式来表达许多程序，采用数据级并行可以很高效地
在复杂乱序设计中运行，Kozyrakis 和 Pattrson［2002］证明了这一点。

采用向量指令，系统可以采用许多方式对向量数据元素进行运算，其中包括对许多元素同
时进行操作。因为有了这种灵活性，向量设计可以采用慢而宽的执行单元，以较低功率获得高
性能。此外，向量指令集中各个元素是相互独立的，这样不需要进行成本高昂的相关性检查就
能调整功能单元，而超标量处理器是需要进行这一检查的。

向量本身就可以容纳不同大小的数据。因此，如果一个向量寄存器可以容纳64个64位元
素，那同样可以容纳128个32位元素、256个16位元素，甚至512个8位元素。向量体系结构
之所以既能用于多媒体应用，又能用于科学应用，就是因为具备这种硬件多样性。

\subsection{向量处理器如何工作：一个示例}

通过查看 VMIPS的向量循环，可以更好地理解向量处理器。让我们来看一个典型的向量问
题，在本节将一直使用这个例子：

Y-axX+Y
X和Y是向量，最初保存在存储器中，a是一个标量。这个问题就是所谓的SAXPY或 DAXPY循
环，它们构成了Linpack 基准测试的内层循环。SAXPY 表示“单精度丑×X加 Y”（single-precision
ax区plus Y）； DAXPY 表示“双精度a× X加Y"（double precision a×plus Y）。Linpack 是一组
线性代数例程，Linpack 基准测试包括执行高斯消去法的例程。

现在假定向量寄存器的元素数或者说其长度为64，与我们关心的向量运算长度匹配。（稍
后将取消这一限制。）

例题
解答
给出 DAXPY循环的 MIPS 和 VMIPS代码。假定X 和Y的起始地址分别为 Rx和 RY。
MIPS代码如下。
\begin{verbatim}
    L.D
    DADDIU
    Loop：
    L.D
    MUL.O
    L.D
    ADD.D
    S.D
    DADDIU
    DADDIU
    DSUBU
    BNEZ
    FO,a
    R4,Rx，#512
    F2,0（Rx）
    F2,F2,FO
    F4,0（Ry）
    F4,F4,F2
    F4,9（Ry）
    RXx,RX，#8
    Ry,RY，#8
    R20,R4,Rx
    R20,Loop
    ；載入标量a
    ；要載入的最后地址
    ；载入X［1］
    ia×XCi］
    ；载入Y［i］
    aXXCI+ YE
    ；存储到 Y［i］
    ；速增X的索
    递增Y的索
    ；计算范图
    ；检壹是否宪成
    4. 何里心结們
    下面是 DAXPY 的 VMIPS代码：
    L.D
    LV
    MULVS.D
    LV
    ADDVV.D
    FD,a
    ；載入标量a
    VI,RX
    ：載入向量X
    ¥2,V1,FO
    ；向量一标量乘
    V3,Ry
    ；載入向量Y
    ¥4，¥2,V3
    ；相加
    V4,Ry
    ；存储结果
\end{verbatim}
最引人注目的差别在于向量处理器大幅缩减了动态指令带宽，仅执行6条指令，而MIPS
几乎要执行600条。这一缩减是因为向量运算是对64个元素执行的，在MIPS 中差不多占据一
半循环的开销指令在VMIPS代码中是不存在的。当编译器为这样一个序列生成向量指令时，所
得到的代码会将大多数时间花费在向量运行模式中，我们将这种代码称为已向量化或可向量化。
如果循环的迭代之间没有相关性（这种相关被称为循环间相关，见4.5节），那么这些循环就可
以向量化。

MIPS 与 VMIPS 之间的另一个重要区别是流水线互锁的频率。在简单的MIPS代码中，每
个 ADD.D都必须等待MUL.D，每个S.D都必须等待ADD.D。在向量处理器中，每个向量指令只会因
为等待每个向量的第一个元素而停顿，然后后续元素会沿着流水线顺畅流动。因此，每条向量
指令仅需要一次流水线停顿，而不是每个向量元素需要一次。向量架构师将元素相关操作的转
发称为链接（chaining），因为这些相关操作是被“链接”在一起的。在这个例子中，MIIPS 中
的流水线停顿频率大约比 VMIPS 高64倍。软件流水线或循环展开（参见附录H）可以減少MIPS
中的流水线停顿，但很难大幅缩减指令带宽方面的巨大差别。

\subsection{向量执行时间}

向量运算序列的执行时间主要取决于3个因素：（1）操作数向量的长度；（2）操作之间的结构
冒险；（3）数据相关。给定向量长度和初始速率（初始速率就是向量单元接受新操作数并生成新
结果的速率），我们可以计算一条向量指令的执行时间。所有现代向量计算机都有具备多条并行
流水线（或车道）的向量功能单元，它们在每个时钟周期可以生成两个或更多个结果，但这些
计算机还可能拥有一些未完全流水化的功能单元。为简便起见，我们的 VMIIPS 实现方式有一条
车道，各个操作的初始速率为每个时钟周期一个元素。因此，一条向量指令的执行时间（以时
钟周期为单位）大约就是向量长度。

为了简化对向量执行和向量性能的讨论，我们使用了一种护航指令组（convoy）的概念，
它是一组可以一直执行的向量指令。稍后可以看到，我们可以通过计算护航指令组的数目来估
计一段代码的性能。护航指令组中的指令不能包含任何结构性冒险，如果存在这种冒险，则需
要在不同护航指令组中序列化和启动这些指令。为了保持分析过程的简单性，假定在开始执行
任意其他指令（标量或向量）之前，护航指令都必须已经完成。

除了具有结构性冒险的向量指令序列之外，具有写后读相关冒险的序列也应该位于不同护
航指令组中，但通过链接操作可以允许它们位于同一护航指令组中。

链接操作允许向量操作在其向量源操作数的各个元素变为可用状态之后立即启动：链中第
一个功能单元的结果被“转发”给第二个功能单元。在实践中经常采用以下方式来实现链接：
允许处理器同时读、写一个特定的向量寄存器，不过读写的是不同元素。早期的链接实现类似
于标量流水线中的转发，但这限制了链中源指令与目标指令的定时。最近的链接实现采用灵活
链接，这种方式允许向量指令链接到几乎任意其他活动向量指令，只要不生成结构性冒险就行。
所有现代向量体系结构都支持灵活链接，这也是本章的假设之一。

为了将护航指令组转换为执行时间，我们需要有一种定时度量，用来估计护航指令组的时间。
这种度量被称为钟鸣（chime），就是执行护航指令组所花费的时间单位。执行由m个护航指令组
构成的向量序列需要m次钟鸣。当向量长度为n时，对于VMIPS来说，大约为mxn个时钟周期。
钟鸣近似值忽略处理器特有的一些开销，许多此类开销都依赖于向量长度。因此，以钟鸣为单
位测量时间时，对于长向量的近似要优于对短向量的近似。我们将使用钟鸣测量结果（而不是
每个结果的时钟周期），用来明确表示忽略了特定的开销。

如果知道向量序列中的护航指令组数，那就知道了用钟鸣表示的执行时间。在以钟鸣为单
位测试执行时间时，所忽略的一个开销源是对单个时钟周期内启动多条向量指令的限制。如果
在一个时钟周期内只能启动一条向量指令（大多数向量处理器都是如此），那钟鸣数会低估护舫
指令组的实际执行时间。由于向量的长度通常远大于护航指令组中的指令数，所以简单地假定
这个护航指令组是在一次钟鸣中执行的。

例题
给出以下代码序列在护航指令组中是如何排列的，假定每个向量功能单元只有一
个副本：
解答
V1, Rx
；载入向量X
MULVS.D
V2，¥1,F0
；向量-标量乘
LY
V3,Ry
；載入向量Y
ADDVV.D
V4，¥2，¥3
；两个向量相加
SV
VA,Ry
；存储所得之和
这个问量序列将花费多少次钟鸣？每个FLOP（浮点运算）需要多少个时钟周期
（忽略向量指令发射开销）？

第一个护航指令组从第一个 LV 指令处开始。MULVS.D依赖于第一个 LV，但链接操
作允许它位于同一护航指令组中。

第二个LV指令必须放在另一个护航指令组中，因为它与上一个LV指令的载人/存
储单元存在结构性冒险。ADDVV.D与第二个LV 相关，但它也可以通过链接操作位于
同一护航指令组中。最后，SV 与第二个护航指令组中的LV存在结构冒险，所以必
须把它放在第三护航指令组中。通过这一分析，将得出向量指令在护航指令组的
如下排列：
1. LV
2. LV
MULYS.D
ADDVV.D
3. SV
这个序列需要3个护航指令组。由于这一序列需要3次钟鸣，而且每个结果有2
个浮点运算，所以每个 FLOP 的时钟周期数目为1.5（忽略任何向量指令发射开
销）。注意，尽管我们允许LV 和 MULVS.D都在第一护航指令组中执行，但大多数问
量计算机将需要两个时钟周期来启动这些指令。

这个例子表明，钟鸣近似值对于长向量是相当准确的。例如，对于包括64个元素
的向量来说，用钟鸣表示的时间为 3，所以这个序列将需要大约64×3=192个时
钟周期。在两个分离时钟周期中发射护航指令组的开销很小。

另一个开销源要比发射限制明显得多。钟鸣模型中忽略的最重要开销源就是向量启动时间。
启动时间主要由向量功能单元的流水线延迟决定。对于 VMIPS，我们使用与Cray-1相同的流水
线深度，不过在更多的现代处理器中，这些延迟有增加的趋势，特别是向量载人操作的延迟。
所有功能单元都被完全流水化。浮点加的流水线深度为6个时钟周期、浮点乘为7个、浮点除
为20个、向量载人为12个。

有了这些向量基础知识之后，接下来的几小节将介绍一些优化方式，或者用来提高性能，
或者增加可以在向量体系结构中完美运行的程序类型。具体来说，它们将回答如下问题。

\begin{itemize}
    \item 向量处理器怎样执行单个向量才能快于每个时钟周期一个元素？每个时钟周期处理多个
    元素可以提高性能。
    \item 向量处理器如何处理那些向量长度与向量寄存器长度（对于VMIPS，此长度为64）不
    相同的程序？由于大多数应用程序向量与体系结构向量长度不匹配，所以需要一种高效
    的解决方案来处理这一常见情景。
    \item 如果要向量化的代码中含有 IF语句，会发生什么？如果可以高效地处理条件语句，就能
    向量化更多的代码。
    \item 向量处理器需要从存储器系统中获得什么？没有充分的存储器带宽，向量执行可能会徒
    劳无益。
    \item 向量处理器如何处理多维矩阵？为使向量体系结构能够很好地工作，必须对这个常见数
    据结构进行向量化。
    \item 向量处理器如何处理稀疏矩阵？这一常见数据结构也必须进行向量化。
    \item 如何为向量计算机进行编程？如果体系结构方面的创新不能与编译器技术相匹配，那可
    能不会被广泛应用。
\end{itemize}

这一节的后续部分将分别介绍向量体系结构的这些优化技术，附录G将更深人地讨论。

\subsection{多条车道：每个时钟周期超过一个元素}

向量指令集的一个重要好处是它允许软件仅使用一条很短的指令就能向硬件传送大量并
行任务。一条向量指令可以包含数十个独立运算，而其编码使用的位数与一条传统的标量指
令相同。向量指令的并行语义允许实现方式在执行这些元素运算时使用：深度流水化的功能
单元（就像我们目前研究过的 VIIPS 实现方式一样）；一组并行功能单元；或者并行与流水
线功能单元的组合方式。图4-3说明如何使用并行流水线来执行一个向量加法指令，从而提
高向量性能。

VMIIPS 指令集有一个特性：所有向量算术指令只允许一个向量寄存器的元素 N与其他向量
寄存器的元素 N进行运算。这一特性极大地简化了一个高度并行向量单元的构造，将其结构设
定为多个并行车道。和高速公路一样，我们可以通过添加更多车道来提高向量单元的峰值吞吐
量。图4-4给出了一种四车道向量单元的结构。这样，从单车道变为四车道之后，將一次钟鸣
的时钟周期数由64个变为16个。由于多车道非常有利，所以应用程序和体系结构都必须支持
长向量；否则，它们的快速执行速度会耗尽指令带宽，这需要ILP技术（见第3章）提供足够
的向量指令。
270
271
272
273
A［B】
A［41
B［8］
B14］
A［9
A［51
E［9］
B［5］
A［61
B［6］
AL71
B［71
十
十
C［0
C［O］
C［1
C［2］
C［31
元素组
（a）
（b）
图 4-3 使用多个功能单元提高单个向量加法指令C=A+B 的性能。左边的向量处理器（a）有一条加法流水
线，每个时钟周期可以完成一次加法。右边的向量处理器（b）有4条加法流水线，每个时钟周期可
以完成4次加法。一条向量加法指令中的元案交错存在于4条流水线中。通过这些流水线结合在
一起的元素集被称为元素組（element group）（此图经 Asanovic［1998］许可后复制。）
每个车道都包含向量寄存器堆的一部分和来自每个向量功能单元的一个执行流水线。每个
向量功能单元使用多条流水线，以每个时钟周期一个元素组的速度执行向量指令，每个车道一
条流水线。第一个车道保存所有向量寄存器的第一个元素（元素0），所以任何向量指令的第一
个元素都会将其源操作数与目标操作数放在第一车道中。这种分配方式使该车道本地的算术流
水线无须与其他车道通信就能完成运算。主存储器的访问也只需要车道内的连接。通过避免车
道间的通信减少了构建高并行执行单元所需要的连接成本与寄存器堆端口，有助于解释向量计
算机为什么能够在每个时钟周期内完成高达64个运算（跨越16个车道的2个算术单元和2个
载人/存储单元）。

增加多个车道是一种提高向量性能的常见技术，它不需要增加太多控制复杂性，也不需要
对现有机器代码进行修改。它还允许设计人员在晶片面积、时钟频率、电压和能耗之间进行权
衡，而且不需要牺牲峰值性能。如果向量处理器的时钟频率减半，只需要使车道数目加倍就能
保持原性能。
\begin{verbatim}
    D L
    车道0
    车道1
    车道2
    车道 3
    浮点加
    法流水
    线0
    浮点加
    法流水
    线1
    浮点加
    法流水
    线2
    浮点加
    法流水
    线3
    向量寄存器：
    元素0,4，&，⋯
    向量寄存器：
    元素1,5,9，
    向量寄存器：
    元素2,6,10
    向量寄存器：
    元素3，7,11
    浮点乘法
    流水线0
    浮点乘法
    流水线1
    浮点乘法
    流水线2
    浮点乘法
    流水线3
    向量载入-存储单元
\end{verbatim}
图 4-4
包4个车道的向量单元的结构。向量寄存器存储分散在各个车道中，每个车道保存每个向量寄
存器每4个元素中的1个。此图显示了三个向量功能单元：一个浮点加法、一个浮点乘法和一个
载人-存储单元。向量算术单元各包含4条执行流水线，每个车道1条，它们共同完成一条向量指
令。注意，向量寄存器堆的每一部分只需要为其车道本地的流水线提供足够的端口即可。本图役
有给出为向量-标量指令提供标量操作数的路径，而是由标量处理器（或控制处理器）向所有车道
广播标量值

\subsection{向量长度寄存器：处理不等于 64的循环}

向量寄存器处理器有一个自然向量长度，这一长度由每个向量寄存器中的元素数目决定。
对于 VMIPS来说，这一长度为64，它不大可能与程序中的实际向量长度相匹配。此外，在实
际程序中，特定向量运算的长度在编译时通常是未知的。事实上，一段代码可能需要不同的向
量长度。例如，考虑以下代码：

for （i=0; i en；
i=i+1）
Y［i］ -a *XEi］ +YIil：

所有这些向量运算的大小都取决于n，而它的取值不可能在运行之前获知。n的值还可能是某个
过程（该过程中包含上述循环）的参数，从而会在执行时发生变化。

对这些问题的解决方案就是创建一个向量长度寄存（VLR）。VLR 控制所有向量运算的
长度，包括向量载入与存储运算。但VLR 中的值不能大于向量寄存器的长度。只要实际长度小
于或等于最大向量长度（MVL），就能解决上述问题。MVL.确定了体系结构的一个向量中的数
据元素数目。这个参数意味着向量寄存器的长度可以随着计算机的发展而增大，不需要改变指
令集；在下一节将会看到，多媒体 SIMID扩展没有与 MVL对等的参数，所以在每次增大向量
长度时都会改变指令集。

如果n的值在编译时未知，从而可能大于 MVL，那该怎么办呢？为了解决向量长于最大长
度的第二问题，可以使用一种名为条带挖掘（strip mining）的技术。条带挖掘是指生成一些代
码，使每个向量运算都是针对小于或等于MVL.的大小来完成的。我们创建两个循环，一个循
环处理选代数为 MVL.倍数的情况，另一个循环处理所有其他迭代及小于MVL 的情况。在实践
中，编译器通常会生成一个条带挖掘循环，为其设定一个参数，通过改变长度来处理这两种情
况。我们以C语言给出 DAXPY循环的条带挖掘版本：

\begin{verbatim}
    VL-（n % MVL）：/*使用求模运算%找出不规则大小郵分*/
    for （j= 0; j &= （m/MVL）；
    j=j+1）｛/*外层循环*/
    for （i = 1ow; 1 <. （1ow+VL）：f=i+1）/*执行长度VL */
    YEi］=a X［i+Y［i］； *主运算*/
    1ow = 1ow + VL;/*开始下一个向量*/
    VL = MVL；/*将长度复位为最大向量长度*/
    ｝
\end{verbatim}
n/MYL项表示截短整数除法。这一循环的效果是将向量分段，然后由内层循环进行处理。第一
段的长度（n %MVL），所有后续段的长度为MVL。图4-5说明如何将这个长向量分到各个段中。
j的值
\begin{verbatim}
    0
    1
    2
    3
    N/MVL
    ［275］
    1的范围
    0
    m
    （m +MVL）
    （m +2xMVL）
    （m-1） （m-1）
    （m-1）
    （m-1）
    （n-MVL）
    （n-1）
    + MVL
    +2×MVL
    + 3 ×MVL
\end{verbatim}
图 4-5 用条带处理的任意长度的向量。除第一块外，所有其他块的长度都是 MVL，充分利用了向量处
理器的功能。本图中使用变量m来表示表达式（n %MVL）（C运算符%表示求模。）
以上代码的内层循环可以进行向量化，长度为 VL，或者等于（n % MML），或者等于MVL。
在此代码中，必须对VLR 寄存器设置两次，也就是在代码中为变量VL 进行赋值时各设置一次。

\subsection{向量遮罩寄存器：处理向量循环中的IF语句}

根据 Amdahl 定律，我们知道对于中低向量化级别的程序，加速比是非常有限的。循环内
部存在条件（IF语句）、稀疏矩阵是向量化程度较低的两个主要原因。如果程序的循环中包含
语句，由于IF 语句会在循环中引入控制相关，所以不能使用前面讨论的技术以向量模式运行
这种程序。同样，利用前面看到的各项功能也不能高效地实现稀疏矩阵。我们现在将讨论处理
条件执行的策略，稀疏矩阵留待后文讨论。

考虑以C语言编写的以下循环：

for （i= 0;i< 64; 1=i+1）
if （XCi］ I= 0）
X［i］ = X［i］ - YIi］；
由于这一循环体需要条件执行，所以它通常是不能进行向量化的；但是，如果对于X［i］ 0
的选代可以运行内层循环，那就可以实现减法的向量化。

这一功能的常见扩展称为向量遮罩控制。遮單寄存器可以用来实现一条向量指令中每个元
素运算的条件执行。向量遮罩控制使用布尔向量来控制向量指令的执行，就像条件执行指令使
用布尔条件来决定是否执行标量指令一样。在启用向量遮單寄存时，任何向量指令都只会针
对符合特定条件的向量元素来执行，即这些元素在向量遮罩寄存器中的相应项目为1。目标向
量寄存器中的其他项目（在遮罩寄存器中的相应项目为1）不受这些向量操作的影响。清除向
量遮罩寄存器会将其置为全1，后续向量指令将针对所有向量元素执行。我们现在可以为以上
循环使用下列代码，假定X、Y的起始地址分别为 Rx和 RY：
\begin{verbatim}
    LY
    V1,Rx
    ；将向重X載入V1
    LY
    V2,Ry
    ；载入向量Y
    L,D
    FO， #0
    ；将浮点零載入 F0
    SNEYS.D
    V1,FO
    ；若VI（1）！=F0，則将VM（i）设置 1
    SUBVV.D
    V1,V1,Y2
    ；在向童速平下执行减法
    SV
    V1,RX
    ；将结果存到X中
\end{verbatim}
编译器写人程序调用转换过程，使用条件执行|F转换将 IF语句修改为直行代码序列。
但是，使用向量遮罩寄存器是有开销的。对于标量体系结构，条件执行的指令在不满足条
件时也需要执行时间。不过，通过消除分支和有关的控制相关性，即使有时会做一些无用功，
也可以加快条件指令的执行速度。与此类似，对于采用向量遮單执行的向量指令，即使遮罩为
0的元素，仍然会占用相同的执行时间。与此类似，即使罩中有大量0，使用向量遮罩控制的
速度也仍然远快于使用标量模式的速度。

在4.4节将会看到，向量处理器与GPU之间的一个区别就是它们处理条件语句的方式不同。
向量处理器将遮罩寄存器作为体系结构状态的一部分，依靠编译器来显式操控遮罩寄存器。而
GPU 则是使用硬件来操控 GPU软件无法看到的内部遮罩寄存器，以实现相同效果。在这两种
情况下，无论遮罩是1还是0，硬件都需要花费时间来执行向量元素，所以 GFLPS 速率在使用
遮罩时会下降。

\subsection{内存组：为向量载入/存储单元提供带宽}

载人/存储向量单元的行为要比算术功能单元的行为复杂得多。载入操作的开始时间就是它
从存储器向寄存器中载入第一个字的时间。如果在无停顿情况下提供向量的其他元素，那么向
量初始化速率就等于提取或存储新字的速度。这一初始化速率不一定是一个时钟周期，因为存
储器组的停顿可能会降低有效吞吐量，这一点不同于较简单的功能单元。

一般情况下，载人/存储单元的起始代价要高于算术单元的这一代价——在许多处理器中要
多于100个时钟周期。对于 VMIPS，我们假定起始时间为12个时钟周期，与Cray-1 相同。（最
近的向量计算机使用缓存来降低向量载人与存储的延迟。）

为了保持每个时钟周期提取或存储一个字的初始化速率，存储器系统必须能够生成或
接受这么多的数据。将访问对象分散在多个独立的存储器组中，通常可以保证所需速率。
稍后将会看到，拥有大量存储器组可以很高效地处理那些访问多行或多列数据的向量载入
或存储指令。

大多数向量处理器都使用存储器组，允许进行多个独立访问，而不是进行简单的存储器交
错，其原因有以下3个。

（1）许多向量计算机每个时钟周期可以进行多个载人或存储操作，存储器组的周期时间通常
比处理器周期时间高几倍。为了支持多个载入或存储操作的同时访问请求，存储器系统需要有
多个组，并能够独立控制对这些组的寻址。
（2） 大多数向量处理器支持载入或存储非连续数据字的功能。在此类情况下，需要进行独立
的组寻址，而不是交叉寻址。
（3）大多数向量计算机支持多个共享同一存储器系统的处理器，所以每个处理器会生成其自
己的独立寻址流。

这些特征综合起来，就有了大量的独立存储器组，如下例所示。

例题
Cray T90（Cray T932） 的最高配置有32个处理器，每个处理器每个时钟周期可以
生成4个载入操作和2个存储操作。处理器时钟周期为 2.167 ms，而存储器系统所
用SRAM的周期时间为15 n\$。计算：为使所有处理器都以完全存储器带宽运行，
最少需要多少个存储器组。

解答
每个时钟周期产生的最大存储器引用数目为192：每个处理器每个时钟周期产生6
次引用共32个处理器。每个 SRAM组的繁忙时钟周期数为15/2.167=6.92，四舍
五入为7个处理器时钟周期。因此，至少需要192×7=1344个存储器组！

Cray T932 实际上有1024个存储器组，所以早期型号不能让所有处理器都同时维
持完全带宽。后来对存储器进行升级时，用流水化同步 SRAM代替了 15 ns 的异
步 SRAM，存储器周期时间缩短一半，从而可以提供足够的带宽。

从更高一级的角度来看，向量载人/存储单元与向量处理器中的预取单元扮演着类似的角
色，它们都是通过向处理器提供数据流来尝试提供数据带宽。

\subsection{步幅：处理向量体系结构中的多维数组}

向量中的相邻元素在内存中的位置可能并不一定是连续的。考虑下面一段非常简单的矩阵
乘法 C语言代码：
for （1 = 0; 1 < 100；
i=i+1）
for （j= 0; j≤ 100; j=j+1）｛
A［i］ Lj］ = 0.0；
for （k = 0; k < 100; k=k+1）
ACi［J］ - A［iIC3］ + B［i］ ［k］ * D［kICJ］；
｝

我们可以将B的每一行与D的每一列的乘法进行向量化，以k为索引变量对内层循环进行条带挖掘。
为此，我们必须考虑如何对B中的相邻元素及D中的相邻元素进行寻址。在为数组分配内
存时，该数组是线性化的，其排序方式要么以行为主（如C语言），要么以列为主（如 Fortran
语言）。这种线性化意味着：要么行中的元素在内存中不相邻，要么列中的元素在内存中不相邻。
例如，上面的C代码是按照以行为主的排序来分配内存的，所以内层循环中各次迭代在访问D
元素时，这些元素之间的间隔等于行大小乘以8（每一项的字节数），共为800个字节。在第2
章中，我们已经知道在基于缓存的系统中通过分层有可能提高局域性。对于没有缓存的向量处
理器，需要使用另一种方法来提取向量在内存中不相邻的元素。

对于那些要收集到一个寄存器中的元素，它们之间的距离称为步幅。在这个例子中，矩阵
D 的步幅为100个双字（800个字节），矩阵B的步幅可能为1个双字（8个字节）。对于以列为
主的排序（Fortran语言采用这一顺序），这两个步幅的大小会颠倒过来。矩阵D的步幅为1，也
就是说连续元素之间相隔1个双字（8个字节），而矩阵B的步幅为100，也就是100个双字（800
个字节）。因此，如果不对循环进行重新排序，编译器就不能隐藏矩阵B和D中连续元素之间的
较长距离。

将向量载入向量寄存器后，它的表现行为就好像它的元素在逻辑上是相邻的。仅利用具有
步幅功能的向量载入及向量存储操作，向量处理器可以处理大于1的步帽，这种步幅称为非单
位步幅。向量处理器的主要优势之一就是能够访问非连续存储器位置，并对其进行调整，放到
一个密集结构中。缓存在本质上是处理单位步幅数据的。增大块大小有助于降低大型科学数据
集（其步幅为单位步幅）的缺失率，但增大块大小也可能会对那些以非单位步幅访问的数据产
生负面影响。尽管分块技术可以解决其中一些问题（见第2章），但在4.7节将会看到，高效访
问非连续数据的功能仍然是向量处理器的一个优势。

在VMIPS结构中，可寻址单位为1个字节，所以我们示例的步幅将为800。由于矩阵的大
小在编译时可能是未知的，或者就像向量长度一样，在每次执行相同语句时可能会发生变化，
所以必须对步幅值进行动态计算。向量步幅可以和向量起始地址一样，放在通用寄存器中。然
后，VMIPS 指令LWuS （load vector with stride）将向量提取到向量寄存器中。同样，在存储非单
位步幅向量时，使用措令 SWS（store vector with stride ）。

为了支持大于1的步幅，会使存储器系统变得复杂。在引入非单位步幅之后，就有可能频
繁访问同一个组。当多个访问对一个存储器组产生竞争时，就会发生存储器组冲突，从而使某
个访问陷人停顿。如果满足以下条件则会产生组冲突，从而产生停顿。

组数
步幅与组数的最小公倍数
＜組繁忙时间
例题

假定有8个存储器组，组繁忙时间为6个时钟周期，总存储器延迟为12个时钟周期。
要以步幅1完成一个64元素的向量载人操作，需要多长时间？步幅为32呢？
解答

由于组数大于组繁忙时间，所以当步幅为1时，该载人操作将耗费 12+64=76个
时钟周期，也就是每个元素需要1.2个时钟周期。最糟糕的步幅是存储器组数目
的倍数，在本例中就是步帽为32、存储器组为8的情况。（在第一次访问之后，）
对存储器的每次访问都会与上一次访问发生冲突，必须等候长度为6个时钟周期
的组繁忙时间。总时间为 12+1+6x63=391个时钟周期，即每个元素6.1个时钟
周期。

\subsection{集中-分散：在向量体系结构中处理稀疏矩阵}

前面曾经提到，稀疏矩阵是很常见的，所以非常需要一些技术，能够以向量模式运行那些
处理稀疏矩阵的程序。在稀疏矩阵中，向量的元素通常以某种紧凑形式存储，然后对其进行间
接访问。假定有一种简化的稀疏结构，我们可能会看到类似下面的代码：

for （i= O;i<n；
i=1+1）
A［K［i］］= A［K［iJ］+ C［MCiJ］：

这一代码实现数组A与数组C的稀疏向量求和，用索引向量K和M来指出A与C中的非零元素。
（A和C的非零元素数必须相等，共有n个，所以K和M的大小相同。）

用于支持稀疏矩阵的主要机制是采用索引向量的集中一分散操作。这种运算的目的是支持在
稀疏矩阵的压缩表示（即不包含零）和正常表示（即包含零）之间进行转换。集中操作是取
得索引向量，并在此向量中提取元素，元素位置的确定是将基础地址加上索引向量中给定的
偏移量。其结果是向量寄存器中的一个密集向量。在以密集形式对这些元素进行操作之后，

再使用同一索引向量，通过分散存储操作，以扩展方式存储这一稀疏向量。对此类操作的硬
件支持被称为集中-分散，几乎所有现代向量处理器都具备这一功能。VMIPS 指令为LVI（载
人索引向量，也就是集中）和SVI（存储索引向量，也就是分散）。例如，如果 Ra、RC、RK
和Rm 中包含以上序列中向量的起始地址，就可以用向量指令来对内层循环进行编码，如下
所示：
LY
Vk,Rk
；载入K
LVI
Va， （Ra+Vk）
；載入 ACKC］］
LV
Vm, Rm
LVI
；載入M
Vc， （Rc+Vm）
：載入 C［ML］］
ADDVY.D
Va, Va,Yc
；求和
SVI
（Ra+Vk）， Va
；存储 ACKLJ］

这一技术允许以向量模式运行带有稀疏矩阵的代码。简单的向量化编译器可能无法自动实
现以上源代码的向量化，因为编译器可能不知道K的元素是离散值，因此也就不存在相关性。
相反，应当由序员发出的指令告诉编译器，可以放心地以向量模式来运行这一循环。

尽管索引载入与存储（集中与分散）操作都可以流水化，但由于存储器组在开始执行指令
时是未知的，所以它们的运行速度通常远低于非索引载人或存储操作。每个元素都有各自的地
址，所以不能对它们进行分组处理，在存储器系统的许多位置都可能存在冲突。因此，每次访
问都会招致严重的延迟。但是，如4.7节所示，如果架构师不是对此类访问采取放任态度，而
是针对这一情景进行设计，使用更多的硬件资源，那存储器系统就能提供更好的性能。

在4.4节将会看到，在GPU中，所有载人操作都是集中，所有存储都是分散。为了避免在
常见的单位步幅情景中缓慢运行，应当由GPU程序员来确保一次集中或分散操作中的所有地址
都处于相邻位置。此外，GPU硬件在执行时间必须能够识别这些地址序列，将集中与分散操作
转换为更高效的存储器单位步幅访问。

\subsection{向量体系结构编程}

向量体系结构的优势在于编译器可以在编译时告诉程序员：某段代码是否可以向量化，通
常还会给出一些暗示，说明这段代码为什么不能向量化。这种简单的执行模型可以让其他领域
的专家了解如何通过修改自己的代码来提高性能，如果可以假定操作之间是相互独立的（比如
集中-分散数据传送操作），那也可以给编译器提供一点提示。这就是编译器与程序员之间的对
话，每一方都为对方提供一些关于如何提高性能的线索，从而简化向量计算的编程。

今天，以向量模式运行的程序能否成功，其主要影响因素是程序本身的结构：循环是否有
真正的数据相关（见4.5节），能否调整它们的结构，使其没有此类相关？这一因素受选定算法
的影响，在一定程度上还受编码方式的影响。

让我们看一下在 Perfect Club 基准测试中观测到的向量化水平，用以指示科学程序中所能实
现的向量化水平。表4-2显示了两种代码版本在 Cray Y-MIP 上运行时，以向量模式运行的运算
比例。第一个版本仅对原代码进行了编译器优化，而第二个版本则利用了Cray Research 程序员
团队给出的一些提示。对向量处理器上的应用程序性能进行多次研究后发现，编译器向量化水
平的变化范围很大。
\begin{verbatim}
    表4-2 在 Cray Y-MP 上执行 Perfect Club 基准测试所获得的向量化水平ajapeyam 1991］
    蒸准测试名称
    以向豐模式执行的运算，
    以向量横式执行的运算，
    编译器优化
    有程序员提供帮助
    BDNA
    96.1%
    97.2%
    MG3D
    95.1%
    94.5%
    FLOS2
    91.$%
    88.7%
    ARC3D
    91.1%
    92.0%
    SPEC77
    90.3%
    90.4%
    MDG
    87.7%
    94.2%
    TRFD
    69.8%
    73.7%
    DYFESM
    68.8%
    65.6%
    ADM
    42.9%
    $9.6%
    OCEAN
    42.8%
    91.2%
    TRACK
    14.4%
    54.6%
    SPICE
    11.5%
    79.9%
    QCD
    4.2%
    75.1%
    根据提示进行优化后
    获得的加速比
    1.$2
    1.00
    NA
    1.01
    1.07
    1.49
    1.67
    N/A
    3.60
    3.92
    2.52
    4.06
    2.15
\end{verbatim}
*第一列显示在没有提示下用编译器获得的向量化水平，而第二列是在根据 Cray Research 程序员团队的提示对代码进
行改进后的结果。

对于编译器自身不能很好地完成向量化的代码来说，根据大量提示进行修改后的版本会大
幅提高向量化水平，现在有超过50%的代码可以进行向量化了。平均向量化水平从大约70%提
高至大约90%。

\section{SIMD 指令集多媒体扩展}
SIMID 多媒体扩展源于一个很容易观察到的事实：许多媒体应用程序操作的数据类型要比
对32位处理器进行针对性优化的数据类型更窄一些。许多图形系统使用8位来表示三基色中的
每一种颜色，再用8位来表示透明度。根据不同的应用程序，音频采样通常用8位或16位来表
示。假定有一个256位加法器，通过划分这个加法器中的进位链，处理器可以同时对一些短向
量进行操作，这些向量可以是32个8位操作数、16个16位操作数、8个32位操作数或者4个
64 位操作数。这些经过划分的加法器的额外成本很小。表4-3总结了典型的多媒体 SIMID指令。
和向量指令一样，SIMID 指令规定了对数据向量的相同操作。一些向量机器拥有大型寄存器堆，
比如 VMIPS 向量寄存器，8个向量寄存器中的每一个都可以保存64个64位元素，SIMD指令
与之不同，它指定的操作数较少，因此使用的寄存器堆也较小。

向量体系结构专门针对向量化编译器提供了一流的指令集，与之相对，SIID扩展主要进
行了以下3项简化。

\begin{itemize}
    \item 多媒体 SIMID扩展固定了操作代码中数据操作数的数目，从而在x86体系结构的 MIMX、
    SSE 和 AVX扩展中添加了数百条指令。向量体系结构有一个向量长度寄存器，用于指定
    当前操作的操作数个数。一些程序的向量长度小于体系结构的最大支持长度，由于这些
    向量寄存器的长度可以变化，所以也能够很轻松地适应此类程序。此外，向量体系结构
    有一个隐含的最大向量长度，它与向量长度寄存器相结合，可以避免使用大量操作码。
    
    \item 多媒体 SIMID 没有提供问量体系结构的更复杂寻址模式，也就是步幅访问和集中-分散访
    问。这些功能增加了向量编译器成功向量化的程序数目（见4.7节）。
    
    \item 多媒体SIMID通常不会像向量体系结构那样，为了支持元素的条件执行而提供遮罩寄存器。
    这些省略增大了编译器生成 SIMD代码的难度，也加大了 SIMD 汇编语言编程的难度。
    表4-3 典型 SIMD 多媒体运算汇总（运算宽度为256位）
\end{itemize}

折伞类别
操作数
无符号加/诚
32个8位、16个16位、8个32位或4个64位
最大/最小
32个8位、16个16位、8个32位或4个64位
平均
32个8位、16个16位、8个32位或4个64位
右/左移位
32个8位、16个16位、8个32位或4个64位
深点
16个16位、8个32位、4个64位或2个128位

 注意，IEEE 754-2008淨点标准增加了半精度（16位）和四精度（128住）浮点运算。
对于 x86 体系结构，1996年增加的 MIMIX 指令重新确定了64位浮点寄存器的用途，所以
基本指令可以同时执行8个8位运算或4个16位运算。这些指令与其他各种指令结合在一起，
包括并行 MAX和 MIN 运算、各种遮罩和条件指令、通常在数字信号处理器中进行的运算以
及人们相信在重要媒体库中有用的专用指令。注意，MMX重复使用浮点数据传送指令来访问
存储器。

1999年推出的后续流式 SIMID扩展（SSE）添加了原来宽128位的独立寄存器，所以现在
的指令可以同时执行16个8位运算、8个16位运算或4个32位运算。它还执行并行单精度浮
点运算。由于 SSE拥有独立寄存器，所以它需要独立的数据传送指令。Intel很快在2001年的
SSE2、2004年的SSE3和2007年的 SSE4中添加了双精度SIMID 浮点数据类型。拥有四个单精
度浮点运算或两个并行双精度运算的指令提高了x86计算机的峰值浮点性能，只要程序员将操
作数并排放在一起即可。在每一代计算机中都添加了一些专用指令，用于加快一些重要的特定
多媒体功能的速度。

2010年增加的高级向量扩展（AVX）再次将寄存器的宽度加倍，变为256位，并提供了
一些措令，将针对所有较窄数据类型的运算数目翻了一番。表4-4给出了可用于进行双精度浮
点计算的 AVX指令。AVX进行了一些准备工作，以便在将来的体系结构中将宽度扩展到 512
位和1024位。

表4-4 在双精度浮点程序中有用的 x86体系结构 AVX指令
AVX指令
说 明
VADDPD
加上4个紧缩双精度操怍数
VSUBPD
减去4个紧缩双精度操作数
VMULPD
乘以4个紧缩双精度操怍数
VDIVPD
除以4个紧缩双精度撮作数
VFMADDPD
乘、加4个紧缩双精度操作数
VFMSUBPD
乘、减4个紧缩双精度操作数
VCMPXX
对比4个紧缩双精度操作数，结果为EQ、NEQ、LT、LE、GT、GE，等等
VMOVAPD
移动对齐的4个紧缩双精度操作数
VBROADCASTSD
将一个多精度操作数广播至256位寄存器中的4个位置
 256 位 AVX的肾縮双精廣是指以 SIMD模式执行的4个64 位操作教。当 AVX 指令的寬度增大时，教据置換指令的
添加也变得更沟雪受，以允许将来自宽寄存器中不同部分的审操作数结合起来。AVX中的一些指令可以在256位寄
存器中分散32位、64位或 128位操作教。比如，BROADCAST 在 AVX 寄存器中将一个64住操作数复制4次。AVC
还包含大量结合在一起的乘加/乘减指令，这里仅给出了其中的两个。


一般来说，这些扩展的目的是加快那些精心编制的库函数运行速度，而不是由编译器来生
成这些库（参见附录H），但近来的x86编译器正在尝试生成此类代码，尤其是针对浮点计算密
集的应用程序。

既然有这些弱点，那多媒体SIMD扩展为什么还如此流行呢？第一，它们不需要花费什么
成本就能添加标准算术单元，而且易于实施。第二，与向量体系结构相比，它们不需要什么额
外状态，上下文切换次数总是要考虑这一因素。第三，需要大量存储器带宽来支持向量体系结
构，而这是许多计算机所不具备的。第四，当一条能够生成64个存储器访同的指令在向量中间
发生页面错误时，SIMD不必处理虚拟内存中的问题。SIMID 扩展对于操作数的每个 SIMD组使
用独立的数据传送（这些操作数在存储器中是对齐的），所以它们不能跨越页面边界。固定长度
的简短SIMLD “向量”还有另一个好处：能够很轻松地引入一些符合新媒体标准的指令，比如
执行置换操作的指令或者所用操作数少于或多于所生成向量的指令。最后，人们还关注向量体
系结构在使用缓存方面的表现。最近的向量体系结构已经解决了所有这些问题，但由于过去一
些缺陷的影响，架构师还是对向量抱有怀疑态度。

例题
为了了解多媒体指令是什么样子的，假定我们向 MIIPS 中添加了 256 位 SIMD 多媒
体指令。在这个例子中主要讨论浮点指令。对于一次能够对4个双精度运算数执
行操作的指令添加后缀“40”。和向量体系结构一样，可以把 SIMID处理器看作是
拥有车道的处理器，在本例中为4个车道。MIPS SIMID 会重复利用浮点寄存器，
作为40指令的操作数，就像原始 MIIP 中的双精度运算重复利用单精度寄存器一样。
这一示例显示了 DAXPY 循环的 MIPS SIMID代码。假定 X和Y的起始地址分别为 Rx
和Ry。用下划线划出为添加 SIMID 而对MIPS代码进行的修改。

解答
下面是 MIPS代码：
\begin{verbatim}
    L.0
    ；載入标量a
    HOY
    MOY
    El F0
    E2,F0
    ：将a复制到 F1，以完成 SIMD MUL
    i将a 复制到F2.以完成 SIMD MUL
    MOY
    ；将a复制到F3，以完成 SIMD MUL
    DADDIU
    R4,Rx，#512
    ；安載入的敢后一个地址
    Loop：
    L.4P
    E4,0（Rx）
    ：載入XC1］，XCi+1］，X厂i+2］，XCi+31
    MUL、4D
    F4,F4,FO
    ：aXC1］，a××1+17,a××T1+2］.a××1+3］
    L.4D
    E8,O（Ry）
    ：載入YEi，Y［i+1］、Y［i+2］，Y［i+3］
    ADD.4D
    E8.F8.F4
    a XXCi］+YLi］a
    axX［i+3］+YTi+3］
    S.AD
    E8,0（Rx）
    ；存储到Y［1］，YTi+i,Yri+2］， Y［t+3
    DADDIU
    Rx,Rx，#32
    ：将索引递增至X
    DADDIU
    Ry,Ry，#32
    ：将索引邀增至Y
    DSUBU
    R20,R4,RX
    ；计算范围
    BNEZ
    R20, Loop
    ；检查是否完成
\end{verbatim}
这些修改包括将所有 MIPS双精度指令用对应的40等价指令代替，将递增步长由8
变为32，将寄存器由F2和F4改为F4 和F8，以在寄存器堆中为4个连续双精度操
作数获取足够的空间。所以，对于标量a，每个 SIMD车道都将拥有自己的一个副
本，我们将 FO的值复制到寄存器F1、F2 和F3。（真正的 SIMD指令扩展有一条指
令，可以向组中的所有其他寄存器广播一个值。）因此，这一乘法将完成F4*FO、
F5*F1、F6*F2 和 F7*F3。尽管 SIMID MIPS 没有像 VIMIIPS那样，将动态指令宽带降
低100倍，但也降低了4倍，共有149条，而 MIPS 则为578条指令。

\subsection{多媒体SIMD体系结构编程}
由于SIMID 多媒体扩展的特有本质，使用这些指令的最简便方法就是通过库或用汇编语
言编写。

最近的扩展变得更加规整，为编译器提供了更为合理的目标。通过借用向量化编译器的技
术，这些编译器也开始自动生成SID 指令。例如，目前的高级编译器可以生成SIMD浮点指
令，大幅提高科学代码的性能。但是，程序员必须确保存储器中的所有数据都与运行代码的
SIMD 单元的宽度对齐，以防止编译器为本来可以向量化的代码生成标量指令。
\subsection{Roofline可视性能模型}
有一种直观的可视方法可以对比各种 SIMID体系结构变体的潜在浮点性能，那就是 Roofine
模型［Williams 等人，2009］。它将浮点性能、存储器性能和运算密度汇总在一个两维图形中。运
算密度等于浮点运算数与所访问存储器字节的比值。其计算方法为：获取一个程序的总浮点运
算数，然后再除以在程序执行期间向主存储器传送的总数据字节。图4-6给出了几种示例内核
的相对运算密度。
O（1）
log（N））
ON）
稀疏
矩阵
频谱方法
（FFTs）
密集矩阵
（BLAS3）
（SpMV）
N主体
（粒子方法）
结构化网
结构化网格
格（Stencil，
（栅格方法）
PDEs）
图 4-6
运算密度，定义为：运行程序时所执行的浮点运算数除以在主存储器中访问的字节数［Williams
等人，2009］。一些内核的运算密度会随问题的规模（比如密集矩阵）而缩放，但有许多核心的
运算密度与问题规模无关

峰值浮点性能可以使用硬件规范求得。这一实例研究中的许多核心都不能放到片上缓存中，
所以峰值性能是由缓存背后的存储器系统确定的。注意，我们需要的是可供处理器使用的峰值
存储器带宽，而不只是4.7节表4-13中 DRAM管脚处的可用宽带。要求出（所提供的）峰值存
储器性能，其中一种方法是运行 Stream 基准测试。

图4-7在左侧给出 NEC SX-9向量处理器的Roofline模型，在右侧给出 Intel Core i7920多
核计算机的相应模型。垂直的Y轴是可以实现的浮点性能，为2~256 GFLOP/s。水平的X轴是
运算密度，在两个图中都是从 1/8 FLOP/DARM访问字节到16 FLOP/DARM 访问字节。注意，
该图为对数-对数图尺，Rootline对于一种计算机仅完成一次。

对于一个给定内核，我们可以根据它的运算密度在X轴上找到一个点。如果过该点画一条
垂线，此内核在该计算机上的性能必须位于该垂线上的某一位置。我们可以绘制一个水平线，
显示该计算机的浮点性能。显然，由于硬件限制，实际浮点性能不可能高于该水平线。

如何绘制峰值存储器性能呢？由于X轴为FLOP/字节，Y轴为FLOP/s，所以字节/s就是
图中45度角的对角线。因此，我们可以画出第三条线，显示该计算机的存储器系统对于给定
运算密度所能支持的最大浮点性能。我们可以用公式来表示这些限制，以绘制图4-7中的相
应曲线：

可获得的GFLOP/s=Min（峰值存储器带宽×运算密度，峰值浮点性能）
Intel Core i7 920
NEC SX-9 CPU
2564
（Nehalem）
- 102.4GFLOP/s
286
2564
128
64
32
16
42.66GFLOP/s
麼 32
精
双 16
图4-7
8
4
8
（Strcam）
4
2
1/8
1/4 1/2
一
2 4
8
16
2
1/8
1/4 1/2
1
2
4
8
16
运算密度
运算密度

左图为 NEC SX-9 向量处理器上的 Roofine 模型，右图为采用SIMD扩展的 Intel Core i7 920
多核计算机的相应模型IWiliams 等人，2009］。这个 Roofline 模型针对的单位步幅的存储器访问
和双精度浮点性能。NEC SC-9 是在2008年发布的超级向量计算机，耗费了数百万美元。根据
Stream 基准测试，它的峰值 DP FP性能为102.4 GFLOP/s，峰值存储髒宽度为162GB/s。Core 7920
的峰值 DP FP 性能为42.66 GFLOP/s 和峰值存储器带宽为16.4GB/s。在运算密度为4 FLOP/字节
处的垂直虚线显示两个处理器都以峰值性能运行。在这个示例中，102.4 GFLOP/s处的Sx-9要比
42.66 GFLOP/s处的 Core i7快2.4倍。在运算密度为0.25 FLOP/字节处，SX-9为 40.5 GFLOP/S，
比 Core i7 的4.1 GFLOP/s快10倍

水平线和对角线给出了这个简单模型的名字，并指出了它的取值。Roofline根据内核的运
算密度设定了其内核的性能上限。如果我们把运算密度看作是触及房顶的柱子，它既可能触及
房顶的平坦部分（表示这一性能是受计算功能限制的），也可能触及房顶的倾斜部分（表示这一
性能最终受存储器带宽的限制）。在图4-7中，右侧的垂直虚线（运算密度为4）是前者的示例，
左侧的垂直虚线（运算密度为1/4）是后者的示例。给定一台计算机的 Roofline模型，就可以重
复应用它，因为它是不会随内核变化的。

注意对角线与水平线交汇的“屋脊点”，通过它可以深入了解这台计算机的性能。如果它非
常靠右，那么只有运算密度非常高的内核才能实现这台计算机的最大性能。如果它非常靠左，
那么几乎所有内核都可能达到最高性能。后面将会看到，与其他SIMID处理器相比，这个向量
处理器的存储器带宽要高得多，屋脊点非常靠左。

图4-7显示SX-9 的峰值计算性能比 Core i7快2.4倍，但存储器性能要快10倍。对于运算
密度为0.25的程序，SX-9快10倍（40.5 GFLOP/s 比 4.1 GFLOP/s）。更宽的存储器带宽将屋脊
点从 Core i7的2.6移动到SX-9的0.6，这就意味着有更多的程序可以在这个向量处理器上达到
峰值计算性能。

\section{图形处理器}

只需要几百美元，任何人都能买一个具有数百个并行浮点单元的GPU，从而更容易实现
高性能计算。当 GPU 的计算潜力与一种简化GPU编程的编程语言相结合时，人们对GPU计
算的兴趣大增。因此，当今许多科学与多媒体应用程序的程序员都在考虑是使用GPU 还是使
用CPU。

GPU 和 CPU 在计算机体系结构谱系中不会上溯到同一个祖先；并没有哪个“过渡环节”
可以解释这两者之间的关系。如4.10节中的介绍，GPU的祖先是图形加速器，而极强的图
形处理能力正是GPU得以存在的原因。尽管GPU 正在转向主流计算领域，但它们不能放弃
继续在图形处理领域保持优异表现的责任。因此，对于能够出色处理图形的硬件，当架构师
询问应当如何进行补充才能提高更广泛应用程序的性能时，GPU的设计就可能体现出更重要
的价值。

注意，这一节主要讨论使用GPU进行计算。若要了解GPU计算如何与传统的图形加速角
色相结合，请参阅 John Nickolls 和 David Kirk等人的文章“图形与计算GPU”（本书作者编著
的《计算机组成与设计》一书第4版的附录A）。

由于这一体系结构的术语和一些硬件功能都与向量和SIMID 体系结构有很大不同，所以我们
认为，在介绍这一体系结构之前，首先从 GPU 的一种简化编模型入手会更容易一些。

\subsection{GPU编程}

关于如何表示算法中的并行，CUDA 绝对是一种非常出色的解决方案，尽管不能表示
所有算法中的并行，却也足够了。它在某种方式上与我们的思考与编码方式相吻合，可以
更轻松、更自然地表达超越任务级别的并行。
— Vincent Natol
“'Kudos for CUDA”，HPC Wire（2010）

CPU程序员的挑战不只是在GPU上获得出色的性能，还要协调系统处理器与GPU上的计
算调度、系统存储器与 GPU存储器之间的数据传输。此外，在本节后面将会看到，GPU 几乎
拥有所有可以由编环境捕获的并行类型：多线程、MIMID、SIMD，甚至还有指令级并行。
NVIDIA 决定开发一种与C类似的语言和编程环境，通过克服异质计算及多种并行带来的
双重挑战来提高 GPU程序员的生产效率。这一系统的名称为 CUDA，表示“计算统一设备体系
结构”（Compute Unified Device Architecture）。CUDA 为系统处理器（主机）生成C/C++，为
GPU（设备，也就是CUDA 中的D）生成C和C++方言。一种类似的编程语言是OpenGL，几
家公司共同开发这一语言，为多种平台提供一种与供应商无关的语言。

NVIDIA 认为，所有这些并行形式的统一主题就是 CUDA线程。以这种最低级别的并行作
为编程原型，编译器和硬件可以将数以千记的CUDA线程聚合在一起，利用CPU 中的各种并
行类型：多线程、MIIMID、SIMD 和指令级并行。因此，NVIDIA 将 CUDA编程模型定义为“单
指令多线程”（SIMT）。这些线程进行了分块，在执行时以32个线程为一组，称为线程块，我
们马上会明白其原因。我们将执行整个线程块的硬件称为多线程SIMD处理酱。

我们只需要几个细节就能给出 CUDA程序的示例。
口为了区分GPU（设备）的功能与系统处理器（主机）的功能，CUDA使用\verb|_device_|或
\verb|global_|表示前者，使用\verb|_host__|表示后者。
口被声明为\verb|_device_|或\verb|_global__functions|的CUDA变量被分配给GPU存储器见下文），
可以供所有多线程 SIMID 处理器访问。

口 对于在 GPU上运行的函数 name 进行扩展函数调用的语法为：
namescdinGrid, dimBlock>>>（... parameter Jist ...）
其中 dinGrid 和 dimBlock 规定了代码的大小（用块表示）和块的大小（用线程表示）。
口除了块识别符（blockIdx）和每个块的线程识别符（threadidx）之外，CUDA 还为每个
块的线数提供了一个关键字（blockDim），它来自上一个细节中提到的 dinBlock 参数。
在查看 CUDA 代码之前，首先来看看4.2节 DAXPY循环的传统C代码：

// 调用 DAXPY
daxpy（n, 2.0. x. y）；
/1 C语富编写的 DAXPY
void daxpy（int n, double a, double *x, double *y）
for （int i=0; isn: tti）
yCij = a*x［ij + yli］：

下面是 CUDA 版本。我们在一个多线程 SINID 处理器中启动n个线程，每个向量元素一个线
程，每个线程块256个 CUDA线程。GPU功能首先根据块ID、每个块的线程数以及线程ID
来计算相应的元素索引1。只要这个索引没有超出数组的范围（isn），它就会执行乘法和加法。
11 调用 DAXPY，每个线程块中有256个线程

host
int nblocks = （n+ 255）/ 256；
daxpycksnblocks, 256>>>（n,2.0,x, y）；
// CUDA 中的 DAXPY
device
void daxpy（int n, double a, double *x, double *y）
｛
int i = blockidx.x*blockDim.x + threadldx.x；
if （i <n） y［i］ = a*xlil + yEis
｝

对比C代码和CUDA代码，我们可以看出一种用于实现数据并行 CUDA代码并行化的共
同模式。C版本中有一个循环的所有迭代都与其他选代相独立，可以很轻松地将这个循环转换
为并行代码，其中每个循环选代都变为一个独立线程。（前面曾经提到，向量化编译器也要求循
环的选代之同没有相关性，这种相关被称为循环间相关，4.5节将详细介绍。）程序员通过明确
指定网格大小及每个 SIMD处理器中的线程数，明确指出 CUDA 中的并行。由于为每个元素都
分配了一个线程，所以在向存储器中写人结果时不需要在线程之间实行同步。

行执行和线程管理由GPU硬件负责，而不是由应用程序或操作系统完成。为了简化硬件处
理的排程，CUDA要求线程块能够按任意顺序独立执行。尽管不同的线程块可以使用全局存储
器中的原子存储器操作进行协调，但它们之间不能直接通信。

马上可以看到，许多GPU硬件概念在CUDA 中不是非常明显。从程序员生产效
来看，这是一件好事，但大多数程序员使用GPU 而不是CPU来提高性能。重视性能的穩
角度
损
在用CUDA编写程序时必须时刻惦记着GPU硬件。他们知道需要将控制流中的32个线種分为
一组，以从多线程 SIMID处理器中获得最佳性能，并在每个多线程SIMID处理器中另外创建许
多线程，以隐藏访问 DRAM的延迟，稍后将解释其原因。它们还需要将数据地址保持在一个或
一些存储器块的局部范围内，以获得所期望的存储器性能。

和许多并行系统一样，CUDA 在生产效率和性能之间进行了一点折中：提供一些本身固有
的功能，让程序员能够显式控制硬件。一方面是生产效率，另一方面是使程序员能够表达硬件
所能完成的所有操作，在并行计算中，这两个方面之间经常会发生竞争。了解编程语言在这一
著名的生产效率与性能大战中如何发展，了解CUDA 是否能够在其他GPU或者其他类型的体
系结构中变得普及，都将是非常有意义的一件事。
\subsection{NVIDIA GPU计算结构}
上文提到的这些罕见传统可以帮助解释为什么 GPU 拥有自己的体系结构类型，为什么拥
有与CPU独立的专门术语。理解 GPU的一个障碍就是术语，有些词汇的名称甚至可能导致误
解。克服这一障碍的难度非常之大，这一章经过多次重写就是一个例证。为了让读者既能理解
GPU 的体系结构，又能学习许多采用非传统定义的GPU术语，我们最终的解决方案是使用
CUDA 术语来描述软件，而在开始时使用更具描述性的术语来介绍硬件，有时还会借用 OpenCL
使用的一些术语。在用我们的术语解释GPU 体系结构之后，再将它们对应到 NVIDIA GPU的
官方术语。

表4-5从左至右列出了本节使用的一些更具描述性的术语、主流计算中的最接近术语、我
们关心的官方 NVIDIA GPU术语，以及这些术语的简短描述。本节的后续部分将使用该表左侧
的描述性术语来解释 GPU的微体系结构特征。

表4-5 本章所用GPU 术语快速指南。第一列为硬件术语
类型
更具描述性的
名称
可向量化循环
除GPU术语之外的
官方CUDA/
书中描述
最接近旧术语
NVIDIA GPU术语
可向量化循环
网格
向量化循环体
桯序
抽象
（条带挖掘后的）
向量化循环体
线程块
SIMD车道操作
序列
标量循环的一次
迭代
CUDA线程
SIMD指令线程
向量指令线程
Warp
机踞
对象
処理
硬件
SIMD指令
多线程SIMD
处理器
线程块谰度程序
向量指令
（多线程）向量
处理嚣
标量处理羇
PTX指令
流式多处理器
Giga线程引擎
在GPU上执行的可向量化循环，由一个或多
个可以并行执行的线程块（向量化循环体）
构成
可以在多线程SIMD处理器上执行的向量化
循环，由一个或多个SIMID指令线程构成。
它们可以通过局部存储器通信
SIMD指令线程的垂直抽取，对应于一个
SIND车道所执行的一个元素。根据遮單和
预测寄存器对结果进行存储
一种传統线程，但它仅包含在多线程SIMD
处理器上执行的SIMDD指令。根据每个元素
的遮罩来存储结果
在多个SIMD车道上执行的单一SIMID指令
多线程SIMD处理器执行SIMD指令的线程，
与其他SID处理器无关
将多个线程块（向量化循环体） 指定给多线
程SIMD处理器
4.4 图形处理器
217
（续）
类型
更具描述性的
名称
SIMD线程调试
程序
除GPU术语之外的
最接近旧术语
多线程CPU中的
线程调度器
官方CUDA/
NVIDIA GPU术语
Warp调度程序
处理
硬件
SIMD车道
向量车道
线程处理器
GPU存储器
主存储器
全局存储器
书中描述
当SIMID指令线程做好执行准备之后，用于
调度和发射这些线程的硬件，包括一个记分
板，用于限踪SIMID线程执行
SIMID车道执行一个SIMD指令线程中针对
单个元素的操作。根据遮罩存储结果
可供GPU中所有多线程SIMD处理器访问的
DRAM存储器
每个SIMD车道专用的DRAM存储器部分
专用存储器
存储嚣
硬件
局部存储器
栈或线程局部存储
（操作系统）
局部存储器
局部存储嚣
共享存储器
一个多线程SIMD处理器的快速本地
SRAM，不可供其他SIMD处理器使用
SIMID车道
向量车道寄存器
线程处理器
跨越完整线程块（向量化循环体）分配的单
寄存器
寄存器
一SIMID车道中的寄存器
*这11种术语分沟 4个组。从上至下汐：程序抽象、机器对象、处理硬件和存储器硬件。表 4-8将向量术语与这里
的最接近术语关联在一起，表4-10和表4-11揭示了官方 CUDA/NVIDIA 和 AMD 术语与定义，以及 OpenCL.使
用的术语。

我们将以 NVIDIA 系统为例，它们是GPU体系结构的代表。具体来说，我们将使用上面
CUDA 并行编程语言的术语，以 Fermi 体系结构为例（见4.7节）。

和向量体系结构一样，GPU只能很好地解决数据级并行问题。这两种类型都拥有集中-分散
数据传送和遮罩寄存器，GPU处理器的寄存器要比向量处理器更多。由于它们没有一种接近的
标量处理器，所以 GPU 有时会在运行时以硬件实现一些功能，而向量计算机通常是在编译时用
软件来实现这些功能的。与大多数向量体系结构不同的是，GPU还依靠单个多线程SIMID处理
器中的多线程来隐藏存储器延迟（见第2章和第3章）。但是，要想为向量体系结构和 GPU编
写出高效代码，程序员还需要考虑 SIMD操作分组。

网格是在 GPU上运行、由一组线程块构成的代码。表4-5给出了网格与向量化循环、线程
块与循环体（已经进行了条带挖掘，所以它是完整的计算循环）之间的相似之处。作为一个具
体例子，假定我们希望把两个向量乘在一起，每个向量的长度为8192个元素。本节中，我们将
反复使用这一示例。图4-8给出了这个示例与前两个 GPU术语之间的关系。执行所有8192个
元素乘法的GPU 代码被称为网格（或向量化循环）。为了将它分解为更便于管理的大小，网格
可以由线程块（或向量化循环体）组成，每个线程块最多512个元素。注意，一条 SIMD 指令
一次执行32个元素。由于向量中有8192个元素，所以这个示例中有16个线程块（16=8192+
512）。网络和线程块是在GPU硬件中实现的编程抽象，可以帮助程序员组织自己的CUDA代
码。（线程块类似于—一个向量长度为32的条带挖掘向量循环。）

线程块调度程序将线程块指定给执行该代码的处理器，我们将这种处理器称为多线程
SIMD 处理器。线程块调度程序与向量体系结构中的控制处理器有某些相似。它决定了该循环
所需要的线程块数，在完成循环之前，一直将它们分配给不同的多线程SIMD处理器。在这个
示例中，会将16个线程块发送给多线程 SIMD处理器，计算这个循环的所有8192个元素。
291
293
218
第4章 向量、SIMI 和GPU体系结构中的数据级并行
294
网格
线秷块
o
…
线程抉
15
SIMID
线程0
SIMD
线程1
SIMID
线程1
…
SIMD
线狴0
SIMD
线程1
SIMD
线程1
5
AL
0
J=B［
］* C［
0
A［
1
〕=B［1J*CL 1
Ai 3iJ-Bc 3i
J* C［ 31
32
= B［32
］*CE
32
33
］
=B［ 33
］*C［
33
…
A［
s9：
〕=B［63］*CC
AI
64
〕=B［ 64］*CC
63
64
AI 479J-B［ 479
］* CC 479
A［ 480 1=
B
［480
* C［
480
A［
481 j=B［ 481
J* CC 481
］
］
］
］
］
］
J
］
］
A［ 511］-B［ 511 ］*CL 511 ］
A［
512 ］=B［ 512
］* CC
］
AT7679］-B ［7679 J *Ct 7679j
AL 7680］
= B［7680
I*C［ 7680
］
AI
768］
B ［ 7681 ］ * C［ 7681
］
A［ 7711］-B［7711 ］*CL 7711］
A［ 7712］=BL7712 ］ * CL 7712
AI 7713］-B ［7713 ］ * CL 7713j
Ai7743J-B17743 J *CL 7743j
AL 7744］- B ［7744 ］ * CI 7744 ］
⋯
A［ 8159］ - B ［ 8159 ］ * CL 8159 ］
A［ 8160］
= B［8160 ］*C［ 8160
AI 81611 - B ［ 8161 ］ * CI 8161
］
8191］-B［8191 ］* Cr 8191
］
段 4-8
网格（可向量化循环）、线程块（SIMD 基本块）和 SIMD 指令线程与向量-向量乘法的对应，
每个向量的长度为8192个元素。每个SIMD指令线程的每条指令计算32个元素，在这个示例
中，每个线程块包含16个 SIMD 指令线程，网格包含16 个线程块。硬件线程块调度程序将线
程块指定给多线程 SIMID 处理器，硬件线程调度程序选择某个 SIMD 指令线程来运行一个
SIMD 处理器中的每个时钟周期。只有同一线程块中的SIMD线程可以通过本地存储器进行通
信。（对于 Tesla 代的 GPU，每个线程块可以同时执行的最大 SIMD线数为16，后来 Fermi
一代的GPU为32。）

图4-9显示了多线程 SIMID处理器的简化框图。它与向量处理器类似，但它有许多并行功
能单元都是深度流水化的，而不是像向量处理器一样只有一小部分如此。在图4-8中的编程示
例中，向每个多线程 SIMD处理器分配这些向量的512个元素以进行处理。SIMI处理器都是具
有独立PC的完整处理器，使用线程进行编程（见第3章）。

Warp调度程序
Warp编号
指令缓存
1
3
3
8
8
地址
42
43
95
96
11
12
记分板
SIMD指令
ld.global.f64
mul.f64
shl.$32
add.$32
ld.global.t64
ld.global.t64
操作数
准备就緒
否
准备就緒
否
准备就緒
准备就緒
指令寄存髁
寄
存器
Reg
Reg
Reg
Reg
Reg
Reg
Reg
Reg
Reg
Reg
Reg
Reg
Rcg
Reg
Rsg
1Kx321Kx32 1Kx32 1K×32
1Kx32
|1Kx32 1K×32
1K×32 1Kx32
1Kx32 1Kx32|1K×321Kx32 1Kx32:1Kx32|1Kx32
载入 载入
存储
：存储
单元 单元
裁人
存储
单元
气
载人
存储
单元
裁入
存储
单元
载人
存储
单元
载人
存储
单元
载人
存储
单元
载人
教人
存储
存储
单元 单元
载人，教入
存储「存储
单元单元
载人载人裁入
存储 存储 存储
单元
单元 单元
载入
存储
单元
地址接合单元
互连网络
SIMD车道
（线程处
理髒）
局部存储器
64 KB
至全局
存储器
图4-9
多线程 SIMD 处理备的简化框图。它有16个 SIMD车道。SIMID线程调度程序拥有大约48个独立
的SIMD指令线程，它用一个包括48个PC的表进行调度

GPU 硬件包含一组用来执行线程块网络（向量化循环体）的多线程SIMD处理器，也就是
说，GPU 是一个由多线程SIMD处理器组成的多处理器。

Fermi体系结构的前四种实现拥有7、11、14或15个多线程SIMD处理器；未来的版本可
能仅有2个或4个。为了在拥有不同个多线程SIMID处理器的GPU型号之间实现透明的可伸缩
功能，线程块调度程序将线程块（向量化循环主体）指定给多线程SIMID处理器。图4-10给出
了Fermi体系结构的GTX 480实现的平面图。

具体地说，硬件创建、管理、调度和执行的机器对象是SIMD指令线程。它是一个包含专
用SIMID指令的传统线程。这些SIMD指令线程有其自己的PC，它们运行在多线程 SIMD处理
器上。SIMD线程调度程序包括一个记分板，让你知道哪些SIMID指令线程已经做好运行准备，
然后将它们发送给分发单元，以在多线程SIMID 处理器上运行。它与传统多线程处理器中的硬
件线程调度程序相同（见第3章），就是对 SIMD指令线程进行调度。因此GPU硬件有两级硬
件调度程序：（1）线程块调度程序，将线程块（向量化循环体）分配给多线程 SIMID处理器，确
保线程块被分配给其局部存储器拥有相应数据的处理器，（2）SIMD处理器内部的SIMD线程调
度程序，由它来调度应当何时运行 SIMD 指令线程。


图 4-10
Fermi GTX 480 GPU 的平面图。本图显示了16个多线程SIMDD处理器。在左侧突出显示了线
程块调度程序。GTX 480有6个GDDRS端口，每个端口的宽度为64位，支持最多6GB 的容量。
主机接口为 PCIExpress2.0x16。Giga线程是将线程块分发给多处理器的调度程序名称，其中每
个处理器都有其自己的SIMID线程调度程序

这些线程的 SIMD 指令的宽度为32，所以这个示例中每个 SIMD 指令线程将执行32个元素
运算。在本示例中，线程块将包含512-32=16 SIMD 线程（见图4-8）。

由于线程由 SIMID指令组成，所以 SIMD 处理器必须拥有并行功能单元来执行运算。我们
称之为 SIMD 车道，它们与4.2节的向量车道非常类似。

每个SIMD处理器中的车道数在各代GPU 中是不同的。对于 Fermi，每个宽度为32的SIMD
指令线程被映射到16个物理 SIMID车道，所以一个 SIMD 指令线程中的每条 SIMD指令需要两
个时钟周期才能完成。每个 SIMID指令线程在锁定步骤执行，仅在开始时进行调度。将 SIMID
处理器类比为向量处理器，可以说它有16个车道，向量长度为32，钟鸣为2个时钟周期。（我
们之所以使用术语“SIMID处理器”，而不是“向量处理器”，就是因为这种既宽且浅的本质，
前者的描述性更强一些。）

根据定义，由于 SIMD指令的线程是独立的，SIMID线程调度程序可以选择任何已经准备
就绪的SIMID 指令线程，而不需要一直盯着线程序列中的下一条 SIMD指令。SIMD线程调度程
序包括一个记分板（见第3章），用于跟踪多达48个 SIMD线程，以了解哪个 SIMD指令已经
做好运行准备。之所以需要这个记分板，是因为存储器访问指令占用的时钟周期数可能无法预
测，比如存储器组的冲突就可能导致这一现象。图4-11给出的SIMID线程调度程序在不同时间
以不同顺序选取 SIMD指令线程。GPU架构师假定GPU 应用程序拥有如此之多的 SIMD指令线
程，因此，实施多线程既可以隐藏到 DRAM 的延迟，又可以提高多线程SIMID处理器的使用率。
但是，为了防止损失，最近的 NVIDIA Fermi GPU 包含了一个L2缓存（见4.7节）。
221
Pholo:Judy Schoonmake
时间
SIMD 线程3 指令95
SIMD 线程3指令96
图 4-11

SIMD 指令线程的调度。调度程序选择一个准备就绪的 SIMID 指令线程，并同时向所有执行该
SIMD线程的SIMID车道发出一条指令。由于 SID指令线程是独立的，所以调度程序可以每次
选择不同的 SIMID线程

继续探讨向量乘法示例，每个多线程SIMD 处理器必须将两个向量的32个元素从存储器载
人寄存器中，通过读、写寄存器来执行乘法，然后将乘积从寄存器存回存储器中。为了保存这
些存储器元素，SIMD处理器拥有32768个32位寄存器，给人以深刻印象。就像向量处理器一
样，从逻辑上在向量车道之间划分这些寄存器，这里自然是在SIMID车道之间划分。每个 SIMID
线程被限制为不超过64个寄存器，所以我们可以认为一个 SIMD线程最多拥有64个向量寄存
器，每个向量寄存器有32个元素，每个元素的宽度为32位。（由于双精度浮点操作数使用两个
相邻的32位寄存器，所以另一种意见是每个 SIMD线程拥有32个各包括32个元素的向量寄存
器，每个宽度为64位。）

由于 Ferni拥有16个物理SIMD车道，各包含2048个寄存器。（GPU没有尝试根据位来设
计硬件寄存器，使其拥有许多读取端口和写入端口，而是像向量处理器一样，使用较简单的存
储器结构，但将它们划分为组，以获得足够的带宽。）每个 CUDA 线程获取每个向量寄存器中
的一个元素。为了用16个 SIMID车道处理每个 SIMD 指令线程的32个元素，线程块的CUDA
线程可以共同使用2048个寄存器的一半。

为了能够执行许多个 SIMID 指令线程，需要在创建SIMD指令线程时在每个SIMD处理器
上动态分配一组物理寄存器，并在退出 SIMD线程时加以释放。

注意，CUDA线程就是SIMID指令线程的垂直抽取，与一个 SIMD车道上执行的元素相对
应。要当心，CUDA线程与 POSIX线程完全不同；不能从 CUDA线程进行任意系统调用。
现在可以去看看 GPU指令是什么样的了。

\subsection{NVIDA GPU指令集体系结构}

与大多数系统处理器不同，NVIDIA编译器的措令集目标是硬件指令集的一种抽象。PTX
（并行线程执行）为编译器提供了一种稳定的指令集，可以实现各代 GPU之间的兼容性。它向
程序员隐藏了硬件指令集。PTX指令描述了对单个 CUDA线程的操作，通常与硬件指令一对一
映射，但一个 PTX可以扩展到许多机器指令，反之亦然。PTX使用虚拟寄存器，所以编译器指
出一个 SIMD线程需要多少物理向量寄存器，然后，由优化程序在 SIMD线程之间划分可用的
寄存器存储。这一优化程序还会清除死亡代码，将指令打包在一起，并计算分支发生发散的位
置和发散路径可能会豪的位置。

尽管 x86 微体系结构与PTX之间有某种类似，这两者都会转换为一种内部形式（x86的微
指令），区别在于：对于x86，这一转换是在执行过程中在运行时以硬件实现的，而对于 GFU，
则是在载人时以软件实现的。

PTX指令的格式为：
opcode.type d. a, b,c；
其中d是目标操作数，a、b和c是源操作数；操作类型如表4-6所示。
表4-6PTX操作类型
\begin{verbatim}
    ［298
    类型
    无类型位8、16、32和64位
    无符号整数8.16、32和64位
    有符号整数8、16、32和64位
    浮点16、32和64位
    类型区分符
    .58.b16.532.b64
    .u8,u16.u32..u64
    .$8..$16..$32..564
    .f16..f32..f64
    源操作数为32位或64位整数或常值。目标操作数为寄存器，存储指令除外。
    表4-7显示了基本 PTX指令集。所有指令都可以由1位谓词寄存器进行判定，这些寄存器
    可以由设置谓词指令（setp）来设定。控制流指令为函数 call 和 return，线程 exit、branch 以
    及线程块内线程的屏障同步（bar.sync）。在分支指令之前放置谓词就可以提供条件分支。编译
    器或PTX程序员将虚拟寄存器声明为32位或64位有类型或无类型值。例如，RO，RI...用于32
    位值，RDO,RD1...用于64位寄存器。回想一下，将虚拟寄存器指定给物理寄存器的过程是在载
    人时由PTX 进行的。
    分
    算术
    组
    指
    令
    示
    例
    arithmetic .type - .s32..u32，.f32..s64..u64，.f64
    add.type
    add.f32 d. a. b
    sub.type
    sub.f32 d,a, b
    mul.type
    mul.f32 d, a, b
    mad.type
    mad.f32 d. a. b,c
    div.type
    div.f32 d. a.b
    rem.type
    rem.L32 d. a. b
    abs.type
    abs.f32 d, a
    表4-7 基本PTX GPU线程指令
    含义
    d =a+ b；
    d= a-b：
    d=a*b：
    d=a *b+c：
    d=a / b：
    d-a%b：
    d- lals
    注
    释
    乘加
    多条微指今
    整余数
    分
    算术
    组
    特殊函数
    逻辑
    存储锵
    访问
    控制流
    ，
    4.4 图形处理器
    223
    （续）
    指
    令
    示
    例
    含
    义
    注释
    neg.type
    neg.f32 d. a
    d=0- a：
    min.type
    min.f32 d. a.D
    d - （a ≤ b）？a:b：
    浮点选择非NaN
    max.type
    max. f32 d. a, b
    d - （a >b）？ a:b：
    浮点选择非NaN
    setp.cmp.type
    setp.1t.f32 p. a. b
    P- （a<D；
    比较和设定调词
    numeric.anp - eq, ne, it. 1e.gt. ge: unordered cnp = equ. neu, 1tu,leu, gtu, geu, nun, nan
    mov.type
    mov.b32 d.a
    d - a：
    移动
    selp.type
    selp.f32 d, a, b,p
    d - p?a:b：
    用训诩选择
    cvt.dtype.atype
    cvt.f32.$32 d,a
    d - convert（a）：
    将atype转换为dtype
    special .type -.f32 （soe .f64）
    rcp.type
    rcp.f32 d. a
    d = 1/a：
    倒数
    sqrt.type
    sqrt.f32 d.a
    d = sqrt（a）；
    平方根
    rsqrt.type
    rsqrt. f32 d, a
    d - 1/sqrt（a）；
    平方根的倒数
    sin.type
    sin.f32 d, a
    d - sin（a）；
    正弦
    cos.type
    cos.f32 d. a
    d - cos（a）；
    余弦
    1g2.type
    1g2.f32 d,a
    d - 1og（a）/1og（2）
    二进制对数
    ex2.type
    Ex2.f32 d. a
    d-2*a；
    二进制指数
    logic.type -.pred..b32..b64
    and.type
    and.b32 d. a.b
    d= a & b；
    or.type
    or.b32 d.a.b
    d - al b；
    xor.type
    xor.b32 d, a.b
    d= a~b：
    not.type
    not.b32 d. a,b
    d- -a；
    1的补数
    cnot.type
    cnot.b32 d, a. b
    d = （a-0）？ 1:0；
    C逻辑非
    shl.type
    shl.b32 d.a,b
    d= a cr b：
    左移位
    shr.type
    shr.$32 d. a,b
    d= a ≥b：
    右移位
    menory.space -.global，.shared，.local，
    .const：.type = .b8，.u8，.s8，.b16．.b32．.b64
    1d.space.type
    1a.g1oba1.b82 d. ［atoffj
    d - *（a+off）：
    从存储器空间载入
    st.space.type
    st.shared. b32 ［dtoff］，a
    *（dHoff）=a：
    存储到存储器空间
    tex.nd.dt.yp.btype
    tex.2d.¥4.f32.f32 d. a, b
    d - texzd（a.b）；
    纹理查询
    atom.spc.op.type
    at.on.global.add.u32 d，［a］，b
    atonic （ d- *a： *a-
    原子读改写操作
    atcm.global.cas.b32 d.［a］， b，
    Op（*a,b）：｝
    atom.op - and, or，
    xor， add， min， max，
    exch，
    cas：.spc-.global；
    .type =.D32
    branch
    @p bra target
    if （p） goto target：
    条件分支
    call
    call （ret）， func， （params）
    ret = func（params）；
    调用函数
    ret
    ret
    return；
    从函数谰用返回
    bar.sync.
    bar.syncd
    wait for threads
    屏障同步
    exit
    exit
    exit：
    終止线程执行
\end{verbatim}

下面的PTX指令序列是4.4.1节 DAXPY循环一次迭代的指令：
sh1.u32 R8, blockIdx,9
；线程块 ID *块大小（512或29）
add.u32 R8, R8, threadIdx ；R8 = i = 我的CUDA 线程 ID
shl.u32 R8, R8, 3
1d.global.f64 RDO，［X+R8］；RDO - X［1］
1d.global.f64 RD2，［Y+R8］
： RD2 = Y［1］
mu1.f64 RDO，
：在RDO 中求乘积RDO = RDO *RD4（标量a）
；在RDO 中求和 RDO = RDO+RD2（Y［i］）
st.global.f64 ［Y+R8］，RDO ; YCi］ = sum （XEi］*a + Y［i］）
如上所述，CUDA编程模型为每个循环迭代指定一个 CUDA，为每个线程块指定一个唯一的识
别编号（blockidx），也为块中的每个 CUDA线指定一个唯一识别编号（threadldx）。因此，
它创建8192个CUDA线程，并使用唯一编号完成数组中每个元素的寻址，因此，不存在递增
和分支编码。前3条PTX指令在R8 中计算出唯一的元素字节偏移，会将这一偏移量加到数组
的基地址中。以下 PTX指令载人两个双精度浮点操作数，对其进行相乘和相加，并存储求和结
果。（下面将描述与 CUDA代码"if（i < n）"相对应的PTX代码。）

注意，GPU与向量体系结构不同，它们没有分别用于顺序数据传送、步幅数据传送和集中-
分散数据传送的指令。所有数据传送都是集中-分散的！为了重新获得顺序（单位步幅）数据传
送的效率，GPU包含了特殊的“地址接合”硬件，用于判断 SIMID指令线程中的SIMID车道什
么时候一同发出顺序地址。运行时硬件随后通知存储器接口单元来请求发送32个顺序字的分块
传送。为了实现这一重要的性能改进，GPU程序员必须确保相邻的 CUDA线程同时访问可以接
合为一个或一些存储器或缓存块的相邻地址，我们的示例就是这样做的。

\subsection{GPU中的条件分支}

和单位步幅数据传送的情况一样，向量体系结构和 GPU在处理吓语句方面非常相似，前
者主要以软件实现这一机制，硬件支持非常有限，而后者则利用了更多的硬件。后面将会看到，
除了显式谓词寄存器之外，GPU分支硬件使用了内部遮罩、分支同步栈和指令标记来控制分支
何时分为多个执行路径，这些路径何时会汇合。

在PTX汇编程序级别，一个 CUDA线程的控制流是由PTX指令分支、调用、返回和退出
描述的，另外还要加上每条指令的各个按线程车道给出的谓词来描述，这些谓词由程序员用每
个线程车道的1位谓词寄存器指定。PTX 汇编程序分析了 PTX分支图，对其进行优化，实现最
快速的 GPU硬件指令序列。

在GPU硬件指令级别，控制流包括分支、跳转、索引跳转、调用、索引调用、返回、退出
和管理分支同步桉的特殊指令。GPU硬件提供了每个拥有自己栈的SIMID线程；一个堆栈项包
含一个标识符标记、一个目标指令地址和一个目标线程活动遮罩。有一些 GPU特殊指令为 SIMID
项目压入栈项，还有一些特殊指令和指令标记用于弹出栈项或者将栈展开为特殊项，并跳转到
具有目标线程活动遮罩的目标指令地址。GPU硬件指令还拥有一些为不同车道设置的不同谓词
（启用/禁用），这些谓词是利用每个车道的1位谓词寄存器指定的。

PTX汇编程序通常会将用PTX分支指令编码的简单外层IF/THEN/ELSE语句优化为设有谓
词的GPU指令，不来用任何GPU分支指令。更复杂控制流的优化通常会混合采用谓词与GPU
分支指令，这些分支指令带有一些特指令和标记，当某些车道跳转到目标地址时，这些GPU
分支指令会使用分支同步栈压人一个栈项，而其他各项将会失败。在这种情况下，NVIDIA 称
为发生了分支分岔。当 SIMID车道执行同步标记或汇合时，也会使用这种混合方式，它会弹出
一个栈项，并跳转到具有栈项线程活动遮罩的栈项地址。

PTX 汇编程序识别出循环分支，并生成GPU分支指令，跳转到循环的顶部，用特殊栈指令
来处理各个跳出循环的车道，并在所有车道完成循环之后，使这些 SIMD车道汇合。GPU 索引
跳转和索引调用指令向栈中压入项目，以便在所有车道完成开关语句或函数调用时，SIMD线
程汇合。

GPU 设定谓词指令（表4-7中的 setp）对IF 语句的条件部分求值。PTX分支指令随后将根
据该谓词来执行。如果PTX汇编程序生成了没有GPU 分支指令的有调词指令，它会使用各个
车道的谓词寄存器来启用或禁用每条指令的每个SIMDD车道。正语句 THEN部分线程中的SIMD
指令向所有SID车道广播操作。谓词被设置为1的车道将执行操作并存储结果，其他SIMID
车道不会执行操作和存储结果。对于 ELSE 语句，指令使用谓词的补数（与THEN 语句相对），
所以原来空闲的SIMID 车道现在执行操作，并存储结果，而它们前面的对应车道则不会执行相
关操作。在ELSE 语句的结尾，会取消这些指令的谓词，以便原始计算能够继续进行。因此，
对于相同长度的路径，IF-THEN-ELSE 的工作效率为50%。

IF语句可以嵌套，因而栈的使用也可以嵌套，所以PTX汇编程序通常会混合使用设有谓词
的指令和 GPU分支与特殊分支指令，用于复杂控制流。注意，尝试嵌套可能意味着大多数 SIMD
车道在执行嵌套条件语句期间是空闲的。因此，等长路径的双重嵌套IF语句的执行效率为25%，
三重嵌套为12.5%，以此类推。与此类似的情景是仅有少数几个遮罩位为1时向量处理器的运
行情况。

具体来说，PTX 汇编程序在每个 SIMD线程中的适当条件分支指令上设置“分支同步”标
记，这个标记会在栈中压入当前活动遮罩。如果条件分支分岔（有些车道进行眺转，有些失败），
它会压人栈项，并根据条件设置当前内容活动遮罩。分支同步标记弹出分岔的分支项，并在 EL,SE
部分之前翻转遮單位。在IF 语句的末尾，PTX汇编程序添加了另一个分支同步标记，它会将先
前的活动遮罩从栈中弹出，放入当前的活动遮罩中。
如果所有遮罩位都被设置为1，那么THEN结束的分支指令将略过ELSE 部分的指令。当
所有遮罩位都为零时，对于 THEN部分也有类似优化，条件分支将跳过THEN指令。并行的IF
语句和PTX分支经常使用没有异议的分支条件（所有车道都同意遵循同一路径），所以 SIMID
指令不会分岔到各个不同的车道控制流。PTX汇编程序对此类分支进行了优化，跳过SIMD线
程中所有车道都不会执行的指令块。这种优化在错误条件检查时是有用的，在这种情况下，必
须进行测试，但很少会被选中。
以下是一个类似于4.2节的条件语句，其代码为：
if （X［i］ 1= 0）
X［i］- XCi］ - YCi］；
else X［i］ = ZLi］；
这个正语句可以编译为以下 PTX指令（假定R8已经拥有经过调整的线程 JD），*Push、*Comp、*Pop
表示由 PTX汇编程序插人的分支同步标记，用于压入旧遮罩、对当前遮罩求补，弹出恢复旧遮罩：
\begin{verbatim}
    1d.global.f64 RDO，［X+R8］
    ；RDO = XC1］
    setp.neq.s32 P1, RDO， #0
    ；P1 是谓词寄存器1
    @1P1,bra ELSEl，*Push
    ；压入旧迷罩，设定新迹革位
    ：ifP1 为假，则转至 ELSE1
    1d.g1obal.f64 RD2，［Y+R8］
    ；RD2 = YIi］
    sub.f64 RDO, RDO，
    RD2
    ；RDD 中的差
    st.global.f64 ［X+R8］，RDO
    ；XCi］ = RDO
    @P1,bra ENDIF1，*Comp
    ；对遮罩位求补
    ；if P1 为真，則转至 ENDIF1
    ELSE1: 1d.global.f64 RDO，［Z+R8］
    ： RDO = Z［1］
    st.global.f64 ［X+R8］，RDO
    ；XC1］ = RDO
    ENDIF1:cpext tnstruction，*Pop
    ；弹出以恢复旧迹罩
\end{verbatim}
同样，IF-THEN-ELSE 语句中的所有指令通常都是由SIMID 处理器执行。一些SIMD车道是为
THEN 语句启用的，另一些车道是为 ELSE指令启用的。前面管经提到，在非常常见的情况中，
各个车道都-致选择设定调词的分支，比如，根据参数值选择分支，而所有车道的这个参数值
都相同，所有活动遮罩位或者都为0，或者都为1，因此，分支会跳过 THEN 指令或 ELSE指令。
这一灵活性清楚地表明元素有其自己的程序计数器，但是，在最缓慢的情况下，只有一个
SIMID 车道可以每两个时钟周期存储其结果，其余车道则会闲置。在向量体系结构中有种与之
类似的最缓慢情景，那就是仅有一个遮罩位被设置为1时进行操作的情况。这一灵活性可能会
导致GPU编程新手无法获得较佳性能，但在早期编程开发阶段可能是有帮助的。但要记住，在
一个时钟周期内，SIMID 车道的唯一选择就是执行在PTX 指令中指定的操作或者处于空闲状态；
两个 SIMID 车道不能同时执行不同指令。

这一灵活性还有助于解释为SIMID 指令线程中每个元素指定的名称—CUDA 线程，它会
给人以独立运行的错觉。编程新手可能会认为这一线程抽象意味着 GPU能够更出色地处理条件
分支。一些线程会沿一条路径执行，其他线程则会沿另一路径执行，只要你不着急，那似乎就
是如此。每个 CUDA线程要么与线程块中的所有其他线程执行相同指令，要么就处于空闲状态。
利用这一同步可以较轻松地处理带有条件分支的循环，这是因为遮罩功能可以关闭 SIMD 车道，
自动检测循环的结束点。

最终得到的性能结果可能会与这种简单的抽象不相符。如果编写一些程序，以这种高度独
立的 MIMD模式来操作 SIMID车道，就好像是编写了一些程序，在一个物理存储器很小的计算
机上使用大量虚拟地址空间。这两种程序都是正确的，但它们的运行速度可能非常慢，程序员
可能会对结果感到不快。

向量编译器可以用遮罩寄存器做到GPU用硬件完成的小技巧，但可能需要使用标量指令来
保存、求补和恢复遮罩寄存器。条件执行就是这样一个例子：GPU在运行时用硬件完成向量体
系结构在编译时完成的工作。有一种优化方法，可以在运行时针对GPU应用，但不能在编译时
对向量体系结构应用，那就是在遮罩位全0或全1时略过THEN 或EL.SE 部分。

因此，GPU执行条件分支的效率决定了分支的分岔频率。例如，某个特征值计算具有深度
条件嵌套，但通过代码测试表明，大约82%的时钟周期发射将32个遮罩位中的29至32位设置
为1，所以GPU执行这一代码的效率可能要超出人们的预期。

注意，同一机制处理向量循环的条带挖掘——当元素数与硬件不完全匹配时。本节开始的
例子表明，用一个IF 语句检查 SIMD 车道元素数（在上例中，该数目存储在R8 中）是否小于
限值（isn），并适当设置遮罩。

\subsection{NVIDIA GPU存储器结构}

图4-12给出了 NVIDIA GPU 的存储器结构。多线程SIMID处理器中的每个 SIMID车道获得
片外 DRAM的一个专用部分，称之为专用存储器，用于栈帧、溢出寄存器和不能放在寄存器中
的私有变量。SIMID 车道不共享专用存储器。最近的GPU将这一专用存储器缓存在L1 和L.2缓
存中，用于辅助寄存器溢出并加速函数调用。

4.4
图形处理器
227
CUDA线程
专用存储銎
序列

图 4-12 GPU 存储锅结构。GPU 存储器由所有网格（向量化循环）共享，本地存储器由线程块（向量
化循环体）中的所有 SID指令线程共享，专用存储器由单个 CUDA线程专用

我们将每个多线程SIMID处理器本地的片上存储器称为本地存储器。这一存储器由多线程
SIMD 处理器内的 SIMID 车道共享，但这一存储器不会在多线程 SIMD处理器之间共享。多线程
SIMID 处理器在创建线程块时，将部分本地存储器动态分配给此线程块，当线程块中的所有线
程都退出时，释放此存储器。这一本地存储器部分由该线程块专用。

最后，我们将由整个 GPU和所有线程块共享的片外 DRAM称为GPU 存储語。这里的向量
乘法示例仅使用GPU存储器。

被称为主机的系统处理器可以读取或写入GPU存储器。本地存储器不能供主机使用，它是
每个多线程 SIMID 专用的。专用存储器也不可供主机使用。

GPU 通常不是依赖大型缓存来包含应用程序的整个工作集，而是使用较少的流式缓存，依
靠大量的SIMD指令多线程来隐藏 DRAM的较长延迟，其主要原因是它们的工作集可能达到数
百MB。在利用多线程隐藏 DRAM延迟的情况下，系统处理器中供缓存使用的芯片面积可以用
于计算资源和大量的寄存器，以保存许多SIMD指令线程的状态。如前文所述，向量的载人和
存储与之相对，是将这些延迟分散在许多元素之间，因为它只需要有一次延迟，随后即可实现
其余访问的流水化。

尽管隐藏存储器延迟是一种优选方法，但要注意，最新的GPU和向量处理器都已经添加了
缓存。例如，最近的Fermi体系结构已经添加了缓存，但它们要么被看作带宽滤选器，以减少
对GPU 存储器的要求，要么被看作有限几种变量的加速器，这些变量的修改不能通过多线程来
隐藏。因此，用于栈帧、函数调用和寄存器溢出的本地存储器与缓存是绝配，这是因为延迟对
于函数调用是有影响的。由于片上缓存访问所需要的能量要远远小于对多个外部 DRAM芯片的
访问，所以使用缓存还可以节省能量。

为了提高存储器带宽、降低开销，如上所述，当地址属于相同块时，PTX数据传送指令
会将来自同一 SIMD线程的各个并行线程请求接合在一起，变成单个存储器块请求。对GPU
程序设置的这些限制，多少类似于系统处理器程序在硬件预取方面的一些准则（见第2章）。
GPU 存储器控制器还会保留请求，将一些请求一同发送给同一个打开的页面，以提高存储器
带宽（见4.6节）。第2章非常详细地介绍了 DRAM，可帮你理解对相关地址进行分组所带来
的潜在好处。

\subsection{Fermi GPU体系结构中的创新}
Fermi 多线程SIMID处理器要比图4-13中的简化版本更复杂一些。为了提高硬件利用率，
每个 SIMD处理器有两个 SIMID线程调度程序和两个指令分派单元。双重 SIMDD线程调度程序
选择两个 SIMID指令线程，并将来自每个线程的一条指令发射给由16个 SIMID 车道、16个载入
/存储单元或4个特功能单元组成的集合。因此，每两个时钟周期将两个 SIMD指令线程调度
至这些集合中的任何一个。由于这些线程是独立的，所以不需要检查指令流中的数据相关性。
这一创新类似于多线程向量处理器，它可以发射来自两个独立线程的向量指令。
图4-13展示了发射指令的双重调度程序，图4-14展示了 Fermi GPU的多线程 SIMID处理
器的框图。
SIMI2线稚调疫程
撮令"分派乎还彬
SIM）线程指分
SIMD线程期度租
拚父分派婵龙
SENII） 线程：怡父33
SINl线程
12
Sl I 线税14指分9の
SiNil）线提少指父！
SIMi线程：指令江4
SlN线程街+3
图 4-13 Fermi 双 SIMD线程调度程序的框图。将这一设计与图4-11 中的单 SIMD线程设计进行对比
Fermi引人了几种创新，使GPU与主流系统处理器的接近程度远远超过 Tesla 和前几代GPU
体系结构。
口 快速双精度浮点运算-
一Fermi对比发现，传统处理器的相对双精度速度大约为单精度速
度的一半，是先前Tesla 代处理器单精度速度的十分之一。也就是说，当准确性需要双
精度时，使用单精度在速度方面没有太大的诱惑力。在使用乘加指令时，峰值双精度性
能从过去 GPU的78 GFLOP/s增长到515 GFLOP/S。

FP 单元
INT单元
图 4-14
Fermi流式多处理器（SM）
Fermi GPU 多线程 SIMD处理暴的框图。每个 SIMD 车道有一个流水线浮点单元、一个流水线
整数单元、还有某些逻辑，用于将指令和操作数分发给这些单元，以及一个队列用于保存结果。
4个特殊函数单元（SFU）计算诸如平方根、求倒数、正弦和余弦等函数
口 GPU 存储器—尽管GPU的基本思想是使用足够多的线程来隐藏DRAM延迟，但仍然
需要在线程之间使用一些变量，比如前面提到的局部变量。Fermi在 GPU中为每个多线
程 SIMD处理器包含了L1 数据缓存和L1 指令缓存，还包含了由所有多线程 SIMID处理
器共享的单个768 KBL.2缓存。如上所述，除了降低对GPU 存储器的带宽压力之外，缓
存因为驻留在芯片上，不用连到片外 DRAM，所以还能节省能量。L.I缓存实际上与本
地存储器使用同一SRAM。Fermi有一个模式位，为用户提供了两种使用64KB SRAM
的选择：两种 16KBL1缓存和48KB 本地存储器，另一种是48KBL1缓存和16KB本
地存储器。注意，GTX480 有一个倒转的存储器层次结构：聚合寄存器堆的大小为2 MB，
所有L1 数据缓存的大小介于0.25与0.75 MB 之间（取决于它们是16KB，还是48 KB），
L.2缓存的大小为0.75 MB。了解这一反转比值对GPU应用程序的影响是有意义的。

口 全部GPU 存储器的64位寻址和统一地址空间——利用这一创新可以非常轻松地提供C
和C++所需要的指针。
口纠错码检测和纠正存储器与寄存器中的错误（见第2章），为了提高数千个服务器上长期
运行的应用程序的可靠性，ECC是数据中心的一种标准配置（见第6章）。
口更快速的上下文切换—由于多线程 SIMID 处理器拥有大量状态，所以 Fermi 以硬件支
持大幅加速上下文的切换速度。Fermi 可以在不到25 微秒内完成切换，比之前的处理器
大约快10倍。
口 更快速的原子指令一一这一特征最早包含在 Telsa 体系结构中，Fermi 将原子指令的性能
提高了5~20倍，达到几微秒的级别。有一个与L2缓存相关的特殊硬件单元（不是在多
线程 SIMID处理器内部）用来处理原子指令。

\subsection{向量体系结构与GPU的相似与不同}
我们已经看到，向量体系结构与GPU之间确实有许多相似之处。这些相似之处和GPU那些
怪异的术语一样，也让体系结构圈的人们难以真正了解新奇的GPU本质。既然我们现在已经了
解了问量体系结构和GPU的一些内幕，那就可以体味一下它们的相似与不同了。这两种体系结
构都是为了执行数据级并行程序而设计的，但它们选取了不同的路径，对比它们是希望更深入地
了解 DLP硬件到底需要什么。表4-8首先给出向量术语，然后给出GPU 中最接近的对等术语。

表4-8
向量术语的对等GPU术语
最接近的
CUDA/NVIDIA
GPU术语
网格
类型
编程
抽象
机干
对象
向量术语
向量化循环
钟鸣
向量指令
集中/分散
遮單寄存器
向量处理器
PTX指今
全局载人/存储
（Id.global/st.global）
谓词寄存器和内部
遮軍寄存罸
多线程SIMDD处理器
处理
与存
储琵
硬件
控制处理器
线程块调度程序
标量处理器
系絖处理器
向量车道
向量寄存器
SIMD车道
SIMID车道寄存器
主存储器
GPU存储器
注释
概念相似，GPU使用了描述性较差的术语

由于一条向最指令（PTX指令）的完成在Fermi上只需要2个时钟周期，
在Tesla上只需要4个时钟問期，所以钟鸣在GPU中很短
SINMD线程的PTX指令会广播到所有SIMD车道，所以它与向量指令类似
所有GPU载入与存储都是集中和分散，因为每个SIMD车道会发送一个
唯一地址。如果自SIMD车道的地址允许，将由GPU接合单元来实现
单位步幅性能

向量逮單寄存器是体系结构状态的组成部分，而GPU遮罩寄存器位于硬件
内部。GPU条件硬件添加了超越谓词寄存器的新特征，可以动态管理遮罩
这些概念是类似的，但SIMD处理器倾向于拥有许多车道，每个车道只需
要几个时钟周期完成一个向量，而向量体系结构的车道较少，需要许多时
钟周期才能完成一个向量。SIMDD还实现了多线程，而向量通常不会
最接近的是线程块调度程序，它将线程块指定给多线程SIMD处理器。
但GPU没有标量-向量运算，没有单位步幅或步幅化数据传送指令，而
控制处理器经常会提供上述指令

由于缺少共享存储器，而且通过PCI总线进行通信的延退较高（时钟周
期时间达1000秒），所以GPU中的系统处理器很少执行标量处理器在向
量体系结构中执行的相同任务
这两者基本上都是带有寄存器的功能单元
向量寄存器的对等术语就是多线程SIMD处理器中所有32个SIMD车道的相
同寄存器（这个多线程SIMID处理器在运行SIMID指令线程）。每个SIMID线
程的寄存器数目是灵活的，但最大为64，所以向量寄存器的最大数目均64
GPU存储器对应向量处理器中的系統存储器

SIMD处理器与向量处理器类似。GPU 中的多个 SIMD处理器像独立 MIIMID 核心一样操作，
就好像是许多向量计算机拥有多个向量处理器。这种观点将 NVIDIA GTX 480 看作一个具有多
线程硬件支持的15核心机器，其中每个核心有16个车道。两者之间最大的区别是多线程，它
是GPU的基本必备技术，而大多数向量处理器则没有采用。

看一下这两种体系结构中的寄存器，VMIPS 寄存器堆拥有整个向量，也就是说，由64
个双精度值构成的连续块。相反，GPU 中的单个向量会分散在所有 SIMD 车道的寄存器中。
VMIPS处理器有8个向量寄存器，各有64个元素，总共512个元素。一个GPU 的SIMD指
令线程拥有多达64个寄存器，各有32个元素，总共2048个元素。这些额外的GPU寄存器
支持多线程。

图4-15的左边是向量处理器执行单元的框图，右侧是 GPU的多线程SIMD处理器。为便
于讲解，假定向量处理器有4个车道，多线程SIMD处理器也有4个 SIMD车道。此图表明，
4个SIMID 车道的工作方式非常像4车道向量单元，SIMD处理器的工作方式与向量处理器非
常类似。

指令缓存
PC
PC
PC
PC
指令缓存
指今寄存器
SIMD线程调度程序
分派单元
指今寄存器
控制
处理器

60
61
62
向量载人/存储单元
63
1023
1023
1023
五
SIMDD载入/存储单元
1023
五
地址接合单元
存储器接口单元
存储器接口单元
图4-15

左侧为具有 4 个车道的向量处理器，右侧为 GPU 的多线程 SIMD处理器。（GPU通常有8~16
个 SIMD 车道。）控制处理器为标量-向量运算提供标量操作数，为对存储器进行单位步幅或非单
位步幅访问而递增地址，执行其他“记账类型”（accounting-type）的运算。只有当地址接合单元
可以发现本地寻址时，才会在GPU中实现峰值存储器性能。与此类似，当所有内部遮罩位被设
置为相同时，会实现峰值计算性能。注意，SIMID 处理器中每个 SIMD线程有一个 PC，以帮助
实现多线程

实际上，GPU 中的车道要多很多，所以GPU“钟鸣”更短一些。尽管向量处理器可能拥有
2~8个车道，向量长度例如为32（因此，钟鸣为 4~16个时钟周期），多线程SIMD处理器可
能拥有8~16个车道。SIMD线程的宽度为32个元素，所以GPU钟鸣仅为2或4个时钟周期。
这一差别就是为什么要使用“SIMD 处理器”作为更具描述性术语的原因，这一术语更接近于
SIMD设计，而不是传统的向量处理器设计。

与向量化循环最接近的GPU术语是网格，PTX 指令与向量指令最接近，这是因为SIND线
程向所有 SIMID 车道广播 PTX指令。

关于两种体系结构中的存储器访问指令，所有GPU载人都是集中指令，所有GPU 存储都
是分散指令。如果CUDA线程的地址引用同一缓存/存储器块的邻近地址，那GPU 的地址接合
单元将会确保较高的存储器带宽。向量体系结构采用显式单位步幅载人并存储指令，而GPU编
程则采用隐式单位步幅，这两者的对比说明为什么在编写高效GPU代码时，需要程序员从SIMD
运算的角度来思考，尽管 CUDA编程模型与MIMTD 看起来非常类似。由于 CUDA线程可以生
成自己的地址、步幅以及集中-分散，所有在向量体系结构和 GPU中都可以找到寻址向量。
我们已经多次提到，这两种体系结构采用了非常不同的方法来隐藏存储器延迟。向量体系
结构通过深度流水化访间让向量的所有元素分担这一延迟，所以每次向量载人或存储只需要付
出一次延迟代价。因此，向量载人和存储类似于在存储器和向量寄存器之间进行的块传送。与
之相对的是，GPU 使用多线程隐藏存储器延迟。（一些研究人员正在研究为向量体系结构添加
多线程，以实现这两者的最佳性能。）

关于条件分支指令，两种体系结构都使用遮罩寄存器来实现。两个条件分支路径即使在未
存储结果时也会占用时间以及（或者）空间。区别在于，向量编译器以软件显式管理遮罩寄存
器，而GPU硬件和汇编序则使用分支同步标记来隐式管理它们，使用内部栈来保存、求补和
恢复遮罩。

前面曾经提到，GPU 的条件分支机制很好地处理了向量体系结构的条带挖掘问题。如果向
量长度在编译时未知，那么程序必须计算应用程序向量长度的模和最大向量长度，并将它存储
在向量长度寄存器中。条带挖掘循环随后将向量长度寄存器重设剩余循环部分的最大向量长
度。这种情况用GPU处理起来要更容易一些，因为它们将会一直迭代循环，直到所有SIMD
车道到达循环范围为止。在最后一次迭代中，一些SIMD 车道将被遮罩屏蔽，然后在循环完成
后恢复。

向量计算机的控制处理器在向量指令的执行过程中扮演着重要角色。它向所有向量车道广
播操作，并广播用于向量-标量运算的标量寄存器值。它还执行一些在 GPU 中显式执行的隐式
计算，比如自动为单位步幅和非单位幅载人、存储指令递增存储器地址。GPU 中没有控制处理
器。最类似的是线程块调度程序，它将线程块（向量循环体）指定给多线程 SIMD处理器。GPU
中的运行时硬件机制一方面生成地址，另一方面还会查看它们是否相邻，这在许多DLP 应用程
序中都是很常见的，其功耗效率可能要低于控制处理器。

向量计算机中的标量处理器执行向量程序的标量指令。也就是说，它执行那些在向量单元
中可能速度过慢的运算。尽管与GPU相关联的系统处理器与向量体系结构中的标量处理器最为
相似，但独立的地址空间再加上通过PCle总线传送，往往会耗费数千个时钟周期的开销。对于
在向量计算机中执行的浮点计算，标量处理器可能要比向量处理器慢一些，但它们的速度比值
不会达到系统处理器与多线程SIMID 处理器的比值（在给定开销的前提下）。

因此，GPU 中的每个“向量单元”必须执行本来指望在向量计算机标量处理器上进行的计
算。也就是说，如果不是在系统处理器上进行计算然后再发送结果，而是使用谓词寄存器和内
置遮罩禁用其他 SIMID 车道，仅留下其中一个 SIMD车道，并用它来完成标量操作，那可以更
快一些。向量计算机中比较简单的标量处理器可能要比GPU解决方案更快一些、功耗效率更高
一些。如果系统处理器和 GPU将来更紧密地结合在一起，那了解一下系统处理器能否扮演标量
处理器在向量及多媒体 SINID体系结构中的角色，那将是很有意义的。

\subsection{多媒体SIMD计算机与GPU之间的相似与不同}
从较高级别的角度来看，具有多媒体 SIMID 指令扩展的多核计算机的确与 GPU有一些相似
之处。表4-9总结了它们之间的相似与不同。
表4-9 具有多媒体SIMD扩展的多核心与最新GPU之间的相似与不同
组件
具有SIMD的多核
SIMD处理器
4~8
SIMD车道/处理器
2~4
对SIMD线程的多线程硬件支持
2~4
单、双精度性能的典型比值
2:1
最大缓存
8MB
存储器地址的大小
64位
主存储器的大小
8-256 GB
页面级别的存储器保护
是
需求分页
是
集成标量处理器/SIMD处理器
是
缓存一致性
是
GPU
8~16
8~16
16~32
2:i
0.75MB
64位
4-6GB
是
否
否
否
这两种多处理器的处理器都使用多个 SIMD车道，只不过GPU的处理器更多一些，车道数
要多很多。它们都使用硬件多线程来提高处理器利用率，不过GPU 为大幅增加线程数目提供了
硬件支持。由于 GPU 中最近的一些创新，现在这两者的单、双精度浮点运算性能比相当。它们
都使用缓存，不过GPU使用的流式缓存要小一些，多核计算机使用大型多级缓存，以尝试完全
包含整个工作集。它们都使用64位地址空间，不过GPU中的物理主存储器要小得多。尽管 GPU
支持页面级别的存储器保护，但它们都不支持需求分页。

除了在处理器、SIMID 车道、硬件线程支持和缓存大小等大量的数字差异之外，还有许多
体系结构方面的区别。在传统计算机中，标量处理器和多媒体SIMID指令紧密集成在一起；它
们由GPU中的1/O总线隔离，它们甚至还有独立的主存储器。GPU 中的多个 SIMD处理器使用
单一地址空间，但这些缓存不是像传统的多核计算机那样是一致的。多媒体SIMD指令与 GPU
不同，它不支持集中-分散存储器访问，4.7节表明，这是一个非常重要的简化。

\subsection{小结}
现在，GPU的神秘面纱已经揭开，可以看出 GPU实际上就是多线程SIMID处理器，只不过
与传统的多核计算机相比，它们的处理器更多、每个处理器的车道更多，多线程硬件更多。例
如，Fermi GTX 480拥有15个 SIMD处理器，每个处理器有16个车道，为32个 SIMID线程提
供硬件支持。Fermi甚至包括指令级并行，可以从两个SIMD线程向两个 SIMD车道集合发射指
令。另外，它们的缓存存储器较少—Fermi的L2缓存为0.75 MB，而且与标量处理器不一致。
CUDA 编程模型将所有这些形式的并行包含在一种抽象中，即 CUDA线程中。因此，CUDA
程序员可以看作是在对数千个线程进行编租，而实际上他们是在许多SIMD处理器的许多车道
上执行各个由32个线程组成的块。希望获得良好性能的CUDA 程序员一定要记住，这些线程
是分块的，一次执行32个，而且为了从存储器系统获得良好性能，其地址需要是相邻的。
尽管本节使用了 CUDA 和 NVLDIA GPU，但我们确信在OpenCL编程语言和其他公司的
GPU 中也采用了相同思想。

在读者已经很好地理解了GPU的工作原理之后，现在可以揭示真正的术语了。表4-10和
4-11 将本节的描述性术语及定义与官方 CUDA/NVIDIA 和 AMD术语及定义对应起来，而且还
给出了 OpenCL 术语。我们相信，GPU学习曲线非常陡峭，一部分原因就是因为使用了如下术
语：用“流式多处理器” 表示 SIMD 处理器，“线程处理器” 表示 SIMID车道，“共享存储器”
表示本地存储器，而本地存储器实际上并非在 SIMD处理器之间共享！我们希望，这种“两步
走”方法可以帮助读者更快速地沿学习曲线上升，尽管这种方法有些不够直接。

表4-10 由本章使用的术语转换为官方NVIDIA/CUDA和AMD术语
类型
本书使用的描述
性更强的名称
可向量化循环
官方CUDA/
NVIDIA术语
书中定义及AMD和OpenCL术语
官方CUDA/NVIDIA定义
网格
一种可向量化循环，在GPU上执行，由一个
网格是一组可以同时、顺
或多个可以并行执行的“线程块”
（或向量
序或混合执行的线程块
化循环体）组成。OpenCL.名称为“索引范围”，
AMD名称為“NDRange”
向量化循环体
线程块
在多线程SIMID处理器上执行的向量化循环，
由一个或多个SIMD指令线程组成。这些
SIMD线程可以通过本地存储器通信。AMID
和OpenCL称片“工作组”
编程
抽象
SIMD车道运算
序列
CUDA线程
SIMD指令线程的垂直抽取，与SIMD车道上
执行的一个元素相对应。根据遮軍位来存储
SIMD指令线程
Warp
机骽
对歙
SIMD指令
* OpenCL. 名称在第4列给出。
PTX指令
一种传统线程，但它仅包含在多线程SIMD
处理器上执行的SIMD指令。根据每个元素的
遮罩位来存储结果。AMID称“波前”
在SIMID车道之间执行的单条SIMD指令。
AMD名称为 “AMDIL” 或 “FSAIL”指令
线程块是一组CUDA线
程，这些线程可以一起同
时执行，并通过共享存储
器和屏障同步进行协调与
通信。线程块在其网格内
有一个线程块ID
CUDA线程是一种轻量级
线程，可以执行顺序程序，
可以与同一线程块中执行
的其他CUDA线合作
CUDA线程在其线程块内
有一个线程ID
Warp是一组并行CUDA线
程（比如32个），可以在
多线程SIMT/SIMD处理
器中一起执行相同指令
PTX指令指定了由CUDA
线程执行的指令

235
类型
本书使用的描述
性更强的名称
多线程SIMD
处理器
表4-11 由本章使用的术语转换为官方 NVIDIA/CUDA 和 AMD术语
官方CUDA/
NVIDIA术语
流式多
处理器
书本定义及AMD和OpenCL术语
多线程SIMD处理器，独立于其他SIMD处
理器执行SIMDD指令线程。AMD和OpenCL
都将其称为“计算单元”。但是，CUDA
程序员是为一个车遊编程，而不是为多个
SIMD车道的“向量”编写
官方CUDA/NVIDIA定义
流式多处理器（SM）是一
种多线程SIMT/SIMD处理
器，它执行CUDA线程的
wap。SIMD程序指定一个
CUDA线程的执行，而不是
多个SIMD车道的向量
线程块调度程序
Giga线程
引擎
将多个向量化循环体分派给多线程SIMD
处理器，AMD称为“超线程分派引擎”
当资源可用时，将网格的线
程块分派、调度至流式多处
处理
硬件
理器
SIMD线程调
程序
Warp调度
程序
SIMD车道
线秷处理醬
GPU存储器
金局存储器
硬件单元，当SIMD指令线程做好执行准备
后调度和发射这些SIMD指令线程，包括跟
踪SIMD线程执行的记分板。AMID称为“工
作组调度程序”
硬件SIMID车道，执行SIMDD指今线程中对
单一元素进行的操作。根据遮罩存储结果。
OpenCL将其称为“处理元素”。AMD也称
为 “SIMD车道”
可供一个GPU中所有多线程SIMD处理器
访问的DRAM存储器。OpenCL称为“全局
存储器"
流式多处理器中的Warp调
度程序调度Warp，以在下一
朱指令做好执行准备后
执行
线程处理骼是流式多处理
器中的一条数据路径和寄
存器堆部分，执行一个Waup
中一条或多条车道的运算
全局存储器可供任意网格
任意线程块中的所有
CUDA线程访问。实现为
DRAM的一个区城，可被
缓存
专用存储器
本地存储器
每个SIMD车道专用的DRAM存储器部分。
AMID和OpenCI称为“专用存储器”
CUDA线程的专用 “线程本
地”存储器。实现为DRAM
的緩存区城
存储
骼硬
件
本地存储器
共享存储器
一个多线程SIMD处理器的快速本地
SRAM，不可供其他SIMID处理器使用。
OpenCL称为“本地存储器”
，AMD称为“组
存储器"
由线程块中CUDA线程共
享的快速SRAM存储器，
由该线程块专用。用于在
屏障同步点在一个线程块
中的CUDA线程之间进行
通信
SIMD车道寄存
器
寄存器
在向量化循环体之间分配的单个SIMD车
道中的寄存器。AMD也称为“寄存器”
CUDA线程的专用寄存器。
实现为每个线处理器的
几个Warp中特定车道的多
线程寄存器堆
* 注意这里的描述性术语“本地存储器”和“专用存储器” 使用了 OpenCL. 术语。NVIDIA 在描述流式多处理器时使用
T SIMIT（单指令多线程）而不是 SIMID。SIMIT 优于 SIMLD 是因为按线程分支和控制流与任何 SIMID机都不同。

\section{检测与增强循环强并行}
程序中的循环是我们前面讨论以及将在第5章讨论的许多并行类型的根源。本节，我们讨
论用于发现并行以在程序中加以开发的编译器技术，以及这些编译器技术的硬件支持。我们准
确地定义一个循环何时是并行的（或可向量化的）、相关性是如何妨碍循环成为并行的，以及用
于消除几类相关性的技术。发现和利用循环级并行对于开发 DLP 和 TLP 以及将在附录日中研
究的更主动静态 ILP方法（例如，VLIW）都至关重要。

循环级并行通常是在源代码级别或接近级别进行分析的，而在编译器生成指令之后，就
完成了对ILP 的大多数分析。循环级分析需要确定循环的操作数在这个循环的各次迭代之间
存在哪种相关性。就目前来说，我们将仅考虑数据相关，在某一时刻写人操作数，并在稍后
时刻读取时会出现这种相关性。名称相关也是存在的，利用第3章讨论的重命名技术可以消
除这种相关。

循环级并行的分析主要是判断后续迭代中的数据访问是否依赖于在先前选代中生成的数据
值；这种相关被称为循环间相关。我们在第2章和第3章考虑的大多数示例都没有循环间相关，
而是循环级并行的。为了了解一个循环是并行的，让我们首先看看源代码：

for （1=999; 1>=0; i=i-1）
x［i］ = xlil + si

在这个循环中，对x［的两次使用是相关的，但这是同一个迭代内的相关，不是循环间相关。
在不同迭代中对i的连续使用之间存在循环间相关，但这种相关涉及一个容易识别和消除的归
纳变量。我们在第2章的2.2节讨论循环展开时见到了一些示例，说明如何消除涉及归纳变量
的相关。

要寻找循环之间的并行，需要识别诸如循环、数组引用和归纳变量计算之类的结构，所以
与机器码级别相比，编译器在源代码级别或相近级别进行这一分析要更轻松一些。让我们看一
个更复杂的例子。

例题
考虑下面这样一个循环：
for （i=0；i<100; i=i+1）｛
A［i+1］ = A［i］
+ C［il；
/*S1 */
B［i+1］ - B［i］ + A［i+l］：/*S2*/
｝

假定A、B和C是没有重叠的不同数组。（在实践中，这些数组有时可能相同，或可
能重叠。因为这些数组可能是作为参数传递给包含这一循环的过程，为了判断数
组是否重叠或相同，通常需要对程序进行复杂的过程间分析。）在这个循环中，语
句 S1 和S2之间的数据相关如何？

解答
共有以下两种不同相关。

（1） S1 使用一个在先前迭代中由S1计算的值，这是因为选代i计算 ACi+1］，然后在
迭代i+1 中读取它。对B［i和 B［i+1］来说，S2 也是如此。
（2） S2使用由同一迭代中SI 计算的值 A［1+1］。

这两种相关是不同的，拥有不同的效果。为了了解它们如何不同，我们假定此类
相关只能同时存在一个。因为语句S1依赖于S1 的先前迭代，所以这种相关是循环
间相关。这种相关迫使这个循环的连续迭代必须按顺序执行。

第二种相关（S2对S1 的依赖）位于一个迭代内，不是循环间相关。因此，如果它
是仅有的相关，那这个循环的多个迭代就能并行执行，只要一个选代中的每对语
句保持相对顺序即可。我们在2.2节的例子中看到过这种类型的相关，通过循环展
开可以暴露这种并行。这种循环内的相关是很常见的，例如，使用链接（chaining）
的向量指令序列就存在此类相关。

还有可能存在一种不会妨碍并行的循环间相关，如下例所示。

例题
考虑下面这样一个循环：
for （1=0;1<100:f=i+1）｛
A［门 = A［i］ +B［i；
〕
S1 和S2之间是什么样的相关？这一循环是否为并行的？如果不是，说明如何使之
成为并行循环。
解笭
语句S1 使用了在上一次迭代中由语句S2指定的值，所以在S2与S1之间存在循环
间相关。尽管存在这一循环间相关，依然可以使这一循环变为并行。与前面的循环
不同，这种相关不存在环式相关：这些语句都没有依赖于自身，而且尽管S1依赖
于\$2，但S2 没有依赖于S1。如果可以将一个循环改写为没有环式相关的形式，那
这个循环就是并行的，因为没有这种环式相关形式就意味着这种相关性对语句进行
了部分排序。
尽管以上循环中没有环式相关，但必须对其进行转换，以符合部分排序，并暴露出
并行。两个观察结果对于这一转换至关重要。
（1） 不存在从S1到S2的相关。如果存在这种相关，那就可能存在环式相关，那循
环就不是并行的。由于没有其他相关，所以两个语句之间的互换不会影响S2的
执行。
（2） 在循环的第一次迭代中，语句S2依赖于B［0］值，它是在开始循环之前计算的。
这两个观察结果可以让我们用以下代码序列来代替以上循环：
A［0］ = AEO］+B［O］；
for （i=0; ixg9; i=i+1）［
B［i+1］ - c［i］ + D［i］；
A［i+1］ = A［i+1］ + B［i+1］；
｝
B［100］ = C［99 + D［99］：
这两个语句之间的相关不再是循环间相关，所以循环的各次迭代可以重叠，只要每
次迭代中的语句保持相对顺序即可。
我们的分析需要首先找出所有循环间相关。这一相关信息是不确切的，也就是说，它告诉
我们此相关可能存在。考虑以下示例：
for （i=0;i¢100;i=i+1） ｛
A［i］
= B［i］
+ C［i］
D［i］ - A［i］*E［
｝
这个例子中对A的第二次引用不需要转换为载入指令，因为我们知道这个值是由上一个语句计
算并存储的；因此，对A的第二个引用可能就是引用计算A的寄存器。执行这一优化需要知道
这两个引用总是指向同一存储器地址，而且不存在对相同位置的干扰访问。通常，数据相关分
析只会告诉我们一个引用可能依赖于另一个引用；要确定两个引用一定指向同一地址，那就需
要进行更复杂的分析。在上面的例子中，进行这一简单分析就足够了，因为这两个引用都处于
同一基本块中。

循环间相关经常会是一种递归（recurrence）形式。如果要确定变量的取值，需要先知道该
变量在前面迭代中的取值时，就会发生递归，这个先前迭代往往就是前面的相邻近代，如以下
代码段所示：

for （i=1;ix100;i=i+1）｛
Yil = YCi-l+ Yi：
｝

检测递归是非常重要的，原因有两个：其一，一些体系结构（特别是向量计算机）对执行
递归提供特殊支持；其二，在 ILP环境中，仍然可能开发相当数量的并行。

\subsection{查找相关}
显然，查找程序中的相关对于确定哪些循环可能包含并行以及如何消除名称相关都很重要。
诸如C或C++语言中存在数组和指针，Fortran 中存在按引用传送的参数传递，这些也都增加了
相关分析的复杂度。由于标量变量引用明确指向名称，所以用别名对它们进行分析是比较轻松
的，因为指针和引用参数会增加分析过程的复杂性和不确定性。

编译器通常是如何检测相关的呢？几乎所有相关分析算法都假定数组索引是仿射的（affime）。
用最简单的话说，一维数组索引可以写为a×i+6的形式，其中a和B是常数，i是循环索引变
量，也就说这个索引是仿射的。如果多维数组每一维的索引都是仿射的，那就称这个多维数组
的索引是仿射的。稀疏数组访问（其典型形式为x［y［i］］）是非仿射访问的主要示例之一。
要判断一个循环中对同一数组的两次访问之间是否存在相关，等价于判断两个仿射函数能
否针对不同索引取同一个值（这些索引当然没有超出循环范围）。例如，假定我们已经以索引值
a x i+b存储了一个数组元素，并以索引值cx i+d从同一数组中载人，其中i是FOR循环索
引变量，其变化范围是m~n。如果满足以下两个条件，则存在相关性。

（1） 有两个迭代索引j和k，它们都在循环范围内。即m≤J≤n、m≤K≤No
（2）此循环以索引ax +b存储一个数组元素，然后以cxkd提取同一数组元素。即ax C×
ktda

一般来说，我们在编译时不能判断是否存在相关。例如，a、 、c和d的值可能是未和的（它
们可能是其他数组中的值），从而不可能判断是否存在相关。在其他情况下，在编译时进行相关
测试的开销可能非常高，但的确可以确定是否存在；例如，可能要依靠多重嵌套循环的迭代索
引来进行访问。但是，许多项目主要包含一些简单的索引，其中a、 、c和d都是常数。对于
这些情况，有可能设计出一些合理的测试程序，在编译时测试。

举个例子，最大公约数（GCD）测试非常简单，但足以判定不存在相关的情况。它基于以
下事实：如果存在循环间相关，那么GCD（c,a）必须能够整除（d-b）。（回想一下，有两个整数
x、y，在计算 y/x除法运算时，如果能够找到一个整数商，使运算结果没有余数，则说x能够
整除y。）

例题
使用 GCD 测试判断以下循环中是否存在相关：
for （i=0; i≤100; i=i+1）｛
X［2*1+3］= X［2*1］ * 5.0；
解答

以不可能存在相关。

GCD 测试足以确保不存在相关，但在某些情况下，GCD测试成功但却不存在相关。例如，
这种情况可能因为GCD测试没有考虑循环范围。

一般来说，要确定是否实际存在相关，就是一个 NP完全（NP-complete）问题。但实际上，
有许多常见情况能够以低成本来准确分析。最近出现了一些既准确又高效的方法，它们使用不
同层次的精确测试，通用性和成本都有所提高。（如果一个测试能够确切地判断是否存在相关，
就说这一测试是确切的。尽管一般情况是“NP完全”的，但对于一些有一定限制的情況，是存
在确切测试的，其成本也要低廉得多。）

除了检测是否存在相关以外，编译器还希望划分相关的种类。编译器可以通过这种分类来
识别名称相关，并在编译时通过重命名和复制操作来消除这些相关。

例题
下面的循环有多种类型的相关。找出所有真相关、输出相关和反相关，并通过重
命名消除输出相关和反相关。

for （i=0:i≤100:1=1+1）【
Y［］- X［i/c: /*SI*/
X［i - X［iJ+ci/*S2*/
z［i］ = Y［i］ +c; /* S3 */
Y［iJ-c - Y［i；/* S4 */
解笞

4个语句之间存在以下相关。

（1） 由于 Y［1的原因，从S1至S3、从S1至SA存在真相关。这些相关不是循环间相
关，所以它们并不妨碍将该循环看作是并行的。这些相关将强制S3和S4等待
S1完成。
（2） 从S1到S2有基于X［1的反相关。
（3） 从S3到S4有关于Y［i的反相关。
（4） 从SI到S4有基于Y［i的输出相关。

以下版本的循环消除了这些假（或伪）相关。
for （i=0; 1<100; i=f+I｛
T［i］- X［1］ /c； /* Y 重命名为T，以消除输出相关*/
X1［i］-XLIJ+c:/*X 重命名 XI，以消除反相关*/
Z［门］-T［i］ + c: /*Y 重命名为T，以消除反相关*/
｝
Y［i］ a c-TCi：

在这个循环之后，变量X被重命名为X1。在此循环之后的代码中，编译器只需要
用x1来代替名称 X即可。在这种情况下，重命名不需要进行实际的复制操作，通
过替换名字或寄存器分配就可以完成重命名。但在其他情况下，重命名是需要复
制操作的。

相关分析是一种非常关键的技术，不仅对开发并行如此，对于第2章介绍的转换分块也是
如此。相关分析是检测循环级别并行的一种基本工具。要针对向量计算机、SIMDD计算机或多
处理器进行有效的程序编译，都依赖于这一分析。相关分析的主要缺点是它仅适用于非常有限
的一些情况，也就是用于分析单个循环嵌套中引用之间的相关以及使用仿射索引功能的情景。
因此，在许多情况下，面向数组的相关分析不能告诉我们希望知道的内容；例如，在分析用指
针而不是数据索引完成的访问时，可能要困难得多。（对于许多并行计算机设计的科学应用程
序，Fortran仍然优于C和C++，上述内容就是其中一个理由。）同理，分析过程调用之间的引
用也极为困难。因此，尽管依然需要分析那些以顺序语言编写的代码，但我们也需要一些编写
显式并行循环的方法，比如 OpenMP 和 CUDA。

\subsection{消除相关计算}
上面曾经提到，相关计算的最重要形式之一就是“递归”。点积是递归的一个完美示例：
for （1=9999;1≥=0; i=i-1）
sum = sum + x［i］ * y［ils

这个循环不是并行的，因为它的变量求和存在循环间相关。但是，我们可以将它转换为一组循
环，其中一个是完全并行的，而另一个可以是部分并行的。第一个循环将执行这一循环的完全
并行部分。它看起来如下所示：

for （i=9999; i>=0; i=i-2）
sum［i］ - x［i］ *y［il；

注意，这一求和已经从标量扩展到向量值（这种转换被称次标量扩展），通过这一转换使新的循
环成为完全并行的循环。但是，在完成转换时，需要进行约简步骤，对向量的元素求和。类似
如下所示：

for （1=9999: 1>=0; i=1-1）
finalsum = finalsum + sum［i］；

尽管这个循环不是并行的，但它有一种非常特殊的结构，称为约简（reduction）。约简在线性
代数中很常见，在第6章将会看到，它们还是仓库级计算机中所有主要并行原型 MapReduce
的关键部分。一般来说，任何函数都可用作约简运算符，常见情况中包含着诸如 max 和 min
之类的运算符。

在向量和SIMD体系结构中，约简有时是由特殊硬件处理的，使约简步骤的执行速度远快
于在标量模式中的完成速度。具体做法是实施一种技术，类似于可在多处理器环境中实现的技
术。尽管一般转换可以使用任意个处理器，但为简便起见，假定有10个处理器。在对求和进行
约简的第一个步骤中，每个处理器执行以下运算（p是变化范围为0~9的进程号）：

for （i=999; i2=0; i=i-1）
finalsum［p］= finalsum［p］ + sum［i+1000*p］；

这个循环在10个处理器中的每个处理器上对1000个元素求和，它是完全并行的。最后用简单
的标量循环来完成最后10个总和的计算。向量和 SIMD处理器中使用了类似的方法。

以上转换依赖于加法的结合性质，观察到这一点是非常重要的。尽管拥有无限范围与精度
的算术运算具有结合性质，但计算机运算却不具备结合性：对于整数运算来说，是因为其范围
有限；对于浮点运算来说，既有范围原因，又有精度原因。因此，使用这些重构技术有时可能
会导致一些错误行为，尽管这种现象很少会发生。为此，大多数编译器要求显式启用那些依赖
结合性的优化。

\section{交叉问题}

\subsection{能耗与DLP：慢而宽与快而窄}
数据级并行体系结构的主要功耗优势来自第1章的能耗公式。由于我们假定有充足的数
据级并行，所以，如果将时钟频率折半、执行资源加倍：将向量计算机的车道数加倍，将多
4.7 融会贯通：移动与服务器 GPU、Tesla 与 Core i7
241
媒体 SIMD的寄存器和ALU加宽、增加GPU的SIMD车道数，那性能是一样的。如果我们
在降低时钟频率的同时降低电压，那就可以降低计算过程的功耗和功率，同时保持峰值性能
不变。因此，DLP处理器的时钟频率可以低于系统处理器，后者依靠高时钟频率来获取性能
（见4.7节）。

与乱序处理器相比，DLP处理器可以采用较简单的控制逻辑，在每个时钟周期中启动大量
计算；例如，这一-控制对于向量处理器中的所有车道都是相同的，没有用于决定多指令发射的
逻辑和推测执行逻辑。利用向量体系结构还可以轻松地关闭芯片中的未使用部分。在发射指令
时，每条向量指令都明确指明它在大量周期内所需要的全部资源。

\subsection{分组存储器和图形存储}
4.2节提到了实际存储器带宽对于向量体系结构支持单位步幅、非单位步幅和集中-分散访
问的重要性。
为了实现高性能，GPU也需要充足的存储器带宽。专为 GPU设计的特殊 DRAM芯片可以
帮助提供这一带宽，这种芯片被称为 GDRAM，即图形DARM。与传统DARM芯片相比，GDRAM
芯片的带宽较高，容量较低。为了提供这带宽，GDRAM芯片经常被直接焊接在GPU所在的
同一电路板上，而不是像系统存储器那样设置在 DIMM 模块中，DIMIM 是插在主板插槽中的。
DIMM 模块便于系统升级和提供更大的容量，这一点与GDRAM不同。这一有限容量（2011年
大约为4GB）与解决更大问题的目标相冲突，随着GPU计算能力的增长，这冲突将成为它的
一个必然趋势。
为了提供最佳性能，GPU试图考虑 GDRAM的所有特性。它们在内部通常被安排为4~8
组，行数是2的幂（通常为16384），每行的位数也是2的幂（通常为8192）。第2章介绍了GPU
尝试匹配的一些 DRAM行为细节。
在给出计算任务及图形加速任务对GDRAM的所有潜在要求之后，存储器系统可能会面对
大量的不相关请求。然而，这种多样性会伤害到存储器性能。为了应对这种情况，GPU的存储
器控制器为不同GDRAM组设定分离的通信量限度队列，要等到具有足够的通信量后才会打开
一行，并同时传送所请求的全部数据。这一延迟提升了带宽，但使延迟时间增长，控制器必须
确保所有处理过程不会因为等待数据而“挨饿”，否则，相邻的处理器可能会处于空闲状态。4.7
节显示了集中-分散技术，与基于缓存的传统体系相比，那些考虑了存储器组的访问技术可以提
高性能。

\subsection{步幅访问和TLB缺失}
步幅访问的一个问题是它们如何与转换旁视缓冲区（TLB）进行交换，以在向量体系结构
或GPU中获得虚拟存储器。（GPU使用TLB来实现存储器映射。）根据TLB的组织方式以及存
储器中受访数组的大小，甚至有可能在每次访问数组的元素时都会遇到一次 TLB缺失。

\section{融会贯通：移动与服务器 GPU、Tesla 与 Core i7}
由于图形应用程序如此普及，所以现在的移动客户端以及传统的服务器或高效桌面计算机
中都可以看到GPU的身影。表4-12列出了 NVIDIA Tegra 2和 Fermi GPU的重要特性，前者用
于 LG Optimus 2X 中，运行 Android 操作系统，后者用于服务器中。GPU服务器工程师希望能
够在一部电影发行后的五年内完成仿真动画。GPU移动工程师则希望再过五年后，移动客户端
能够完成今天的服务器或游戏主机所完成的工作。更具体地说，总体目的是，希望到2015年能
够在服务器 GPU上实时实现诸如《阿凡达》“这样一部电影的图形质量，而到2020年可以在務
动 GPU上实现同一质量。

表4-12 用于移动客户端及服务器的GPU的关键特性
NVIDIA Tegra 2
NVIDIA Fermi GTX 480
市场
移动客户端
桌面机、服务器
系统处理器
双核ARM Cortex-A9
不适用
系统接口
不适用
PCI Express 2.0x16
系统接口带宽
不适用
6GB/s（每个方向）、12 GB/s（总共）
肘钟频率
最高1GHz
1.4 GHI
SIMD多处理器
不可用
15
SIMID车道/SIMD多处理器
不可用
32
存储器接口
32位LP-DDR2/DDR2
384位GDDRS
存储器带宽
2.7 GB/s
177 GB/s
存储器容鱼
1 GB
1.5 GB
晶体管
242 M
3030M
工艺
40 nm TSMC I艺G
40 mm TSMC 工艺G
晶片面积
57 mm'
\$20 m？
功率
1.5 瓦
167 瓦

 Tegra2是 Android OS的参考平台，LG Optimus2X移动电话中采用丁这一处理器。

供移动设备使用的 NVIDIA Tegra2在使用单一物理存储器的单块芯片上同时提供系统处理
器和 GPU。系统处理器是一个双核 ARM Cortex-A9，每个核心采用乱序执行和双指令发射。每
个核心包括可选的浮点单元。

GPU 为可编程像素着色、可编程顶点与光照和 3D图形提供了硬件加速，但没有包含运行
CUDA 或 OpenCL 程序所需要的GPU计算功能。

在 40nm TSMC工艺中，晶片大小为57 mm2（7.5×7.5 mm），包含2.42亿个晶体管。功率
为1.5瓦。

表4-12中的NVIDIA GTX 480是第一次实现 Fermi体系结构的GPU。时钟频率为1.4GHz，
其中包括15个 SIMD处理器。芯片本身拥有16个处理器，但为了提高合格率，对于这一产品
而言，只需要其中的15个处理器正常工作。连到 GDDR5存储器的路径宽度为384（6× 64），
时钟频率为1.84 GHz，在双数据率存储器的两个时钟沿上传送数据，所以峰值存储器带宽为
177 GB/s。它通过PCI Express2.0x16链路连接到主机系统处理器和存储器，其峰值双向速率
为12 GB/s。

GTX 480 晶片的所有物理特性都非常庞大：包含30亿个晶体管，采用40 nm TSMC工艺，
晶片大小为520 mm’（22.8mmx22.8 mm），典型功率为167瓦。整个模块的功率为250瓦，其
中包括 GPU、GDRAM、风扇、功率调节器，等等。

① 电影《阿凡达》的上映时间是2010年。—一编者注

\subsection{对比GPU与具有多媒体SIMD的MIMD}
Intel 的一群研究人员发表了一篇论文［Lee 等人2010］，将具有多媒体 SIMD扩展的四核 Intel
i7（见第3章）与上一代 GPU-Tesla GTX 280进行对比。表4-13列出了两个系统的特性。这两
种产品都是在2009年秋天生产的。Core i7采用 Intel 的45 nm半导体技术，而GPU则采用TSMC
的65 nm技术。尽管由中立方或两个利益方都进行这一对比更公正一些，但这一节的目的不是
确定这件产品比那件产品快多少，而是希望理解这两种相对体系结构类型的相对特征值。

表4-13 Intel Core i7-960、NVIDIA GTX 280 和GTX 480 技术指标
Core
Ratio
i7-960
GTX 280
处理元素数目（核心或SM）
4
30
GTX 480
15
280/7
7.5
时钟频率（GHz）
3.2
1.3
0.41
晶片大小
263
576
520
2.2
工艺
Intel 45 mm
TSMC 65mm
TSMC 40mm
1.6
功率（芯片，不是榄块）
130
130
167
1.0
晶体管
700 M
1400 M
3030M
2.0
存储器带宽（GB/s）
32
141
177
4.4
单精度SIMDD宽度
4
8
32
2.0
双精度SIMDD宽度
2
1
16
0.5
峰值单精度标量FLOPS（GFLOP/S）
26
117
63
4.6
峰值单精度SIMDD FLOPS（GFLOP/s）
102
311~933
515~1344
3.0~9.1
（SP 1加或乘）
N.A.
（311）
（515）
（3.0）
（SP 1指令融合乘-加）
N.A.
（622）
（1344）
（6.1）
Ratio
480/7
3.8
0.44
2.0
1.0
13
4.4
5.5
8.0
8.0
2.5
6.6~13.1
（6.6）
（13.1）
（罕见的SP双精度融合乘-加与乘）
N.A.
（933）
N.A.
（9.1）
峰值双精度SIMD FLOPS （GFLOP/s）
51
78
515
1.5
10.1

* 最右边两列给出 GTX 280与 GTX480与 Core i7之比。 于 GTX280上的单精度 SIMDD FLOPS，较高速度（933）源
于一种非常罕见的情况：融合乘-加与乘法的双发射。对于单融合乘-加来说，較合理的值是622。尽管此实例研究是
在280与i7之间的对比，但由于本章描述了480，所以图中也包含了480，以显示它与280的关系。注意，園4-16
中的这些存储器带宽更高一些，这是因为本表中为 DRAM 管腳带宽，而田4-16 中則是由基准测试程序在处理器测出
的。（摘自 Lee 等人［2010］论文中的表2。）

图4-16中 Core i7 920和 GTX 280 Roofline 模型显示了两种计算机之间的差别。920的时钟
频率慢于960（分别是2.66 GHz、3.2GHz），但系统的其他部分都是相同的。GTX 280 不仅拥
有高得多的存储器带宽和双精度浮点性能，而且它的双精度脊点也要明显靠左。如前文所述，
Roofline 的脊点越靠左，就越容易达到峰值计算性能。GTX280的双精度脊点为0.6，而Core i7
则为2.6。对于单精度性能，脊点大幅右移，这是因为它的单精度性能过高，所以要达到其峰值
性能要难得多。注意，内核的运算密度取决于进入主存储器的字节数，而不是进入缓存存储器
的字节数。因此，如果假定大多数引用实际指向缓存，那缓存可以改变特定计算机上内核的运
算密度。Roofline 可以帮助解释这一实例研究中的相对性能。还要注意到，在两种体系结构中，
这一带宽都是针对单位步幅访问的。后面将会看到，对于那些没有接合的集中一分散寻址，在
GTX 280中要比在 Core i7 中慢一些。

Roofine 模型，Williams 等人［2009］。这些 Roofline 在上半部分显示了双精度浮点性能，在下
半部分中显示了单精廈性能。（还给出了 DPFP 性能值，希望提供一些观察角度。）左边Corei7
920的峰值 DPFP性能为 42.66 GFLOP/s,SPFP 峰值性能为85.33 GFLOP/s，峰值存储器带宽为
16.4 GB/s。NVIDIA GTX280的DPFP峰值性能为 78 GFLOP/s,SPFP 峰值为624 GFLOP/s，存
储器带宽为127GB/s。左侧的垂直虚线表示等于0.5 FLOP/B 的运算密度。对于 Core i7，它受存
储器带宽的限制，不超过8DP GFLOP/s或8 SP GFLOP/s。右侧的虚线表示等于4 FLOP/B 的运
算密度。在Core i7上，它仅受计算限制，为42.66 DP GFLOP/s 和64 SP GFLOP/s，在GTX 280
上为 78 DP GFLOP/s和 512 DP GFLOP/s。为了达到 Core i7的最高计算速率，需要使用所有4
个核心和具有相同数量乘-加的 SSE 指令。对于 GTX280，需要在所有多线程SIMD处理器上使
用整合乘-加指令。Guz 等人［2009］给出了这两种体系结构的一种重要分析模型

据这些研究人员所述，他们在选择基准测试程序时，分析了最近提出的4个基准测试套件
的计算特性与存储器特性，然后“设计了一组收集这些特性数值的吞吐量计算内核”。表4-14
描述了这14种内核，表4-15显示了性能结果，数值越大，表示速度越快。

内
核
SGEMMM（SGEMM）
豪特卡洛（MC）
卷积（Conv）
表4-14 吞吐量计算内核特性值（摘自 Lee 等人［2010］的表1）
应用
SIMD
TLP
线性代数
常规
跨2D贴图
计算金融
常规
跨路径
图像分析
常规
跨像素
FET （FET）
信号处理
常规
跨更小的FFT
SAXPY （SAXPY）
LBM （LBM）
約束解算器（Solv）
SpMV（SpMV）
GIK （GJK）
排序（Sort）
光线投射 （RC）
点积
时间迁移
刚体物理学
豨疏矩阵解算器
冲突检测
数据库
体渲染
常规
常规
集中/分散
集中
集中/分散
集中/分散
集中
跨向量
跨单元
跨约束条件
跨非零
跨对象
跨元素
跨射线

搜索（Search）
数据库
集宁/分散
跨查询
特
性
贴图之后的计算隰制
计算限制
计算限例，小型滤波器的
BW限制
计算限制或取决于大小
的BW限制
大型向量的BW限制
BW限制
同步限制
典型大矩阵的BW限制
讨箅限制
甘算限制
4~8MB一级工作集，超
过500 MB的末级工作集
小型树的计算限制，大型
树底的BW限制
还原/同步限制
柱状图（Hist）
图像分析
需要冲突检测
跨像素

*括号中的名称表示本节的基准测试名称。作者指出，两种机器的代码都进行了同样的优化努力。
表4-15 在两个平台上测量的原始性能及相对性能

内核
单位
Core i7-960
GTX 280
GTX 280/17-960
SGEMM
GFLOP/s
94
364
3.9
MC
十亿路径/秒
0.8
1.4
1.8
Conv
百万像素秒
1250
3500
2.3
FFT
GFLOP/s
71.4
213
3.0
SAXPY
GB/s
16.8
88.8
5.3
LBM
百万查询/秒
85
426
5.0
Eo
帧/秒
103
$2
0.5
Spivv
GFLOP/s
4.9
9.1
1.9
GIK
懒桫
67
1020
15.2
Sort
百万元素/秒
250
198
0.8
RC
5
8.1
1.6
Search
百万查询/秒
50
90
1.8
Hist
百万像素
1517
2583
1.7
Bilat
百万像素/
83
475
$.7

*在这一研究中，SAXPY仅用作存储器带宽的一种度量，所以右边的单位为 GB/s，而不是 GFLOP/s（根据山eE等人
2010］中的表3。）

已知 GTX 280 的原始性能技术指标由较低的2.5倍变化到较快的7.5倍（每芯片的数），
而性能由较慢的2.0倍（Solv）变化到15.2倍（GJK），Intel研究人员研究了这些差异的
要有以下儿方面。

\begin{itemize}
    \item 存储器带宽。GPU 的存储器带宽为4.4倍，它可以帮助解释为什么 LBM 和 SAXPY的运
    行速度要快5.0和5.3倍；它们的工作集为数百 MB，因此不能放到Core i7缓存中。（为
    了密集访问存储器，它们没有在 SAXPY上使用缓存分块。）因此，Roofline 的斜率解释
    了它们的性能。SpMV还有一个大型工作集，但由于GTX280的双精度浮点仅比Corei7
    快1.5倍，所以 SpMV的速度仅为1.9倍。（回想一下，Fermi GTX 480 的双精度比Tesla
    GTX280快4倍。）
    \item 计算带宽。其余内核中有5个是计算限制：SGEMM、Conv、FFT、MC 和 Bilat。GTX
    分别快3.9、2.8、3.0、1.8和5.7。前3个使用单精度浮点运算，GTX280单精度快3~6
    倍。（只有在GTX280每个时钟周期可发射一个融合乘加和乘法指令的极特情况下，
    才会出现表4-13所示比 Core i7快9倍的情况。）MC使用双精度，它解释了为什么 DP
    性能仅快1.5倍，MC仅快1.8倍。Bilat使用超越函数，GTX280直接支持这些函数（见
    表4-7）。Core i7 将三分之二的时间花费在计算超越函数上，所以 GTX280快5.7倍。这
    一观察结果有助于指出硬件支持对工作负载中某些运算的价值：双精度浮点，还有可能
    包括超越函数。
    \item 缓存优势。光线投射（RC）在GTX上仅快1.6倍，这是因为利用Core i7 进行缓存分块
    可以防止它像在GPU’上一样，变为存储器带宽限制。缓存分块也可以帮助 Search。如果
    索引树很小，可以放在缓存中，那么Core i7将会快2倍。索引树较大时，则会成为存储
    器带宽限制。整体上，GTX280执行搜索的速度快1.8倍。缓存分块出对Sort有所帮助。
    大多数程序员不会在SIMID处理器上运行 Sort，可以用一个称为分割的1位 Sort 原型来
    编写它。但是，分割算法执行的指令数要比标量排序多很多。结果，GTX280的运行速
    度仅是 Core i7的0.8倍。注意，缓存还可以帮助 Corei7上的其他内核，这是因为缓存
    分块允许 SGEMIM、FFT和SpMV成为计算限制。这一观察结果再次强调了第2章缓存
    分块优化的重要性。（了解 Fermi GTX480中的缓存将会如何影响这一段提到的6种内核
    是很有意义的。）
    \item 集中分散。如果数据分散在主存储器内，那多媒体SIMID扩展的帮助就不是很大；只有
    当数据与16字节边界对齐时，才会产生优化性能。因此，在Core i7上，GJK 从 SIMD
    中获得的好处很少。前面曾经提到，GPU提供了集中-分散寻址，在向量体系结构中使
    用了这一技术，但在 SINID 扩展中则被忽略。地址接合单元也可以提供帮助：合并对相
    同DRAM线的访问，从而减少集中与分散的数目。存储器控制器还会对相同 DRAM 页
    面进行批访问。这种组合意味着GTX280运行GJK的速度要比 Core i7快出15.2倍之多，
    这一数字要比表4-13中给出的任一单个物理参数都要大。这一观测结果再次强调了集
    中-分散对向量与GPU体系结构的重要性，这是SIMD扩展中所没有的。
    \item 同步。同步的性能受原子更新的限制，尽管Core i7 中有硬件提取与递增指令，但原子更
    新仍然占 Core i7 总运行时间的28\%。因此，Hist 在GTX280上仅快1.7倍。前面曾经提
    到，Fermi GTX 480 的原子更新比 Tesla GTX280上快5~20倍，所以在这一较新 GPU
    上运行 Hist同样是有意义的。Solv以少量带有屏障同步的计算克服了一批独立约束。Core
    i7获益于原子指令和存储器一致性模型，即使还没有完成对存储器层次结构的所有先前
    访问，也能确保得到正确结果。GTX 280版本中没有存储器一致性模型，它会从系统处
    理器启动一些批处理指令，使GTX280的运算速度为 Core i7的二分之一。这一观察点
    指出，同步性能对于某些数据并行问题是多么的重要。
\end{itemize}

Intel 研究人员选择的内核在 Tesla GTX280中暴露了许多弱点，而这些弱点往往在Tesla 的
后续体系结构中得以克服，例如，Fermi 拥有更快速的双精度浮点性能、原子操作和缓存。（在
一项相关研究中，IBM研究人员给出了相同的观测结果［Bordawrekar 2010］。）还有一点也非常重
要：向量体系结构对分散-集中的支持要比 SIMID 指令早几十年，它对于有效应用这些 SIMD扩
展非常重要，在进行这一对比之前就已经有人对此进行了预测［Gebis 和 Patterson 2007］。Intel
研究人员指出，这14个内核中的6个能够更好地利用 SIMD，在 Core i7上拥有更高效的集中-
分散支持。这一研究同样确定了缓存分块的重要性。了解未来各代多核及 GPU硬件、编译器和
库是如何响应这些提高内核性能的特性，也是很有意义的。

我们希望对比更多的多核GPU。注意，这一对比中省略了一个重要的特性，也就是为没有
给出为在这两种系统上获得运行结果所付出的努力。未来的对比最好还发布这两种系统上使用
的代码，使其他人能够在不同硬件平台上再现相同试验，并可能对结果进行改进。

\section{谬论与易犯错误}
尽管，从程序员的角度来看，数据级并行是 TLP之后的最简单并行形式，从架构师的角度
来看也可能是最简单的形式，但它仍然有许多谬论和易犯错误。

\textbf{谬论 GPU因作为协处理器而窘迫。}

尽管主存储器与 GPU之间的分割有一些缺点，但与 CPU保持一定距离也有一些优势。
例如，PTX之所以存在，部分原因就在于 GPU 实质上是一种1/O设备。编译器与硬件之间
的间接水平为GPU架构师提供了很大的灵活性，这一点要远远高于系统处理器架构师。人们通
常很难事先知道一种体系结构的创新是否受到编译器和库的支持，是否对应用程序非常重要。
有时，一种新的机制对于一两代是有效的，之后，其重要性就会随着 IT 世界的变化而下降。PTX
使GPU架构师大胆尝试创新，如果它们令人失望或者重要性下降，就会在后续GPU 中删除，
从而鼓励架构师进行实践。一项创新要被包含在系统处理器中，那就必须具备非常充分的理由
（此后可以进行的试验要少得多），这是因为：一旦发布二进制机器码，通常就意味着在该体系
结构的所有后续版本中都必须支持这些新特性。

Fermi 体系结构快速地改变了硬件指令集（从类似于x86的面向存储器转为类似于MIIPS 的
面向寄存器，另外，还将地址大小加倍，变为64位），但没有对 NVIDIA 软件栈产生任何影响，
330
这一事例充分体现了 PTX的价值。

\textbf{易犯错误 关注向量体系结构的峰值性能，忽略启动开销。}

早期存储器-存储器向量处理器（比如 TI ASC 和CDC STAR-100）的启动时间都很长。
对于某些向量问题，向量长度必须大于100时才能使向量代码快于标量代码！在 CYBER 205
（派生自STAR-100）上，DAXPY的启动开销为158个时钟周期，它充分提高了收支平衡点。
如果 Cray-1 和 CYBER 205 的时钟频率相同，当向量长度小于64时，Cray-1要更快一些。因
为Cray-1 的时钟也要更快一些（尽管205更新），所以当向量长度大于100之后，CYBER 205
才会更快。

\textbf{易犯错误 提高向量性能，但都没有相应地提高标量性能。}

在许多早期向量处理器中，这些不均衡性都成为一个问题，这也正是 Seymour Cray （Cray
计算机的架构师）改写规则的地方。许多早期向量处理器的标量单元都比较慢（启动开销也比
较大）。即使到了今天，向量性能较低但标量性能较佳的处理器，也要优于峰值向量性能较佳的
处理器。良好的标量性能可以降低开销成本（比如条带挖掘），并降低 Amdahl定律的影响。
有一个好的例子就是对比快速标量处理器和标量性能较差的向量处理器。Livermore Fortran
内核是一组24个向量化程度不同的科学内核。表4-16显示了在这一基准测试上测得两种不同
处理器的性能。尽管向量处理器的峰值性能较高，但由于其标量性能较低，从而拖慢了快速标
量处理器，从测得的调和平均值可以看出这一点。

表4-16 Livermore Fortran 内核在两个不同处理器上的性能测量值
任意循环的最低速率
任意循环的最高速率 所有24个循环的调和平均值
处理課
（MFLOPS）
（MFLOPS）
（MFLOPS）
MIPS M/120-5
0.80
3.89
1.85
Stardent-1500
0.41
10.08
1.72

* MIPS M/120-5 和 Stardent-1500（正式名称为 Ardent Titan-1）都使用 16.7 MHz MIPS R2000芯片作为主 CPU。Startdent-
1500使用其向量单元进行标量浮，点运算，其标量性能（用最低速率測试）是 MIIPS M/120-5的一半，后者使用MIRS
R2010FP芯片。对于可以调度向量化的循环，向量处理器要快2.5倍以上（最高频率）。但是，在对所有24 个循环
测量总性能的调和平均值时，Stardent-1500较差的标量性能抵消了其较好的向量性能。
今天，这一危险颠倒了过来：提高了向量性能（比如通过增加车道数目），而不提高标量性
能。这种短视是另一个导致非平衡计算机的因素。

下一个谬论与此密切相关。

\textbf{谬论 可以在不提高存储器带宽的情况下获得良好的向量性能。}

通过 DAXPY循环和 Roofline 模型可以看出，存储器带宽对所有 SIMD体系结构都非常重
要。DAXPY 进行每次浮点运算需要1.5次存储器引用，这一数值在许多科学代码都非常典型。
即使浮点运算不占时间，Cray-1也不能提高所用向量序列的性能，因为它受到存储器的限制。
当编译器使用分块来改变计算，以在向量寄存器中保存所有值时，Cray-1执行 Linpack 的性能
会有跳跃。这种方法降低了每个 FLOP 的存储器引用数目，将性能提升将近两倍！因此，对于
以前需要更多带宽的循环来说，Cray-1 上的存储器带宽就足够了。

\textbf{谬论 在GPU上，如果存储器性能不够好，只需要添加更多线程就可以了。}

GPU 使用许多 CUDA线程来隐藏到主存储器的延迟。如果存储器访问被分散，或者各个
CUDA线程之间没有相关，那么存储器系统在响应每个请求时会明显变慢。最终，即使有许多
线程也不能隐藏延迟。为使“增加 CUDA 线程” 的策略生效，不仅需要大量CUDA线程，而且
这些 CUDA线程自身还必须在存储器地址局域性方面有良好表现。
\section{结语}
随着个人移动设备上各种应用的普及，显示了音频、视频和游戏和这些游戏上的重要性，
数据级并行对个人移动设备的重要性也在增加。如果有一种比任务级并行更简单的编程模型，
而且可能具有更佳的性能效率，很容易就能预测到数据级并行在接下来十年中的复兴。事实上，
我们已经看到一些产品对数据级并行的重视，GPU和传统处理器都已经在增加 SIMD车道的效
目，其增加速度至少与添加处理器的速度一样快（见图4-1）。

因此，我们看到系统处理器正在拥有更多的GPU特性，反之亦然。传统处理器和 GPU的
一个最大性能差别是集中-分散寻址。传统的向量体系结构说明如何向 SIMID指令添加此类寻
址，我们希望随着时间的推移，越来越多的在向量体系结构中到证明的好思想能够添加到
SIMD 扩展中。

在4.4节开头时曾经说过，GPU 问题并不是简单地问一句哪种体系结构最好，而是当硬件
投人能够出色地完成图形处理时，如何对其进行改进以支持更具一般性的计算任务？尽管向量
体系结构名义上有许多优势，但向量体系结构能否像GPU一样成为出色的图形处理基础，还有
待证明。

GPU SIMD 处理器和编译器仍然是相对简单的设计。为了提高GPU 的利用率，特别是因为
GPU 计算应用程序刚刚开始进入开发阶段，所以随着时间的推移可能会引入更积极的技术。通
过研究这些新程序，GPU 设计人员肯定会发现和实现新的机器优化方法。有这样一个问题：在
向量处理器中用于节省硬件和能耗的标量处理器（或控制处理器），是否会出现在GPU中？
Fermi 体系结构已经引入了传统处理器中的许多功能，使GPU更具主流特性，但两者之间
还是有一些需要弥补的差距。以下是我们希望在不远的将来能够解决的问题。

\begin{itemize}
    \item 可虚拟化 GPU。事实已经证明，虚拟化对于服务器非常重要，它是云计算的基础（见第
    6章）。为了在云中包括GPU，它们也应当像它们要连接的处理器和存储器一样是可虚拟
    化的。
    \item GPU 存储器的容量较小。加快运算速度的意义就体现在解决更大规模的问题上，而更大
    规模的问题通常需要更大的存储器足迹。GPU在速度与大小方面的这种不一致性可以通
    过增加存储器容量来解决。其挑战是要在提高容量的同时还能保持高带宽。
    \item 直接对GPU 存储器进行1/O操作。真实程序对存储器设备和帧缓冲区进行VO操作，大
    型程序可能需要大量 1/O操作和相当多的存储器。今天的GPU 系统必须在1/O设备和系
    统存储器之间进行传送，然后在系统存储器和 GPU 存储器之间传送。这种额外的跳转会
    显著降低一些程序的1/O性能，降低 GPU的吸引力。Amdahl定律警告我们：如果在加
    快一项任务的处理速度时忽视其中一部分，可能会发生什么样的问题。我们希望未来的
    GPU 将所有1/O都看作一等公民，就像它今天处理帧缓冲区I/O一样。
    \item 统一的物理存储。对于以上两个问题有一种替代解决方案：为系统和 GPU采用同一个
    物理存储器，一些廉价的 GPU在 PMID和膝上电脑上就是这样做的。AMD Fusion 体系
    结构是在刚刚完成本书英文版第5版时发布的，它是传统 GPU和传统 CPU 的最初结合
    体。NVIDIA 也发布了 Project Denver，它在同一个地址空间中合并了 ARM标量处理器
    和 NVIDIA GPU。在这些系统交付时，了解它们是如何紧密集成的，以及这种集成对于
    数据级并行及图形应用产生的性能与能耗影响，都是很有意义的。
\end{itemize}

在研究了众多 SIMID版本之后，下一章将开始钻研 MIIMID领域。
\section{历史回顾与参考文献}
附录L.6节讨论了 riac TV（早期SIMID 体系结构的代表）和 Cray-1（向量体系结构的代表）。
该附录还研究了多媒体 SIMD扩展和 GPU的历史。
案例研究与练习（Jason D. Bakos 设计）
案例研究：在向量处理器和GPU上实施向量内核
本案例研究说明的概念
口 向量处理器编程
口 GPU 编程
口 性能评估
MrBayes 是一个著名的常用计算生物学应用程序，可以根据一组输人物种的多雪比对 DNA 序列数据
（长度为n）来推测这一物种的进化历史。MrBayes 的工作方式是对所有二叉树拓扑空间（这些输人物种就
是这个树的叶子）执行试探性搜索。要对具体树进行求值，应用程序必须为每个内部节点计算一个n×4
的条件似然表（名称为cIP）。这个表是另外4个表的函数，其中两个表分别是该节点两个子节点的条件
似然表（C1L和cIR，单精度浮点），另外两个表是相关的n × 4 ×4转移概率表（tiPL 和 tiPR，单精度浮
点）。这个应用程序的内核之一就是计算这个条件似然表，如下所示：
\begin{verbatim}
    for （K=0;kxseq_length;k++）｛
    cIP［h++］-｛tiPL［AA］*CIL［A］ + tiPL［AC］ *CIL［C］ + tiPL［AG］*cIL［G］ + tiPL［AT］ *C1L［TJ）
    *（t1PR［AA］*C1R［A］ + tiPR［AC］*c1R［C］
    + tiPR［AG］ *cIR［G］ + tiPR［AT］ *CIR［TJ）；
    cIP［ht+］- （tiPL［CA］*cIL［A］ + tiPL［CC］*cIL［C］ + tiPL［cG］ *clL［G］ + tiPL［CT］*clL［T］）
    *（tiPR［CA］ *CIR［A］ + tiPR［CC］*cIR［C］
    + tiPR［CG］*CIR［G］ + t1PR［CT］*CIR［TJ）；
    clP［h+］- （tiPL［GA］*CIL［A］ + tiPL［GC］ *CIL［C］ + tiPL［GG］*clL［G］ + tiPL［GT］*c1L［T］）
    *（tiPR［GA］*c1R［A］ + tiPR［GC］*cIR［C］ + tiPR［GG］*cIR［G］ + tiPR［GT］*c1R［T］）；
    c1P［ht+］- （tiPL［TA］*cIL［A］ + tiPL［TC］*CIL［C］ + tiPL［TG］*clL［G + tiPL［TT］*c1L［T］）
    *（tiPR［TA］*CIR［A］ + tiPR［TC］ *CIR［C］ + tiPR［TG］ *cIR［G］ + tiPR［TT］*CIRCT］）；
    C1L+ 4；
    clR += 4；
    tiPL += 16；
    tiPR += 16；
    4.1
    ｝
\end{verbatim}
［25］<4.2、4.3>假定有如表4-17所示的常量。给出 MIIPS 和 VMIIPS 的代码。假定我们不能使用
分散-集中载入或存储。假定tiPL、tiPR、CIL、CTR和CIP 的起始地址分别在 RtiPL、RtiPR、
RClL、RC1R和RCIP 中。假定 VMIPS 寄存器长度可由用户编程设定，可以通过设定特殊寄存器
VL（即IiVI.4）来指定。为便于筒化向量加法，假定向 VMIPS 中添加以下指令：
SUMR.S Fd, Vs Vector Summation Reduction Single Precisfon：
表4-17 本案例研究的常量和取值
常
量
AA、AG、AG、AT
CA、CC、CG、CT
GA.GC、GS.GT
TA、TC、TG、TT
A.C.G.T
；
0.1,2、3
4.5.6.7
8.9、10.11
12.13.14、15
0.1.2.3
案例研究与练习（Jason D. Bakos 设计）
251
这个指令对向量寄存器 VS执行求和化简，并将总和写到标量寄存器 Fd中。
4.2
［5］<4.2、4.3>假定 \verb|seq_length- 500|，两种实施的动态指令数为多少？
4.3
［25］<4.2、4.3>假定在向量功能单元上执行向量化简指令，它类似于向量加法指令。假定每个
向量功能单元只有一个实例，给出代码序列是如何安排的。此代码需要多少次钟鸣？每个 FLOP
需要多少个周期（忽略向量指令发射开销）？
4.4
［15］<4.2、4.3>现在假定我们可以使用分散-集中载入和存储指令（LVI 和SVI）。假定tiPL、
tiPR、CIL、CIR和cIP在存储器中顺序排列。例如，如果 \verb|seq_length一500|，则t1PR数组将在
tjPL 数组之后500*4字节开始。这将如何影响为这个内核编写 VMIPS代码的方式？假定可以
使用以下技术以整数来初始化向量寄存器，此技术将以数值（0，0，2000,2000）来初始化向
量寄存器：
LI R2,0
SH RZ,vec
SW R2,vec+4
LI R2,2000
SH R2,vec+8
SH R2,vec+12
LV V1,vec
假定最大向量长度为64。使用集中-分散装人指令有没有办法提高性能？如果有，可以提高多少？
4.5
［25］<4.4>现在假定我们希望使用单个线程块在GPU上实施 MrBayes 内核。使用CUDA 改写内核
的C代码。假定指向条件似然表和转稼概率表的指令以内核参数的形式指定。为循环的每个选代
调用一个线程。对于任何需要重复使用的值，应当先将其载人共享存储器中，然后再进行操作。
［15］<4.4>利用CUDA，我们可以使用块级别的粗粒度并行来并行计算多个节点的条件似然。假
定我们希望由树的底部向上计算条件似然。假定条件似然和转移概率数组在存储器中的组织方
式如问题4所述，12个叶节点中每个节点的表组也按照节点顺序存储在连续存储器地址中。假
定我们希望计算为节点12至节点17计算条件似然，如图4-17所示。修改在解答练习4.5时计
算数组索引的方法，将块编号包含在内。
4.7
［15］<4.4>将练习 4.6的代码转换为 PTX代码。这个内核需要多少条指令？
［10］<4.4>你认为这一代码在GPU上执行得怎么样？对你的回答给出解释。
22
20
图 4-17
示例树
335
B36
252
练习
4.9 ［10/20/20/15/15］<4.2>考虑以下代码，它将两个包含单精度复数值的向量相乘：
\begin{verbatim}
    for （i=0:1≤300:1++）｛
    c_relil- a_reli］ *b.relil- a.imlil *b intils
    c.in［il= a.rell］*b_imCil + a_Im［i］*b_relil：
    ｝
\end{verbatim}
假定处理器的运行頻率为 700MIz，最大向量长度为64。载人/存储单元的启动开销为15个时
钟周期，乘法单元为8个时钟周期，加法/减法单元为\$个时钟周期。
a.［10］<4.2>这个内核的运算密度为多少？给出理由。
b.［20］<4.2>将此循环转换为使用条带挖掘的VMIIPS 汇编代码。
c.［20］<4.2>假定采用链接和单一存储器流水线，需要多少次钟鸣？每个复数结果值需要多少
个时钟周期（包括启动开销在内）？
d. ［1S］ <4.2>如果向量序列被链接在一起，每个复数结果值需要多少个时钟周期（包含开销）？
e.［15］<4.2>现在假定处理器有三条存储器流水线和链接。如果该循环的访问过程中没有组冲
突，每个结果需要多少个时钟周期？
4.10
［30］<4.4>在这个问题中，我们将对比向量处理器与一种混合系统的性能，这一混合系统包含一
个标量处理器和一个基于GPU的协处理器。在混合系统中，主机处理器的标量性能优于GPU，
所以在这种情况下，所有标量代码都在主机处理器上执行，而所有向量代码都在GPU上执行。
我们将第一种系统称为向量计算机，将第二种系统称为混合计算机。假定你的目标应用程序包
含一个向量内核，运算密度为0.5FLOP/被访问DRAM字节；但是，这个应用程序还有一个标
量组件，必须在此内核之前和之后执行该组件，以分别准备输入向量和输出向量。对于示例数
据集，此代码的标量部分在向量处理器和混合系统的主机上都需要400ms 的执行时间。此内核
读取包含200MB 数据的输人向量，输出数据包含100B 数据。向量处理器的峰值存储器带宽
为30GB/s，GPU 的峰值存储器带宽为150GB/s。混合系统有一些额外开销，在调用该内核前
后，需要在主存储器和 GPU本地存储器之间传送所有输人向量。此混合系统的直接存储器访问
（DMA）带宽为10 GB/s，平均延迟为10 ms。假定向量处理器和GPU的性能都受存储器带宽的
限制。计算两种计算机执行这一应用程序所需要的执行时间。
4.11
［15/25/25］<4.4、4.5>4.5节讨论了化简运算，它通过重复应用一种运算而将向量简化为标量。
化简是一种特殊类型的循环递归。下面给出一个例子：
dot=0.0；
for （i=0:ix64:i++） dot = dot + ali］ * b［i］；
向量化编译器可以应用一种被称为标量扩展的转换，它将 dot扩展到向量中，并对循环进行分
割，从而可以用向量运算来执行乘法，而将化简运算当作独立的标量运算：
for （i=0;ix64;i++） dot.［i］ = alil *b［i；
for （i=1;i<64;i++）dot［0］ = dot［o］ + dot［i］s
4.5 节曾经提到，如果允许浮点加法符合结合律，那就有几种技术可用于实现化简的并行化。
8. ［15］<4.4、4.5>一种技术称为递归加倍，它对一个逐渐缩短的向量序列（即两个32元素向量，
然后是两个 16元素向量，以此类推）进行加法运算。说明C代码如何寻求以这种方式来执
行第二个循环。
b.［25］<4.4、4.5>在一些向量处理器中，向量寄存器中的各个元素是可以单独寻址的。在这种
情况下，一个向量运算的操作数可能是同一向量寄存器的两个不同部分。这就有了另外一种
化简方案—部分求和。其思想是将向量化简为 m次求和，其中m是通过该向量功能单元
的总延迟，包括操作数读写时间。假定 VMIPS 向量寄存器是可寻址的（例如，可以用操作
案例研究与练习（Jason D. Bakos 设计）
253
数V1（16）启动向量运算，表示输入操作数从元素16开始。）另外，假定加法运算的总延迟
（包括运算数读取和结果写人）为8个时钟周期。写一段 VIMIPS 代码序列，将VI 的内容化
简为八个部分求和。
c.［25］<4.4、4.5>在GPU上执行化简时，输入向量中的每个元素都有一个线程与其相关联。每
个线程的第一步是将其相应值写到共享存储器中。接下来，每个线程进入一个循环，对每对
输入值求和。每次迭代将元素数减半，在每次选代后，活动线程数也会减半。为将化简性能
提升至最大，应当将通过该循环过程的完整填充Warp 数提升至最大。换句话说，活动线程
应当是连续的。每个线程对共享数组进行索引的方式也应当避免在共享存储器中发生组冲
突。以下循环仅违犯了这些指南的第一条，还用到了求模运算符，对GPU来说，这种运算
符的成本是非常高的：
\begin{verbatim}
    umsigned int tid -threadIdx.x；
    for（unsigned int s=1; s < blockDim.xs s *= 2） ｛
    if （（tid & （2*s））=-0）｛
    sdataftid］ += sdataltid + s］；
    ｝
    _syncthreads（）；
\end{verbatim}
重写该循环，使其满足这些指南，而且不再使用求模运算符。假定每个 Warp 中有32 个线程，
只要来自相同 Warp 的两个或更多个线程引用索引，而这些索引对32求模的结果相同，就会发
生组冲突。
4.12 ［10/10/10/10］<4.3>以下内核执行有限时域差分法（FDTD）的一部分，用来计算三维空间的Max-
well 方程，它是 SPECO6fp 基准测试的一部分：
\begin{verbatim}
    for （int x=0; x≤NX-1; x++）｛
    for （int y=0; ysNY-1; y++）｛
    for （int z=0; z≤NZ-1; z++）｛
    int index = X*NY*NZ + y*NZ + 2；
    if （y>0 &&x>0）｛
    material = IDx［index］；
    dH1 =（Hz［index］ - Hz［index-increnenty］）/dy［y］；
    dH2 = （Hy［index］- Hy［index-incrementZ］）/dz［z］；
    Ex［index］= Ca［material］*Ex［index］+Cb［materia1］*（dH2-dH1）；
    ｝｝｝
\end{verbatim}
定dH1、dH2、HY、HZ、dy、d、Ca、Cb 和Ex 都是单精度浮点数组。假定IDx是无符号整数
数组。
a.［10］<4.3>这一内核的运算密度为多少？
b.［10］<4.3>这一内核是否可以执行向量或SIMD？说明理由。
c.［10］<4.3>假定这一内核将在存储器带宽为30GB/s的处理器上执行，这一内核是受存储器的
限制还是受计算的限制？
d. ［10］ <4.3>为这一处理器确定 Roofline模型，假定其峰值计算吞吐量为85 GFLOP/S。
［10/15］<4.4>假定有一种包含10个SIMD处理器的GPU体系结构。每条SIMD指令的宽度
为32，每个SIMD处理器包含8个车道，用于执行单精度运算和载入/存储指令，也就是说，
每个非分岔 SIMD 指令每4个时钟周期可以生成32个结果。假定内核的分岔分支将导致平均
80%的线程为活动的。俄定在所执行的全部 SIMD 指令中，70%为单精度运算、20%为载人/
存储。由于并不包含所有存储器延迟，所以假定SIMID 指令平均发射率为0.85。假定GPU的时
338
B39
254
340
第4章
向量、SIMD 和GPU体系结构中的数据級并行
钟速度为1.5GHz。
a. ［10］<4.4>计算这个内核在这个GPU上的吞吐量，单位为GFLOP/S。
b.［15］ <4.4>假定我们有以下选项：
（1）将单精度车道数增大至16
（2） 将 SIMD 处理器数增大至15（假定这一改变不会影响所有其他性能度量，代码会扩展到增
加的处理器上）。
（3） 添加缓存可以有效地将存储器延迟缩减40%，这样会将指令发射率增加至0.95，对于这些改
进中的每一项。
吞吐量的加速比为多少？
4.14 ［10/15/15］<4.5>在这一练习中，我们将研究几个循环，并分析它们在并行化方面的潜力。
a.［10］ <4.5>以下循环是否存在循环间相关？
for （i=0;ix100;it+）1
A［i］= B［2*i+4］；
B［4*1+5］ = A［1］；
b.［15］<4.5>在以下循环中，找出所有真相关、输出相关和反相关。通过重命名来消除输出相关
和反相关。
for （i=0:i<100;i++）｛
A［i］ = A［i］ *B［i］：/* S1 */
BI1J = A［1］ +c;/* $2*/
Alil = cfi］ *ci /* $3 */
c［i］= D［i*A［i］：/*S4*/
c.［15］<4.5>考虑以下循环：
for （i=0;i< 100;i++）｛
A［i】=A［i］ +B［i:/*S1*/
B［i+1］ = c［i］ +D［i;/*S2*/
S1 和S2之间是否存在相关？这一循环是否为并行的？如果不是，说明如何使其成为并行的。
4.15 ［10］<4.4>列出并介绍至少4种可以影响GPU 内核性能的因素。换句话说，哪些由内核代码导
致的运行时行为会降低内核执行时的资源利用率？
4.16 ［10］<4.4>假定一个虚设GPU 具有以下特性：
口 时钟频率为 1.5 GHz：
口包含16个 SIMD处理器，每个处理器包含16个单精度浮点单元；
口 片外存储器带宽为100 GB/S。
不考虑存储器带宽，假定所有存储器延迟可以隐藏，则这一GPU的峰值单精度浮点吞吐量为
多少 GFLOP/s？在给定存储器带宽限制下，这一吞吐量是否可持续？
4.17
［60］ <4.4>对于这一编程练习，写出并描述CUDA 内核的行为特征，这一内核中不仅包含大量
数据级并行，还包含条件执行行为。使用 NVIDIA CUDA 工.具套件和英属哥伦比亚大学的 GPU-
SIM （http://www.ece.ubc.ca/~aamodt/gpgpu-sim/或者 CUDA Profiler 来编写并编译 CUDA 内核，
对 256 ×256棋盘执行Conway“生命游戏”的100次迭代，并将棋盘的最终状态返回给主机。
假定这一棋盘由主机初始化。为每个单元格关联一个线程。确保在每次游戏迭代之后添加一个
屏障。使用以下游戏规则。
口 对于任意存活单元格，如果其相邻的存活单元格少于两个，则该存活单元格死亡。
案例研究与练习（Jason D. Bakos 设计）
255
口对于任意存活单元格，如果其相邻的存活单元格为两个或三个，则该存活单元格将生存到下
一代。
口 对于任意存活单元格，如果其相邻的存活单元格超过三个，则该存活单元格死亡。
口 对于任意死亡单元格，如果它恰有三个相邻的存活单元格，则该死亡单元格变为存活单元格。
在完成该内核后，回答以下问题。
2.［60］<4.4~-使用-pt 选项编译代码，并查看该内核的PTX 表示方式。有多少个 PTX 指令构成
了该内核的PTX实施方式？该内核的条件部分是否包含分支指令，还是仅有可预测的非分
支指令？
b.［60］<4.4>在模拟器上执行代码之后，动态指令数为多少？所实现的的每周期指令数（IPC）
为多少？或者说指令发射率为多少？在控制指令、算术辑单元（ALU）指令和存储器指令
方面，什么是动态指令分解？是否存在任何存储器组冲突？有效片外存储器带宽为多少？
c.［60］<4.4>对该内核进行改进，其中片外存储器引用被接合在一起，观察运行时性能的差别。
341
342
343