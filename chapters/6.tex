\chapter{以仓库级计算机开发请求级、数据级并行}
数据中心就是计算机。
—Luiz Andre Barroso，
Google （2007）
100多年前，各家公司不再自行用蒸汽机和发电机发电，而
是接入新建成的电网中。电力公用设施提供的廉价电能所改变的
内容并非仅限于这些公司的运行方式。这一现象导致了一连串的
经济与社会转型，拉开了现代世界的序幕。今天，一场类似的革
命正在进行之中。海量信息处理工厂挂在相互联网的全球计算网
格上，开始将数据和软件代码注入我们的家中和公司中。这一次
变 公用设施的东西是计算能力。
-Nicholas Carr，
《大变革：重新审视世界，从爱迪生到 Google》
（The Big Switch:Rewiring the World, from Edison to Google）（2008）
\section{引言}
谁都可以搭建一块快速CPU。难点在于如何搭建一个快速系统。
—Seymour Cray，
公认的超级计算机之父

仓库级计算机（WSC）®是许多人每日所用因特网服务的基础，这些服务包括：搜索、社
交网络、在线地图、视频共享、网上购物、电子邮件服务，等等。此类因特网服务深受大众喜
爱，从而有了创建 WSC的必要，以满足公众迅速增长的需求。尽管 WSC看起来可能就像是一
些大型数据中心，但它们的体系结构和操作有很大的不同，稍后我们将会看到。今天的 WSC
像是一个巨型机，其成本大约在1亿5千万美元量级，包括机房、配电与制冷基础设施、服务
器和联网设备，它们连接和容纳了50 000 至100000个服务器。此外，云计算的快速发展（见
6.5节）让每一个拥有信用卡的人都能使用WSC。

计算机体系结构很自然地扩展到 WSC 的设计中。例如，Google 公司 Luiz Barroso（前文
引用了他的话）的学位论文就是从事计算机体系结构方面的研究。他认为，架构师在设计过程
中实现可扩展性、提高可靠性的技巧以及调试硬件的技巧，对于创建和操作WSC都有很大的
帮助。

wSC 的规模已经达到一种极致，需要在配电、制冷、监控和运行等各个方面作出创新，它
是超级计算机的现代衍生物—使 Seymour Cray 成为当今WSC架构师的教父。他的极限计算
机可以处理一些在其他所有地方都无法完成的计算，但又是如此昂贵，只有很少几家公司可以
承受得起。而现在的目标是为整个世界提供信息技术，而不再是为科学家和工程师们提供高性
能计算（HPC）；因此，相对于 Cray的超级计算机在过去发挥的作用，WSC在当今社会中扮演
了更为重要的角色。

毫无疑问，WSC 的用户要比高性能计算的用户多出好几个量级，它们在 IT 市场占有的份
额也要大得多。无论是按用户数量计算还是按收入计算，Google 公司都至少是 Cray Research公
司的250倍。

WSC架构师有许多目标和需求与服务器架构师是一致的。

\begin{itemize}
    \item 成本-性能—每元钱能够完成的工作量是至关重要的，部分原因就是它的规模太大了。
    将 WSC的资本成本降低10\%就可能节省1500万美元。
    \item 能耗效率—配电成本与功率消耗具有函数关系，需要有充足的配电供给才能消耗功率。
    机械系统成本与功率具有函数关系，需要将导人的热能排出去。因此，峰值功率和消耗
    功率推高了配电与制冷系统两项成本。另外，能耗效率也是环境管理的一个重要组成部
    分。因此，单位焦耳完成的工作对于 WSC和服务器来说都至关重要，对于仓库级计算
    机来说，主要是因为建造电力与机械基础设施的成本很高，对于服务器而言，则是因为
    每月的公共供电账单费用很高。
\end{itemize}

\begin{verbatim}
    ① 本章基础资料的来源包括：Google公司 Luiz Andre Barroso、Urs H&lzle 所著的 The Datacenter as a Computer: An
    Introduction to the Design of Warehouse-Scale Machines［2009］；Amazon Web 服务部门 James Hamilton 在 mvdirona.
    com 的博客 Perspectives 和访谈 “Cloud-Computing Bconomies ofScale" 以及 "Data Center Networks Are in My Way”
    ［2009, 2010］：Michael Armbrust 等人的技术报告 “Above the Clouds: A Berkeley View of Cloud Computing” ［2009］。
\end{verbatim}

\begin{itemize}
    \item 通过冗余提高可靠性—因特网服务的性质要求其必须长时间运行，这就意味着 WSC
    中的硬件和软件在整体上至少提供99.99\%的可用性；也就是说，它每年的宕机时间必须
    低于1小时。对于 WSC和服务器来说，冗余都是提高可靠性的关键。服务器架构师经
    常利用成本更高的更多硬件来实现高可用性，而WSC架构师则依靠多个更具成本效率
    的服务器，这些服务器用低成本网络连接在一起，由软件实现冗余管理。另外，如果可
    用性目标远远超过“四个九”，那就需要利用多个 WSC来屏蔽可能摧毁整个 WSC的事
    件。对于广泛部署的服务，还可以利用多个 WSC降低延迟。
    \item 网络 1/O一服务器架构师必须提供一个连向外部世界的出色网络接口，WSC架梅也
    必须如此。为保持多个 WSC之间的数据一致性，并能够与公众进行交互，需要进行
    联网。
    \item 交互式与批处理工作负载——对于诸如搜索和社交网络等拥有数百万用户的服务，人们
    期望它们的工作负载具有很强的交互性，与此同时，WSC与服务器类似，还运行着大量
    并行批处理程序，用以计算对此类服务有用的元数据。例如，它们可以执行 MapReduce
    作业，将通过爬网返回的页面转换为搜索索引（见6.2节）。
    当然，WSC也有一些不同于服务器体系结构的特性。
    \item 充足的并行—服务器架构师的一个顾虑是：目标市场中的应用程序是否有足够的并行
    度，以充分发挥大量并行硬件的功用；为开发这一并行，需要有足够的通信硬件，其成
    本是否过高。WSC架构师没有此类顾虑。首先，批处理应用程序获益于大量需要独立处
    理的独立数据集，比如爬网时的数十亿个 Web 页面。这一处理过程就是数据级并行，其
    适用于存储（storage）中的数据，而不是存储器（memory）中的数据，我们在第4章介
    绍过。第二，交互式因特网服务应用程序（也称为软件即服务，Saas）可获益于交互式
    因特网服务数以百万计的独立用户。在 SaaS 中，读取与写入很少是相关的，所以 Saas
    很少需要同步。例如，搜索服务使用的是只读索引，而电子邮件通常是读取与写人相独
    立的信息。我们将这种简单并行称为请求级并行，有大量独立工作可以很自然地并行开
    发，几乎不需要通信或同步；例如，基于日志的更新过程可以降低吞吐量需求。由于Saas
    和 WSC所取得的成功，很多传统应用程序（比如关系数据库）已经被弱化，转而依靠
    请求级并行。为使所提供的存储能够扩展到现代WSC的规模，甚至删除了一些读/写相
    关的特征。
    \item 运行成本计算—服务器架构师通常是在成本预算内使设计的系统实现峰值性能，对于
    功率的唯一顾虑就是确保它们不会超出机柜的冷却能力。他们通常会忽略服务器的运行
    成本，假定其相对于购买成本是微不足道的。WSC的寿命要长于服务器—机房、配电
    和制冷基础设施经常要使用10年以上，所以运营成本也不可小视：在10年中，能源、
    配电和制冷方面的费用占 WSC成本的30\%以上。
    \item 规模、与规模相关的机会/问题—通常，极端计算机都是极端昂贵的，因为它们需要定
    制硬件，但又因为极端计算机的制造数目很低，所以就无法有效地分摊定制成本。不过，
    如果我们购买 SO000台服务器和相关基础设施，用来构造一个WSC，那的确可以获得
    总额折扣。由于 WSC本身就非常庞大，所以即使没有太多WSC，也可以实现规模经济
    效应。在6.5节和6.10节将会看到，这些规模经济导致了云计算的出现，这是因为WSC
    的单位成本较低，也就是说，一些公司可以向外租借这些设施，其收费低于租借者自行
    完成这些工作所需要的成本。50000台服务器的不利之处就是容易发生故障。图6-1给
    出了2400台服务器的停用与异常情况。即使一台服务器的平均无故障时间（MTTF）达
    到了令人惊叹的25年（200000小时），WSC架构师在进行设计时也要应对每天有5台
    服务器发生故障的情况。表6-1列出的年度磁盘故障率为2\%~10\%。如果每台服务器有
    4块硬盘，它们的年度故障率为4\%，那拥有50000台服务器时，WSC架构师预计每一
    小时就会看到一块磁盘发生故障。
\end{itemize}
表6-1一个由2400台服务组成的新集群在第一年发生的停用与异常，并给出了近似的发生频率
第1年的近似事件数
原因
1或2
电力设施故障
结果
整个WSC失去供电，如果UPS和发电机正常工作（发电器的正常工作
时间大约占总时间的99%），不会导致WSC岩机
计划内停用，用于升级基础设计，许多时间是出于升级网络的需求，
4
集群升级
比如重新连接线缆、交换机固件升级，等等。每一次计划外停用大约
会有9次计划内集群停用
硬盘故障
2%-10%的年度磁盘故障率［inheiro2007］
磁盘缓慢
仍然能够运行，但运行速度减緩10-20倍
1000s
存储器损坏
每年一次不可糾正的DRAM错误［Schroeder等人，2009］
机器配置错误
配置会导致大约30%的服多中断［Barroso和HOlzle 2009］
脆弱的机器
大約1%的服多器毎星期重启一次以上［Barroso和HOIzle2009］
5000
个别服务器崩溃
机器重启，通常需要大约5分钟
*我们将GO0gle所说的集群标记为阵列；见围6-2。（据Barroso ［2010］。）

例题
解答
一个服务运行在表6-1中的2400台服务器上，试计算该服务的可用性。本例题中
的服务与实际WSC 中的服务不同，它不能容忍硬件或软件故障。假定重启软件的
时间为5分钟，修改硬件的时间为1小时。
通过计算因为每个组件发生故障所导致的停用时间，可以用来估计服务可用性。
我们保守地取表6-1 中每个类别的最低数字，将1000次停用平均分配在4个组件
之间。我们忽略了运行缓慢的磁盘（1000次停用的第二个分量）和电力设施故障，
这是因为磁盘缓存会影响性能但不会影响可用性，而99%的电力设施故障可以通
过不间断电源（UPS）系统加以隐藏。
停用时间服务=（4+250+250+250）×1 小时＋（250+5000）×5分钟
=754+438=1192小时
一年有365 × 24=8760小时，所以可用性为：
可用性 =（8760-1192）二
7568
-=86%
8760
8760
即，如果没有软件冗余来屏蔽如此之多的停用次数，在这2400台服务器上运行的
服务将会达到平均每周一天的宕机时间，其可用性只能达到“零个九”的級别了！

在6.10节将会解释，WSC的先驱是计算机集群。集群是一组使用标准局域网（LAN）和
商用交换机连接在一起的独立计算机。对于不需要大量通信的工作负载，集群计算的成本效率
要远高于共享存储器多处理器。（共享存储器多处理器是第5章所讨论的多核计算机的先驱。）
集群在 20世纪90年代后期开始流行，先用于科学计算，后来用于互联网服务。关于WSC有
这样一个观点：它们就是过去数百个服务器组成的集群向今天数万个服务器所组成集群的逻辑
发展。

人们自然会问：WSC 是否与高性能计算（HPC）使用的现代集群类似。尽管有一些 WSC
和 HPC 的规模与成本相近（有些HPC设计拥有数百万个处理器，花费数亿美元），但HPC的
处理器和节点之间的网络通常要比WSC中快得多，这是因为HPC应用程序的独立性更强，通
信更频繁（见6.3节）。HPC设计还倾向于使用定制硬件（特别是在网络中），所以它们通常不
能通过使用大众化商用芯片来降低成本。例如，单是 IBM Power 7微处理器的成本和耗费的功
率就高于 Google WSC中的一个完整服务器节点。其编程环境还强调线程级并行或数据级并行
（见第4章、第5章），通常强调完成单项任务的延迟，而不是通过请求级并行完成许多独立任
务的带宽。HPC集群往往还拥有长时间运行的作业，它们会使服务器满负荷运行，甚至能持续
数周以上，而WSC中服务器的利用率通常在10\%~50\%之间（见图6-1），而且每天都会发生
变化。

WSC 与传统数据中心相比又怎么样呢？传统数据中心的运营商通常会从组织的许多部门
收集机器和第三方软件，并集中为他人运行这些机器和软件。它们的关注点通常是将许多服务
整合到较少的机器中，这些机器相互隔离，以保护敏感信息。因此，虚拟机在数据中心的重要
性日益增加。与 WSC 不同的是，传统数据中心往往拥有各种不同的硬件和软件，为一家组织
中的各种不用客户提供服务。WSC程序员则定制第三方软件或者自行开发软件，WSC的硬件
一致性要强得多；WSC 的目标是让仓库中的硬件/软件看起来就像是只有一台计算机，只是上
面运行着种不同的应用程序。传统数据中心的最大成本通常是维护人员的费用，而在6.4节
将会看到，在设计完善的 WSC中，服务器硬件是最大的成本，人力成本从最大成本变得几乎
可以忽略。传统数据中心也不具备 WSC的规模，所以它们无法获得上述的规模经济效益。因
此，尽管你可能会将 WSC 看作一种超级数据中心（因为这些计算机都分别放在具有特殊配电
和制冷基础设施的空间内），但典型的数据中心通常没有 WSC面对的挑战和机遇，无论是体系
结构方面还是运转方面都是如此。

由于很少有架构师了解WSC中运行的软件，所以我们首先介绍WSC的工作负载和编程模型。

\section{仓库级计算机的编程模型与工作负载}
如果一个问题没有解决的办法，那它可能就不是一个问题，而是一个事实——不是找
到其解决办法，而是随着时间的推移找到应对方法。
—-Shimon Peres

一些面向公众的因特网服务，比如搜索、视频共享和社交网络等，使WSC有了名气，除
了这些服务之外，WSC还运行一些批处理应用程序，比如将视频转换为新的格式，或者通过爬
网生成搜索索引。

今天，WSC 中最流行的批处理框架是 MapReduce［Dean 和 Ghemawat2008］和它的开源孪生
框架 Hadoop。表6-2显示了 Google公司 MapReduce 的流行程度日益提高。（据估计，2011年拥
有60 000台服务器，Facebook 在其中的 2000台批处理服务器上运行 Hadoop。）受同名 Lisp 函
数的启发，Map 首先将程序员提供的函数应用于每条逻辑输人记录。Map 在数千台计算机上运
行，生成由键/值对组成的中间结果。Reduce 收集这些分布式任务的输出，并使用另一个由程序
员定义的函数来分解它们。通过适当的软件支持，这两者都是高度并行的，而且其理解和使用
都很容易。在30分钟之内，程序员新手就可以在数千台计算机上运行 MapReduce 任务。

表6-2 Google公司MapReduce的年度使用数据
2004年8月
2006年3月
2007年9月
MapReduce作业数
29 000
171 000
2217000
平均完成时间（秒）
634
874
395
所用的服务器年数
217
2002
11 081
读取的输入数据（TB）
3288
52 254
403 152
中间数据（TB）
758
6743
34774
写出的输出数据（TB）
193
2970
14018
每项作业的平均服务器数
157
268
394
2009年9月
3467000
475
25 562
\$44 130
90 120
$7 $20
488

*在5年的时间里，MapReduce 作业数增加了100倍，每项作业的平均服务器数增长了3倍。在后两年中，增长因素
分别为1.6 和 1.2［Dean 2009］。表6-9估算：在 Amazon 的云计算服务EC2上执行2009 工作负載时，将会耗费1.33亿
美元。

例如，一个 MapReduce程序计算一大组文档中每个英文单词的出现次数。下面是这个程序
的简化版本，仅给出了内层循环，并假定所有英文单词仅在文档中出现一次［Dean 和 Ghemawat
2008］：

\begin{verbatim}
    map（String key, String value）：
    // key： 文档名
    // value； 文档内容
    for each word w in value：
    EmitInternediate（w，
    "1"）；// 光所有单词生成清单
    reduce（String key, Iterator values）：
    // key：一个单词
    // values：一个计数清单
    int result = 0；
    for each v in values：
    result += ParseInt（v）： // 从键值对中取得整数
    Emit（AsString（result））；
\end{verbatim}
Map 函数中使用的 EnitIntermediate 函数给出文档中的每个单词，并取值1。然后，Reduce 函数
使用 ParseInt（对每个单词在每篇文档中所有取值求和，得出每个单词在所有文档中的出现次数。
MapReduce 运行时环境将 map 任务和 reduce任务调度到WSC的节点中。（这个程序的完整版本
可以在 Dean 和 Ghemawat［2004］的参考文献中找到。）

MapReduce 可以看作单指令多数据（SIMD）操作（见第4章）的推广（只有一点不同：我
们向它传递了一个将应用于数据的函数），这一操作后面跟有一个函数，用于对Map 任务的输
出进行约简操作（reduction）。因为约简在 SIMD程序中很常见，所以 SIMID硬件经常会为它们
提供特殊操作。例如，Intel最近的 AVX SIMD 指令包含了“横向"（horizontal） 指令，对寄存
器中相邻的操作数对进行相加。

为了适应数千台计算机的性能变化，MapReduce 调度程序根据各个节点完成先前任务的速
度来分配新的任务。显然，哪怕只有一个速度缓慢的任务，也可能会阻挡大型 MapReduce 作业
的完成。在WSC中，对缓慢任务的解决办法是提供—一种软件机制，以应对这一规模所固有的
这种性能变化。这种方法与传统数据库中心中服务器采取的解决方案截然不同，在后一解决方
案中，任务缓慢通常意味着硬件损坏，需要替换，或者服务器软件需要调优或重写。对于WSC
中的50000 台服务器来说，性能出现差异是正常现象。例如，当 MapReduce 程序快结束时，系
统将开始在其他节点上备份那些尚未完成的任务，并从那些首先完成的任务中获取结果。在将
资源利用率提高几个百分点之后，Dean和 Ghemawat［2008］发现一些大型任务的完成速度可以加
快30%。

另一个说明 WSC差异的例子是使用数据复制来应对故障。由于 WSC中的设备如此之多，
经常发生故障并不是什么让人惊讶的事情，上个例子就证明了这一点。为了实现 99.99%的可用
性，系统软件必须能够应对 WSC中的这一现实问题。为了降低运行成本，所有 WSC都使用自
动监视软件，使一位操作员可以负责1000多台服务器。

编程框架（比如用于批处理的 MapReduce）和面向外部的 Saas（比如搜索）依靠内部软件
服务才能成功运行。例如，MapReduce 依靠 Google 文件系统（GFS）（Ghemawat、Gobioff和
Leung ［2003］）向任意计算机提供文件，因此，可以将 MapReduce 任务调度到任意地方。

除了 GFS之外，此类可伸缩存储系统的示例还包括 Amazon的键值存储系统 Dynamo
［DeCandia 等人2007］和 Google 记录存储系统 Bigtable ［Chang 2006］。注意，此类系统经常是相
互依赖的。例如，Bigtable 将其日志和数据存储在GFS 中，就像是关系数据库可以利用内核操
作系统提供的文件系统。

这些内部服务作出的决定经常不同于类似软件在单个服务器上运行时作出的决定。例如，
这些系统并没有假定存储是可靠的（比如使用RAID 存储器系统来保证其可靠性），而是经常生
成数据的完整副本。制作副本有助于提高读取性能和可用性，通过正确放置这些副本，可以解
决许多其他系统故障，比如表6-1 中列出的一些。某些系统采用删除编码，而不是完整复制，
但有一点是不变的，那就是实现跨服务器冗余，而不是实现服务器内部或存储阵列内部的冗余。
因此，整个服务器或存储设备发生故障时，不会对数据可用性产生负面影响。

还有另外一个例子说明 WSC中采用的不同方法：WSC存储软件经常使用宽松一致性，而
没有遵循传统数据库系统的所有 ACID（原子性、一致性、隔离性和持久性）需求。这里的重
点是：数据的多个副本最终一致才是最重要的，而对于大多数应用程序来说，它们并不需要在
所有时间都保持一致。例如，视频共享只需要最终保持一致就行。最终一致性大大简化了系统
的扩展，而扩展绝对是 WSC必不可少的要求。

这些公共交互式服务的工作负载需求都会有大幅波动，即使是Google搜索这样流行的全球
性服务，在一天中的不同时间也可能会有两倍的变化幅度。如果针对某些应用程序考虑周末、
假日和一年中高峰时间等因素（比如万圣节之后的图片共享服务，或者圣诞节之前的网上购物
活动），将会看到提供因特网服务的服务器在利用率方面都波动较大。图 6-1给出了5000台
Google 服务器在6个月时间内的平均利用率。注意，不到 0.5\%的服务器平均利用率达到 100%，
大多数服务器的利用率介于10\%至50\%之间。换句话说，利用率超过50\%的服务器仅占全部服
务器的10\%。因此，对于 WSC 的服务器来说，在工作负载很低时的良好表现要远比峰值负载
时的高效运行重要得多，因为它们很少会在峰值状态下运行。

0.03
0.025
0.02
0.015
0.01
0.005
0
0
0.1
0.2
0.3
0.4
0.s 0.6 0.7
0.8
0.9
1.0
CPU利用率
图 6-1

Google 公司 5000多台服务器在6个月时间内的平均 CPU 利用率。服务器很少是完全空闲或全
负荷工作的，在大多数时间里，介于其最大利用率的10%到50%之间。（据 Barroso和.Htlzle
［2007］）。表6-3 右侧第三列计算了百分比并加减5%，以提供权值；因此，90%一行的1.2%表示
有1.2%的服务器达到了85%至95%的利用率

总之，WSC硬件和软件必须能够应对因为用户需求所造成的负载变化，以及因为硬件在这
一规模的各种变化而造成性能和可靠性的变化。

例题
解菩
负載
100%
90%
80%
70%
60%
50%
针对不同的负载情况，利用SPECPower 基准测试对功率和性能进行测试，负载变
化范围为0%~100%，每次递增10%（见第1章），测量结果如图6-1所示。可以
用一个整体度量来总结这一基准测试的结果，即：将所有性能测试值（单位：服
务器端每秒执行的 Java 操作数）之和除以所有功率测量值（单位：瓦）之和。因
此，每个级别的可能性相等。如果利用图6-1 中的利用频率对这些级别进行加权，
这一数字汇总数据将如何变化？

表6-3给出了与图6-1匹配的原权重和新权重。这些权重将性能汇总值降低了30%，
由3210 $ssi_ops/W$ 降低到 $2454ssj_ops/W$。

表6-3 来自表6-10的SPECPower结果，使用图6-1中的权重，而非平均权重
\begin{verbatim}
    性能
    瓦
    SPEC权簠
    加权后的 加权后的
    倒6-1的权蘆
    性能
    瓦数
    2 889 020
    662
    9.08%
    262 638
    60
    0.80%
    2611 130
    617
    9.09%
    237375
    56
    1.20%
    2319900
    $T6
    9.09%
    210900
    $2
    1.50%
    2031 260
    533
    9.09%
    184 660
    48
    2.10%
    1740980
    490
    9.09%
    158 271
    45
    5.10%
    1448 810
    451
    9.09%
    131 710
    41
    11.50%
    加权后的性能
    22 206
    31 756
    35 889
    42491
    88 082
    166 335
    加权后的
    瓦数
    8
    11
    25
    $2
    6.3
    仓库级计算机的计算机体系结构
    327
    伍载
    40%
    30%
    20%
    10%
    0%
    总数
    性能
    1 159 760
    869 077
    581 126
    290 762.
    o
    15 941 825
    瓦
    416
    382
    351
    308
    181
    4967
    SPEC杈置
    9.09%
    9.09%
    9.09%
    9.09%
    9.09%
    加权后的
    性能
    105 433
    79007
    52.830
    26 433
    1449 257
    ssi_ops/w
    加权后的
    瓦数
    38
    35
    32
    28
    16
    452
    3210
    图6-1的权重
    19.10%
    24.60%
    15.30%
    8.00%
    10.90%
    加权后的性能
    221 165
    213 929
    88 769
    23 198
    0
    933 820
    ssj_ops/W
    （续）
    加权后的
    瓦数
    79
    94
    54
    25
    20
    380
    2454
\end{verbatim}
由于规模的原因，软件必须能够应对故障，这就意味着没有什么理由再去购买那些降低故
障频率的“镀金”硬件。它只能是提高成本。Barroso和 Holzle ［2009］发现，在运行TPC-C数据
库基准测试时，高端共享存储器多处理器与大众化 HP 服务器的价格性能比相差20倍。可以想
到，Google 公司购买的都是低端大众化服务器。

此类 WSC服务还倾向于开发自己的软件，而不是购买第三方商用软件，部分原因是为了
应对这种庞大的规模，部分原因是为了节省资金。例如，即使是在2011年TPC-C的最佳性价
比平台上，将 Oracle 数据库和 Windows 操作系统的成本包含在内时，也会使 Dell Poweredge 710
服务器的成本加倍。相反，Google 在其自己的服务器上运行 Bigtable 和 L.inux 操作系统，无须
为其支付版权费用。

在对WSC中的应用程序和系统软件进行以上简单综述后，现在可以开始研究WSC的计算
机体系结构了。

\section{仓库级计算机的计算机体系结构}
网络是将50000台服务器连接在一起的结缔组织。类似于第2章的存储器层次结构，WSC
使用一种网络层次结构。图6-2给出了一个示例。理想情况下，这种组合网络提供的性能应当
接近于S0000台服器定制的高端交换机，而每端口的成本则接近于为50台服务器设计的
大众化交换机。在6.6节可以看到，目前的解决方案与理根情况相去甚远，WSC的网络是一个
活跃的探索领域。

19英寸（48.26厘米）机架仍然是容纳服务器的标准框架，尽管这一标准要追溯到20世纪
30年代的铁路硬件。服务器的大小按照它们在机架内占用的机架单元（U）数计算。1U高1.75
英寸（4.45厘米），这是一个服务器可以占用的最小空间。

7英尺（213.36厘米）的机架提供48U，因此，针对一个机架生成的最流行交换机都是48
端口以太网交换机，这就不是巧合了。这一产品已经成为—种大众化商品，2011年，1 Gbit/s
以太网连接的每端口成本只有30美元［Barroso 和 Holzle2009］。注意，机架内的速度对于每个
服务器都是一样的，所以软件将发送器和接收器放在哪个位置都没关系，只要它们位于同一机
架就行。从软件的角度来看，这种灵活性是很理想的。

这些交换机通常提供2~8个上行链接，它们离开机架，进人网络层次结构的下一层更高阶
交换机。因此，离开机架的速度是机架内速度的1/6到1/24（8/48到2/48）。这一比值称为超额认
购率（oversubscription）。当超额认购率很高时，程序员必须知道将发送机和接收机放在不同机架
时导致的性能后果。这样会增大软件调度负担，这是支持数据中心专用网络交换机的另一个理由。

阵列
交换机
机架
交换机
1U服务器
机架
图6-2 WSC 中的交换机层次结构。（基于 Barroso和 Holzle［2009］论文中的图1-2）
442

\subsection{存储}
一种很自然的设计是用服务器填充一个机架，当然要扣除大众化以太网机架交换机所需要
的空间。这种设计带来一个问题：把存储放在哪儿。从硬件构建的角度来看，最简单的解决方案
是将磁盘包含在服务器中，通过以太网连接访问远程服务器磁盘上的信息。另一种替代方案是使
用网络连接存储（NAS），可能是通过类似于 Infiniband 的存储网络。NAS解决方案的每TB存储
容量成本通常要更高一些，但它提供了许多功能，包括用于提高存储器可靠性的RAID技术。

根据上一节表达的思想，可以预计：WSC 通常会依靠本地磁盘，并提供用于处理连接性和
可靠性的存储软件。例如，GFS使用本地磁盘，至少维护三个副本，以克服可靠性问题。这一
冗余不仅可以应对本地磁盘故障，还能应对机架和整个集群的电源故障。GFS的最终一致灵活
性降低了使这些副本保持一致所需的成本，还降低了存储系统的网络带宽需求。稍后将会看到，
本地访问模式还意味着连向本地存储的高带宽。

要小心：在讨论 WSC的体系结构时，对于集群一词有一点混淆。根据6.1节的定义，WSC就
是一个超大型集群。而 Barroso 和 Holzle ［2009则用集群一词来表示更大一级的计算机组，在本例
中大约为30个机架。在本章中，为了避免混淆，我们使用阵列一词来表示一组机架，使集群一词
保持其最初含义，既可以表示一个机架内的联网计算机组，也可以表示整整-一仓库的联网计算机。

\subsection{阵列交换机}

将一个机架阵列连在一起的交换机要比48口大众化以太网交换机贵得多。之所以出现这种
高成本，部分原因是由于较高的连通性，部分原因是为了减轻超额认购问题，必须大幅增大通
过交换机的带宽。据Barroso和H8lzle［2009］报告，如果一个交换机的二分带宽（基本上就是最
糟情況下的内部带宽）是机架交换机二分带宽的10倍，那其成本将大约为100倍。其中一个原
因是：n端口交換机带宽的成本成长速度与：成比例。

这种高成本的另一原因是这些产品为其生产公司提供了很高的利润率。为了证明这种高价
格的正当性，这些公司的一种做法就是提供一些诸如数据包检测之类的昂贵功能，这些功能之
所以昂贵，是因为它们必须以极高速率运转。例如，网络交换机是内容可寻址存储器芯片和现
场可编程门阵列（FPGA）的主要用户，这样当然有助于提高这些功能，但这些芯片本身是很昂
贵的。这些功能在因特网设置上可能很有价值，但一-般不会在数据中心中使用。

\subsection{WSC存储器层次结构}
表6-4给出了 WSC 内部存储器层次结构的延迟、带宽和容量，图6-3以可视方式显示了同
一数据。这些数字基于以下假设［Barroso 和 Holzle 2009］。

表6-4 WSC存储层次结构的延迟、带宽和容量［Barroso和HOlzle 2009］
本地
架
DRAM延迟（ms）
磁盘延迟（ms）
DRAM带宽（MB/s）
磁盘带宽（MB/s）
DRAM容量（GB）
磁盘容量（GB）
*图6-3绘制了这一相同信息。
10000 000
0.1
10000
20 000
200
16
2000
机
100
11000
100
100
1040
160000
列
阵
300
12 000
10
10
31200
4 800 000
443
磁盘容量（CB）
1 000 000
100 000
10000
1000
100
10
磁盘带宽（MB/s）
DRAM容量（GB）
磁盘延迟H（TS）
DRAM延迟（IS）
磁盘带宽
（MB/s）
DRAM带宽 （MB/s）
0
本地
机架
阵列
图 6-3 WSC 存储层次结构的延迟、带宽和容量数据曲线，数据与表6-4中相同［Barroso 和 Holzle 2009］

\begin{itemize}
    \item 每个服务器包含16GB存储器，访问时间为100ns，传输速度为20GB/s，还有一个2TB
    磁盘，访问时间为10 ms，传输速度为200MB/s。每块主板上有两个插槽，它们共享一
    个1Gbits 以太网端口。
    \item 每对机架包括一个机架交换机，容纳80个 2U服务器（见6.7节）。联网软件再加上交换
    机开销将到 DRAM 的延迟增加到100 ms，磁盘访问延迟增加到11 ms。因此，一个机架
    的总存储容量大约为1TB的 DRAM和160 TB的磁盘存储。1 Gbit/s 以太网将连向该机
    架内的 DRAM或磁盘的远程带宽限制为100 MB/S。
    \item 阵列交换机可以处理30个机架，所以一个阵列的存储容量增加30倍：DRAM为30TB，磁
    ．盘为 4.8PB。阵列交换机硬件和软件将连向阵列内 DRAM的延迟增加到 500 ms，磁盘延迟
    增加到12 ms。数据交换机的带宽将连向阵列DRAM或阵列磁盘的远程带宽限制为10 MB/s。
\end{itemize}

表6-4 和图 6-3显示：网络开销大幅延长了从本地 DRAM 到机架 DRAM 和阵列 DRAM的
延迟，但这两者的延迟性能仍然优于本地磁盘，不到其十分之一。网络消除了机架 DRAM与机
架磁盘之间、阵列 DRAM 和阵列磁盘之间的带宽差别。

这个 WSC需要20个阵列连接到50 000个服务器，所以联网层次结构又多了一级。图6-4
给出了用于将阵列连接在一起并连至因特网的传统L3路由器。
因特网
数据库L3
L2
关键字
CR =L3核心路由器
AR=L3访问路由器
S=阵列交換机
L.B=负载均衡器
A=包括80台服务器的机
架，带有机架交換机

图6-4 用于将阵列连接在一起并连接到困特网的L3 网络［Greenberg 等人，2009】。一些 WSC使用独立
的边界路由器将因特网连接到数据中心L3交换机

大多数应用程序可以放在 WSC 中的单个阵列上。那些需要多个阵列的应用程序会使用分
片或分区，也就是说将数据集分为独立片断，然后再分散到不同阵列中。对整个数据集执行的
操作被发送到托管这些片断的服务器，其结果由客户端计算机接合起来。

例题
假定90%的访问为服务器的本地访问，9%的访问超出服务器但在机架范围内，1%
的访问超出机架但在阵列范围内，则平均存储器延迟为多少？
解答
平均存储器访问时间为：
（90% × 0.1）+（9% × 100）+（1% ×300）=0.09+9+3=12.09微秒
6.4 仓庫级计算机的物理基础设施与成本
331
或者说，比100%的本地访问减缓120倍以上。显然，实现一个服务器内的访问局
域性对于 WSC 性能来说是至关重要的。
例题
在服务器内部的磁盘之间、在机架内的服务器之间、在阵列中不同机架内的服务器
之间，传递1000 MB 需要多少时间？在这三种情况下，在 DRAM之间传递 1000MBB
可以加快多少时间？
解答
在磁盘之间传递1000MB 需要的时间为：
在服务器内部=1000/200=5秒
在机架内部=1000/100=10秒
在阵列内部=1000/10=100秒
在存储器之间传送块时需要的时间为：
在服务器内部=1000/20000=0.05秒
在机架内部=1000/100=10秒
在阵列内部=1000/10=100秒

因此，对于单个服务器外部的块传输而言，由于机架交换机和阵列交换机是瓶颈
所在，所以数据是在存储器中还是磁盘中并不重要。这些性能限制影响了 WSC软
件的设计，并激发了对更高性能交换机的需求（见6.6节）。

知道了T设备的体系结构，我们现在可以看看如何对其进行摆放、供电和冷却，并讨论构
建和运行整个 WSC的成本，与其中IT设备本身的成本进行对比。

\section{仓库级计算机的物理基础设施与成本}
要构建WSC，首先需要建造一个仓库。第一个问题就是：在哪里建造？房地产代理强调位
置，但对 WSC来说，位置意味着接近因特网骨干光纤、电力成本低、环境灾难风险低（比如
地震、洪水和飓风）。对于一个拥有许多WSC的公司来说，另一个关注点是找到一个在地理上
接近当前或未来因特网用户群的地方，以降低通过因特网的延迟。还有其他许多现实考虑因素，
比如不动产税率。

配电与制冷的基础设施成本会让 WSC的构造成本相形见绌，所以我们把主要精力放在前
者。图6-5和图6-6给出了WSC内的配电与制冷基础设施。

尽管有许多不同的部署方式，但在北美洲，从 115千伏的电力塔高压线开始，电力通常经
过大约5个步骤、四级电压变换到达服务器。
\begin{enumerate}
    \item 变电站从115 000伏转换为13200伏的中压线，效率为99.7%。
    \item 为防止整个 WSC在停电时离线，WSC和一些服务器一样，配备有不间断电源（UPS）。
    在这种情况下，其中涉及大型柴油机，可以在发生紧急状况时将供电任务从电力公司接管过来，
    还会涉及电池或惯性轮，用于在电力服务中止但柴油机尚未准备就绪时维持供电。这些发电机
    和电池可能占用很大的空间，通常将它们和 IT设备放在不同的房间中。UPS扮演着三个角色：
    电力调整（保持正常的电压和其他性能指标）、在发电机启动并正常供电时保持电气负载、在从
    发电机切换回公共用电时保持电气负载。这种超大型 UPS 的效率为94%，所以使用 UPS 会损
    失6\%的电能。WSC UPS 可能占全部 TT设备成本的7%~12%。
    \item 系统中的下一个组件是配电单元（PDU），它将转換为480伏的三相内部低压电源。这
    一转换效率为 98%。典型的PDU 可以承载75~225千瓦的负载，相当于10个机架的负载。
    \item 还有另外一个下变换步骤，将其转换为服务器可以使用的208伏两相电源，其效率也是
    98%。（在服务器内部，还有另外一些步骤，将电压降到芯片可以使用的级别；见6.7节。）
    \item 连接器、断路器和至服务器电气连接的整体效率为99%。
\end{enumerate}

高压公用配电
发动机
IT负载（服务器.
存储、网络⋯⋯）
115KV
子站
13.2KV
UPS旋转发电或电池
UPS和发电机的
电压通常为480V
变压器
>
2K
变压器
大約1%损耗在开
关装置和导体上
0.3%损耗
99.7% 效率
在8MW设施中的溢出与蒸发
损耗：约200 000加仑/天
13.2KV
13.2KV
480
6% 损耗
2% 搅耗
94%效率，大约97%可用
98% 效率
2% 损耗
98% 效率
图6-5 配电与发生损耗的地方。注意，最佳改进值为11%（据 Hamilton［2010］o）
热交换器
（节水装置）
士系
冷却塔
CWS泵
A/C冷凝器
A/C蒸发器
A/C压缩机
服务器风扇，每6~9
计算机室
空气处理器
空气推动器
图6-6

制冷系统的机制设计。CWS 表示循环水系统（据 Hamilton［2010］。）

北美之外的WSC使用不同的变换值，但整体设计是类似的。

总而言之，将来自公共用电的115 千伏电源转换为服务器可以使用的208伏电源，其效率
为 89%：
99.7%×94%×98%×98%×99%=89%
这一总效率仅留下10%多一点的改进空间，但后面将会看到，工程师们仍然在尝试锦上添花。

制冷基础设施中的改进机会要大得多。计算机室空调（CRAC）单元使用冷却水来冷却服
务器室内的空间，类似于冰箱通过向外部释放热量来降低温度。当液体吸收热量时，它会蒸发。
相反，当液体释放热量时，它会冷凝。空调机将液体注人低压螺旋管中，使其蒸发并吸收热量，
然后再将其发送到外部冷凝器，在这里释放热量。因此，在CRAC 单元中，风扇吹动热空气，
穿过一组装有冷水的旋管，水泵将加热后的水移到外部冷凝器进行冷却。服务器的冷却空气通
常介于64F与71之间（18°C和22°C之间。）图6-6显示了一大组风扇和水泵，使空气和水在
整个系统中流动。

显然，提高能耗效率的最简单方法之一就是让IT设备在较高温度下运行，降低冷却空气的
需求。一些 WSC在远高于71（22°C）的温度下运行自己的设备。

除了冷凝器之外，在一些数据中心中还会利用冷却塔，先充分利用外部的较冷空气对水进
行冷却，然后再将初步冷却后的水发送给冷凝器。真正要紧的温度称为湿球温度。在测量湿球
温度时，会向一个温度计沾有水的球形未端吹动空气。这是通过空气流动蒸发水分所能达到的
最低温度。

温水流过冷却塔的一个大面积表面，通过蒸发将热传递给外部空气，从而使水冷却。这种
技术被称为经济化供风。一种替换方法是使用冷水，而不是冷空气。Google 公司在比利时的
WSC使用一种“水到水”中间冷却器，从工业导管取得冷水，用于冷却来自 WSC内部的热水。

IT设备本身的空气流进行了仔细规划，有些设计甚至还使用了空气流仿真器。高效的空气
流设计可以减少冷空气与热空气的混合机会，从而保持冷空气的温度。例如，在机架上摆放服
务器时，可以让相邻行中的服务器朝向相反方向，使排出的高温废气吹向相反方向，从而使
WSC的热空气通道和冷空气通道交替存在。

除了能量损耗之外，冷却系统还会因为蒸发或溢人下水道的原因而消耗大量水。例如，8兆瓦
设施每天可能使用70000到200000加仑的水。

在典型数据中心中，制冷系统与 IT设备的相对功率成本［Barroso 和 H6lzle 2009］如下：

\begin{itemize}
    \item 冷凝器占 IT设备功率的30%~50%；
    \item CRAC 占 IT设备功率的10%~20%，大多消耗在风扇上。
\end{itemize}

让人奇怪的是，在减去配电设备与制冷系统的开销之后，仍然不能很清楚地看出一个 WSC
可以支持多少服务器。服务器制造商提供的所谓铭牌额定功率总是很保守的；它是一个服务器
可能消耗的最大功率。因此，第一步就是在 WSC中可能部署的各种工作负载下对一台服务器
进行测量。（联网设备通常占总功耗的5%，在开始时可以忽略。）

为了确定 WSC的服务器数目，可以将 TT 的可用功率除以测得的服务器功率；但是，根据
Fan、Weber和 Barroso［2007］所说，这同样太过保守了。他们发现，由于很少有真正的工作负载
可以让数千台服务器同时工作于峰值状态，所以数千台服务器理论上在最糟情况所做的工作与
它们实际所做的工作之间有很大的差距。他们发现，根据单个服务器的功率，他们可以放心地
将服务器数目超额认购40%。他们建议，WSC架构师应当做一些工作，提高WSC内部的平均
功率利用率；但是，他们还建议使用大量的监视软件和安全机制，当工作负载偏移时可以取消
某些较低优先级的任务。

Barroso 和 Holzle［2009］对rT 设备自身内部的功率利用进行了分解，据此给出了在2007年
部署的 Google WSC 的数据，如下所示：

\begin{itemize}
    \item 33%的功率用于处理器；
    \item 30%用于 DRAM；
    \item 10%用于磁盘；
    \item 5%用于联网；
    \item 22%用于其他原因（服务器内部）。
\end{itemize}
\subsection{测量 WSC 的效率}
有一种广泛使用的简单度量可以用来评估一个数据中心或 WSC的效率，称为功率利用效
率（或PUE）：

PUE=（总设施功率）/（IT设备功率）

因此，PUE总是大于或等于1，PUE 越大，WSC的效率就越低。

Greenberg 等人［2006］报告了19个数据中心的PUE，以及制冷基础设施开销所占的比例。
图6-7展示了他们的研究成果，按照 PUE 从最高效到最低效排列。PUE 的中值为1.69，制冷基
础设施使用的功率超过服务器本身的一半，平均起来，1.69 中有0.55用于制冷。注意，这些都
是平均 PUE，可能会根据工作负载、甚至外部空气温度而发生日常变化，稍后将会看到这一点。

2006年19 个数据中心的功率利用效率［Greenberg 等人，2006】。在计算 PUE 时，以TT设备的功
率为基准，对空调（AC）及其他应用（比如配电）的功率进行了归一化。因此，IT设备的功率必
然为1.0，AC 的功率为IT设备功率的0.3~1.4倍。“其他”功率为TT设备的0.05~0.6倍

由于最终度量是每美元实现的性能，所以仍然需要测试性能。如前面的图6-3所示，距离
数据越远，带宽越低，延迟越大。在WSC 中，服务器内部的 DRAM带宽为机架内带宽的 200
倍，而后者又是阵列内带宽的10倍。因此，在WSC 内部放置数据利程序时，还需要考虑另一
种局域性。

WSC的设计人员经常关注带宽，而为WSC开发应用程序的程序员还会关注延迟，因为这
些延迟会让用户感受到。用户的满意度和生产效率都与服务的响应时间紧密联系在一起。在分
时时代的几项研究表明：用户生产效率与交互时间成反比，这一交互时间通常被分解为人员输
人时间、系统响应时间、人们在输入下一项时考虑回应的时间。试验结果表明：将系统响应时
间削减30%可以将交互时间减少70%。这一令人难以相信的结果可以用人类自身的特性来解释：
人们得到响应的速度越快，需要思考的时间越短，因为在这种情况下不太容易分神，一直保持
“高速运转”。

表6-5给出了对Bing搜索引擎进行这一试验的结果，在此试验中，搜索服务器端插人了 50 ms
至2000ms的延迟。和前面研究中的预测一样，到下一次单击之前的时间差不多是这一延迟的
两倍；也就是说，服务器端延迟为 200 ms 时，下一次单击之前的时间会增加500 ms。收入随着
延迟的增加而线性下降，用户满意度也是如此。对Google 搜索引擎进行的另一项研究发现，这
些影响在试验结束4周之后还没有消失。五个星期之后，当用户体验到的延迟为200ms时，每
天的搜索人员会减少0.1%，当用户体验到的延迟为400ms 时，每天的搜索人员会减少0.2%。
考虑到搜索产生的经济效益，即使如此之小的变化也是令人不安的。事实上，这些结果的负面
影响非常严重，以致这项试验提前结束［Schurman 和 Brutlag 2009］。

表6-5 Bing授索服务話的延迟对用户行为的负面影响（Schurman和Brutlag ［2009］）
服务器延迟（ms）
下一次单击之前的增加时间（ms）
查询/用户
任意单击/用户
用户满意度
收益/用户
\$0
-
200
500
1000
2000
500
1200
1900
3100
—
-0.7%
-1.8%
-0.3%
-1.0%
-1.9%
-4.4%
-0.4%
-0.9%
-1,6%
-3.8%
-1.2%
-2.8%
-4.3%

由于因特网服务极端看重所有用户的满意度，所以通常在指定性能目标时，不是提供一个
平均延迟目标，而是给出一个很高的百分比，要求至少有如此比例的请求低于某一延迟门限。
这种门限目标被称为服务级目标（SLO）或者服务级协议（SLA）。某一SLO可能是要求99%
的请求都必须低于100 ms。因此，Amazon Dynamo键值存储系统的设计者决定：为使服务能够
在 Dynamo的顶层提供好的延迟性能，他们的存储系统必须在99.9%的时间内实现其延迟目标
［DeCandia 等人2007］。例如，Dynamo 的一项改进对第99.9个百分点的帮助要远多于平均情景，
这反映了他们的侧重点。

\subsection{WSC的成本}
在引言中曾经提到，WSC的设计者与大多数架构师不同，他们要同时关注WSC的运行成
本和构建成本。会计部门将前一成本记为运行性支出（OPEX），后一成本记为资
性支出（CAPEX）。

为了正确合理地看待能源成本，Hamilton［2010］通过一项案例研究了WSC的成本。
帕定
8 MW设施的CAPEX为8800万美元，大约46 000 台服务器和相应的网络设备向 WSC的C
另外增加了7900万美元。表6-6给出了这一案例研究的其他假定。

表6-6 WSC的案例研究，据Hamilton［2010］，四舍五入到最接近的5000美元
设施规模（临界负載：瓦）
8000000
平均功率利用率（%）
80%
功率利用效率
1.45
电力成本（美元/千瓦时）
0.07美元
电力和制冷基础设施百分比（占总设施成本的百分比）
82%
设施的CAPEX（不包括I设备）
88 000 000美元
服务器数
45 978
成本/服务器
1450美元
服务器的CAPEX
66 700 000美元
机架交換机数
11\$0
成本/机架交换机
4800美元
阵列交换机数
22
成本/阵列交换机
300000美元
L3交换机数
2
成本/3交换机
500 000美元
边界路由器数目
2
成本/边界路由器
144800美元
联网设备的CAPEX
12810 000美元
WSC的总CAPEX
167 510 000美元
服务器分摊时间
3年
网络设备分摊时间
4年
设施分摊时间
10年
借款的年度利率
5%

*图特网带宽成本随应用程序变化，所以这里来包會在内。设施CAPEX的其余18%包括购买知识产权和机房的建设成
本。我们在表6-7中加入了安全和设施管理的人力成本，此案例研究没有包括这一部分。注意，Hamilton的评估是在
他加入 Amazon之前究成的，这些评估值并非以特定公司的WSC 为基础。

由于美国会计规则允许我们将 CAPEX 转换为OPEX，所以我们现在可以为总能耗成本定价
了。我们只需要将 CAPEX 分摊到设备有效寿命内的每个月份，使其为—个固定值。表6-6对这
一案例研究的月度 OPEX 进行了分解。注意，不同设备的分摊率有很大不同，设施的分摊期限
为10年，网络设备为4年，服务器为3年。因此，WSC设施持续一个年代，但需要每3年将服
务器更换一次，每4年将网络设备更换一次。通过分摊 CAPEX，Hamilton 得到了月度 OPEX，
包括借钱支付 WSC款项的利率（每年5%）。月度 OPEX为380万美元，大约为CAPEX的2%。

利用此表，可以算出一个很便捷的准则，在决定使用哪些与能源有关的组件时，一定要记
住这一准则。在一个 WSC中，每年每瓦的全额成本（包括分摊电力和制冷基础设施的成本）为：

基础设施的月度成本＋电力的月度成本 x12=-
76.5万美元+47.5万美元
×12=1.86美元
设施规模（单位：瓦）
800万

此成本大约是2美元（瓦年）。因此，要想通过节省能源来降低成本，花费不应当超过2美元（瓦年）
（见6.8节）。

注意，有超过三分之一的 OPEX 是与电力有关的，当服务器成本随时间下降时，这一部分
反而会上升。网络设备的成本也很高，占总 OPEX的8%，服务器 CAPEX的19%，网络设备成
本不会像服务器成本那样快速下降。特别是对于那些在联网层次结构中高于机架的交换机，这
一点尤其正确，大多数联网成本都花费在这些交换机上（见6.6节）。关于安全与设施管理的人
力成本大约次 OPEX的2%。将表6-7中的OPEX除以服务器的数目及每个月的小时数，可以得
到其成本大约为每小时0.11美元/服务器。

表6-7 表6-6的月度OPEX，四舍五入到最接近的5000美元
费用（占总费用的百分比）
分摊CAPEX（85%）
类别
月度成本（美元）
月度成本百分比
服务器
2000000
53%
网络设备
290000
8%
电力与制冷基础设施
765 000
20%
其他基础设施
170 000
4%
OPEX（15%）
月度用电成本
475 000
13%
月度人员薪金与津贴
85 000
2%
总OPEX
3 800 000
100%

*注意，服务器的3年分摊期意味着需要每3年购买一次新服务器，而设施的分摊期为10年。因此，服务器的分摊构建
成本大约是设施的3倍。人力成本包括3个安保岗位，每天連续24小时，每年365天，每人每小时约20美元；1个设
施人员，每天连续24小时，每年365天，每人每小时 30美元。津點力薪金的30%。这一计算没有包含因特网网络带
宽成本，因为它是随应用程序的变化而变化的，也没有包含供应商维护费用，因它们是随设备与协议的变化而变化的。

例题
答案
美国不同地区的电力成本变化范围为 0.03~0.15美元/千瓦时。这两种极端费率对
每小时的服务器成本有什么样的影响？
我们将8兆瓦的临界负载乘以表6-6中的PUE 和平均功率利用率，以计算平均功
率使用率：
8×1.45 ×80%=9.28兆瓦
于是，若费率为 0.03 美元/千瓦时，月度电力成本从表6-7的475000 美元变为
205 000美元，若费用为0.15美元/千瓦时，则变为1015 000美元。电力成本的这
些变化使每小时的服务器成本分别由0.11美元变为0.10美元和 0.13美元。

例题
解答
如果将所有分摊时间都变为相同的（比如）5年，那月度成本会发生什么变化？每
台服务器每小时成本会发生什么变化？
可以从 http://mvdirona.com/jrh/TalksAndPapers/Perspectives DataCenter Cost AndPower.xls
下载该电子表格。将分摊时间改为5年会将表6-7的前4行变为：
服务器
1260 000美元
37%
网络设备
242 000美元
7%
电力与制冷基础设施
1 115 000美元
33%
其他基础设施
245 000美元
7%
总月度OPEX 为3422000美元。如果我们每5年更换所有东西，成本将为每小时
0.103 美元/服务器，分摊成本的主体现在是设施，而不是服务器，如表6-7所示。

每小时0.11美元/服务器的费率远低于许多公司拥有和运行自有（较小）传统数据中心的成
本。WSC的成本优势导致大型因特网公司都将计算功能当作一种公用设施来提供，和电力一样，
你只需要为自己使用的那一部分付费即可。今天，公用计算有一个更好的名字—云计算。

\section{云计算：公用计算的回报}
如果未来的计算机就是我所倡导的计算机类型，那可能会有那么一天，计算功能也被
当作一种公用设施，就像电话系統是一种公用设施一样⋯计算机公共设施可能成为一种
新的重要行业的基础。

-John MeCarthy，

《MIT 百年庆典》（MIT Centennial celebration）（1961）

由于用户数目不断增大，受其需求的推动，诸如 Amazon、Google 和微软等因特网公司用
大众化组件构建了日益增大的仓库级计算机。这一需求导致了系统软件的革新，以能够在这一
规模正常工作，这些系统软件包括 Bigtable、Dynamo、GFS 和 MapReduce。它还要求改进运行
技术，尽管存在组件故障和安全攻击，也要使所提供的服务能够保证至少在99.99%的时间内可
供使用。这些技术的示例包括故障转移、防火墙、虚拟机和防御分布式拒绝服务攻击的保护。
有了提供扩展能力的软件和专业知识，再加上日益增长的客户需求证明了投资的合理性，拥有
S0 000 到100000 台服务器的WSC已经在2011年变得很常见。

随着规模的增大，规模经济的好处也日益明显。2006年的一项研究对比了 WSC 和仅有1000
台服务器的数据中心，根据这一研究，Hamilton［2010］报告了 WSC的以下优势。

\begin{itemize}
    \item 存储成本缩减为数据中心的17.7\%-WSC用于磁盘存储的费用为每年4.6美元/GB，
    而数据中心则为26美元/GB。
    \item 管理成本缩减为数据中心的14.0\%-—WSC 的服务器与管理员之比超过1000，而数据
    中心仅为140。
    \item 联网成本缩减为数据中心的13.7\%——WSC的因特网带宽成本为每月13 美元/Mbps，而
    数据中心为95美元。不难想到，在协商带宽价格时，订购1000 Mbps 的单位 Mbps 价格
    肯定可以远低于订购10Mbps 的价格。
\end{itemize}

规模经济在采购期间也得到了体现。大规模的采购可以获得服务器与网络设备的大额折扣价
格。它可以对供应链进行优化。Dell、IBM和 SGi都是在一周内完成有关WSC的订单，而不会拖
到4~6个月之后。提交时间的缩短大大简化了这一公共设施的扩展过程，能够更好地满足需求。

规模经济同样适用于运行成本。我们在上一节看到，许多数据中心的运行 PUE 为2.0。大型公
司可以雇用机械工程师和电力工程师对WSC进行改进，使其PUE更低，降至1.2左右（见6.7节）。

出于对可靠性和降低延迟的考虑（特别是对于国际市场），因特网服务需要分布在多个 WSC
上。由于这一原因，所有大型公司都采用多个 WSC。各个公司在世界各地创建多个小型数据中
心的成本要远高于在公司总部创建单个数据中心。

最后，出于6.1节给出的原因，数据中心中服务器的利用时间往往只有总时间的10%~20%。
将 WSC推向公众使用之后，来自不同客户的不相关峰值可以将平均利用率提高到50%以上。

因此，因为WSC的几种组件，可以使WSC的规模经济因数达到5~7，整个 WSC的规模
经济因数为1.5~2。

尽臂有多家云计算服务提供商，但我们这里以 Amazon Web Services（AWS）为例，一方面
是因为它的普及性，另一方面是因为这些服务的抽象层级较低，更为灵活。Google App Engine
和 Microsott Azure 将抽象层级提高到托管运行时，提供自动扩展服务，这样可以更好地适应一
些客户，但不像 AWS那样更适于作为本书中的介绍材料。

\subsection{Amazon Web Services}
公用计算可以追溯到20世纪60年代和70年代的商业分时系统，甚至是批处理系统，当时
公司只需要预先支付终端和电话线路的费用，然后再根据它们所使用的计算量来计费。在分时
时代结束之后，人们进行了许多尝试，希望再为服务提供这种支付模式，但经常遭遇失败。
当 Amazon 在2006年开始通过Amazon 简单存储服务（Amazon \$3）和后来的 Amazon弹
性计算机云来提供公用计算时，它作出了几项不同寻常的技术与商业决策。

\begin{itemize}
    \item 虚拟机。利用运行 Linux 操作系统和 Xen虚拟机的 x86大众化计算来构建 WSC，解决了
    几个问题。第一，通过这一方法可以让 Amazon 为用户提供保护，使他们免受其他用户
    的伤害。第二，简化了 WSC 中的软件分布，因为客户只需要安装一个映像，AWS之后
    会自动将它发布到所使用的全部实例上。第三，能够可靠终止虚拟机的功能使 Amazon
    和客户能够轻松地控制资源利用情况。第四，由于虚拟机可以限制它们利用物理处理器、
    磁盘和网络的速率和使用主存储器的数量，从而为 AWS提供了多种不同价格选择：将
    多个虚拟核心挤在单个服务器上，价格最低；独占使用所有机器资源，价格最高；还有
    介于两者之间的几个中间选择。第五，虚拟机隐藏了较旧硬件的身份，使AWS 可以继
    续出售旧机器的工作时间，如果让客户知道了这些机器的年龄，很可能就会失去对他们
    的吸引力。最后，虚拟机允许AWS引入新的更快硬件，具体方式可以是在每个服务器
    上运行更多的虚拟核心，也可以是提供一些在每个虚拟核心上拥有更高性能的实例；虛
    拟化的使用，意味着所提供的性能不一定是硬件性能的整数倍。
    \item 极低成本。当AWS在2006年宣布每个实例0.10美元小时的费率时，这个数字低得让人
    吃惊。一个实例就是一个虚拟机，以每小时 0.10美元的价格，AWS 可以在一个多核心
    处理器的每个核心上运行两个实例。因此，一个 EC2 计算机单元等价于那个时代的1.0
    至 1.2 GHz AMD Opteron 或 Intel Xeon。
    \item 对开源软件的（初始）依赖。一些开源软件的质量很高，而且没有版权问题，在数百乃
    至数干台服务器上运行时也没有相关成本，利用这些软件大幅提高了公用计算的经济性，
    对 Amazon利客户都是如此。最近，AWS开始提供一些包含商用第三方软件的实例，其
    价格要高一些。
    \item 没有（最初的）服务保证。Amazon 最初仅承诺尽最大努力。极具吸引力的成本让许多
    服务在没有保证的情况下也能生存下来。今天，AWS 对一些服务（比如 Amazon EC2
    和 Amazon S3）提供了高达99.95%的可用性SL.A。此外，Amazon S3的设计目标是达
    到 99.999999999%的持久性，具体方法是在多个位置保存每一对象的多个副本。也就
    是说，永久丢失一个对象的机率是1万亿分之一。AWS还提供了一种服务健康仪表板，
    它可以实时给出每个AWS服务器的当前运行状态，使AWS的工作时间与性能做到完
    ．全透明。
    \item 不需要合同。部分原因是其成本如此之低，只需要有一张信用卡就能开始使用EC2。
\end{itemize}

表6-8给出了 2011年许多EC2实例每小时的价格。除了计算之外，EC2还负责长期存储和
因特网通信流量。（AWS 区域内部的网络通信流量没有成本。）弹性块存储的成本为每月0.10
美元/GB、0.10美元/百万次 1/O请求。进人EC2 的因特网通信流量的成本为0.10美元/GB，离
开EC2的成本为0.08~0.15美元/GB，具体取决于流量大小。从历史的角度来看，每月支付100
美元所能使用的容量相当于1960年生产的全部磁盘容量总和！

表6-8 2011年1月美国维吉尼亚地区EC2实例按需计费的价格和性能指标
每小时的
与小型实
计算
虚拟
计算单
夹例
费用（美元）
例的比值
单元
核心
元/核心
微型
0.020
0.5~2.0
0.5~2.0
1
0.5~2.0
标准小型
0.085
1.0
1.0
1
1.00
标准大型
0.340
4.0
4.0
2
2.00
标准超大型
0.680
8.0
8.0
4
2.00
高存储器超大型
0.500
5.9
6.5
2
3.25
高存储器双超大型
1.000
11.8
13.0
4
3.25
高存储器四倍超大型
2.000
23.5
26.0
8
3.25
高CPU中型
0.170
2.0
5.0
2
2.50
高CPU超大型
0.680
8.0
20.0
8
2.50
集群四倍超大型
1.600
18.8
33.5
8
4.20
存储𤰉
（GB）
0.6
1.7
7.5
15.0
17.1
34.2
68.4
1.7
7.0
23.0
磁盘
（GB）
EBS
160
850
1690
哒MMogo
1690
1690
地址大小
（位）
32/64
32
64
64
64
64
64
32
64
64

*微型实例是最新、最便宜的类别，它们提供最多两个计算单元的短时袋发，每小时仅 0.02 美元。客户报告，微型实
例平均仅0.5个计算单元。最后一行的集群计算实例（AWS确认该实例为专用双插槽 Intel Xeon X5570服务器，每个
插横有四个核心，工作频率为2.93 GHz）提供10Gbits的网络。它们是为 HPC应用准备的。AWS 还以低得多的价格
提供了“现货实例”，在这种情况下，用户设定自己希望支付的价格以及希望运行的实例数目，当“现货价格”降低到用
户的设置水平时，AWS将会运行这性服务，并将一直持续下去，直到用户停止这些服务，或者现货价格超出用户设
置的限度为止。在2011年1月日间的一次取样表明：现货价格的下降因数为 2.3~3.1，具体取决于实例类型。有些
客户知道自己在一年的大多数时间内都会使用某一实例，AWS 为这种情况提供了“保留实例”。用户可以支付每个实
例的年度费用，然后再按小时支付其使用费用，该费率大約是第1列的30%。如果需要整年使用保留实例，每小时
的平均成本包括了年度费用的分摊值，大約沟第一列费車的65%。与表6-6和表6-7 中等价的服务器內标准超大型或
高 CPU超大型实例，我们为其算出的成本为0.11美元小小时。

例题
解答
在EC2上运行表6-2中的平均 MapReduce 作业，计算其成本。假定作业数非常充
足，所以不存在为获得整数小时数进行舍人而带来的大量额外成本。忽略月度存储
成本，但包含 AWS弹性块存储（EBS）执行磁盘1/O的成本。接下来计算运行所
有 MapReduce 作业所需要的年度成本。
第一个问题是：多大规模的实例能与Google 的典型服务器相匹配？6.7节的图6-11
显示：2007年，一个典型 Google服务器有四个核心，运行频率为 2.2 GHz，存储
器为8GB。由于单个实例就是一个虚拟核心，等价于1到1.2 GHzAMD Opteron，
所以与表6-8最匹配的是拥有8个虚拟核心、7.0GB存储器的高 CPU 超大型实例。
为简单起见，我们假定平均EBS存储访问为64 KB，以计算 I/O数目。
表6-9计算了在EC2上运行 Google MapReduce 工作负载的平均年度成本及总年度
成本。2009年在EC2上运行平均 MapReduce 作业的成本略低于40美元，2009年
的总工作负载将在 AWS上花费1.33亿美元。注意，这些作业的EBS 访问大约占
总成本的1%。

表6-9
按照 2011年的 AWS ECS、EBS 价格（表6-8）运行 Google MapReduce 工作负载（表6-2）时
\begin{verbatim}
    的估测成本
    8月4日
    3月6日
    9月7日
    9月9日
    平均完成时间（小）
    0.15
    0.21
    每项作业的平均服务器数目
    157
    268
    EC2高CPU XL实例每小时的成本
    0.68（美元）
    0.68（美元）
    每项MapReduce作业的平均EC2成本
    16.35（美元）
    38.47（美元）
    EBS VO请求平均数（百万个）
    2.34
    5.80
    每百万1/O请求的EBS成本
    0.10（美元）
    0.10（美元）
    每项MapReduce作业的平均EBSI/O成本
    0.23（美元）
    0.58（美元）
    每项MapReduce作业的平均总成本
    16.58（美元）
    39.05（美元）
    MapReduce作业的年度数目
    29 000
    171 000
    EC2/EBS_上MapReduce作业的总成本
    480910（美元）
    6678011（美元）
    0.10
    394
    0.68（美元）
    25.56（美元）
    3.26
    0.10（美元）
    0.33（美元）
    25.89（美元）
    2217000
    57394985（美元）
    0.11
    488
    0.68（美元）
    38.07（美元）
    3.19
    0.10（美元）
    0.32（美元）
    38.39（美元）
    3467 000
    133107414（美元）
\end{verbatim}
* 由于使用的是2011年价格，所以前几年估测值的准确性低于近几年的估测 。

例题
由于 MapReduce 作业的成本正在增长，已经超过每年1亿美元，假想老板希望你
研究一些降低成本的方法。两种可供选择的低成本选项是 AWS 保留实例或 AWS
现货实例。你推荐哪一种？

解答
AWS保留实例收取固定的年度费用，再加上按小时计算的使用费用。2011年，高
CPU 超大型实例的年度成本为1820美元，每小时的费率为0.24美元。由于无论我
们是否使用这些保留实例都要为其付费，所以假定保留实例的平均利用率为80%。
则每小时的平均价格变为：

年度价格
每年的小时级+每小时的价格
1820美元＋0.24美元
8760
-=（0.21+0.241）×1.25=0.56美元
使用率
80%

因此，对于2009年MapReduce 工作负载，使用保留实例大约可以节省17%，也就
是2300万美元。

对2011年1月份的一些日子进行采样，得出高CPU超大型现货实例的每小时成本
平均为0.235美元。由于这是使用一台服务器的最低价格，而且我们通常希望顺利
地完成任务，所以平均成本不可能是最低价格。我们假定为了完成大型 MapReduce
作业，需要支付的价格为最低价格的两倍。对于2009年工作负载，使用现货实例
节省的成本大约为31%，即4100万美元。

因此，你试探性地向老板推荐现货实例，因为它的预付金较低，而且可能节省更多
资金。但是，你告诉老板，需要尝试在现货示例上运行 MapReduce 作业，以了解
最终采用哪种方式来确保作业可以顺利完成，并确保确实有数百个高CPU超大型
实例可用于每天运行这些作业。

除了公用计算的低成本与按需付费模型之外，另一个强烈吸引云计算用户的因素是云计算
提供商承担了过度供应或供应不足的风险。风险规避对刚起步的公司来说是一件天赐之物，因
为随便一个错误都可能是致命的。如果在产品得到广泛应用之前，将过多的宝贵资源花费在服
务器上，那公司可能会耗尽自己的资金。如果这种服务突然变得流行起来，但又没有足够的服
务器来满足需求，那公司就会给自己希望拼命发展的潜在新客户留下很坏的印象。

这种情景的一个典型代表是 Zynga的 FarmVile，它是 Facebook上的一个社交网络游戏。
在发布 FarmVille之前，最大型的社交游戏大约有500万个每日玩家。FarmVille 在发行之后的4
天拥有100万个玩家，60天之后拥有1000万个玩家。在270天之后；它拥有2800万个每日玩
家和7500万个每月玩家。由于它们部署在AWS上，所以能够随着用户的数目无縫增长。此外，
它还根据用户需求来降低负载。

有更多的知名公司也正在充分利用云的可伸缩性。Netfix 将其网站和流视频服务由传统的数
据中心移植到 AWS 上。Netlix的目标是让用户能够在回家的路上在（比如）自己的手机上观看
电影，到家后无缝切换到电视机上，继续观看还没有看完的电影。这种工作量涉及一些批处理操
作，以便将新电影转换为无数种格式，在向手机、平板电脑、笔记本电脑、台式机、游戏主机和
数字视频录像机上发送这些电影时需要这些格式。这些 AWS批处理作业可能需要数千台机器花费
几周的时间来完成这些转换。这个事务流的后端在 AWS 完成，编码文件的提供通过“内容分发
网络”完成（比如 Akamai和Level 3）。这种联机服务的价格比邮寄 DVD便宜得多。这种低成本使
这一新服务变得非常普及。一项研究认为 Netlix 占据了美国晚上峰值时段30%的因特网下载流量。
（与之相对，YouTube 仅占晚上8点至10点同一时间段的10%。）事实上，其平均占到因特网流量
的22%，北美最大部分的互联网流量都是由 Nettlix 一家公司产生的。尽管 Nettix 订阅账户的增长
速度还在增加，Netllix 数据中心的增长速度已经停止，所有容量扩展都是通过 AWS 完成的。

云计算已经可以让每个人都能享受到 WSC带来的好处。云计算向用户提供了成本相关性
（cost associativity ），人们可以想象它拥有无限的可伸缩性，但却没有任何额外成本：1000台服
务器工作1小时的成本不大于1台服务器工作1000小时的成本。由云计算提供者来负责确保有
充足的服务器、存储和因特网带宽来满足需求。上面提到的优化供应链（这一供应链将新计算
机的交付时间缩短到一周）可以帮供应商一个大忙，让他们能够在不至破产的前提下实现这一
愿景。这种风险的转移、成本相关性和按需付款的定价方式，是各种不同规模的公司使用云计
算的强有力依据。

有两个交叉问题影响到了 WSC的性价比，从而影响了云计算的性价比，即WSC 网络和服
务器软硬件的效率。

\section{交叉问题}
网络设备是数据中心的 SUV。
-James Hamilton （2009）

\subsection{成为瓶颈的WSC网络}

6.4节表明，机架交换机上层的网络设备占去WSC成本中很大的一部分。在完全配置时，
Juniper 出品的一个 128端口1 Gbit数据中心交换机的定价是716000美元，其中不包括光纤接
口，若包括这些接口则为908000美元。（这些定价有很大的打折幅度，但它们的成本仍然是机
架交换机的50倍。）这些交换机往往还是耗电大户。例如，EX8216 消耗大约19200瓦，是 WSC
中一台交换机的500~1000倍。此外，这些大型交换机大多是手动配置的，而且非常脆弱。由
于其价格原因，所以在 WSC 中很难承受用这些大型交换机来实现双重冗余，这就限制了对容
错性的选择［Hamilton 2009］。

但是，对交换机的真正影响是超额认购如何影响 WSC中软件的设计和服务器与数据的布
置。理想的 WSC 网络可能是一个黑盒，人们对其拓扑和带宽不感兴趣，因为它们不存在限制：
我们可以在任意地方运行任意工作负载，针对服务器利用情况进行优化，而不是针对网络通信
流量的局域性优化。这些WSC网络瓶颈现在包含数据布置，而它又使WSC软件变得更为复杂。
由于这个软件是WSC公司最有价值的资产，增加复杂性带来的成本可能是非常庞大的。

如果读者想了解有关交换机设计的更多知识，可以参阅附录 F，其中介绍了设计互联网络
时涉及的问题。此外，Thacker ［2007］建议借用超级计算的网络技术来克服价格和性能问题。
Vahdat 等人［2010］做得也非常出色，提出了一种网络基础设计，可以扩展到100000个端口和1 Pbps
的对分带宽。这些新颖数据中心交换机的主要好处就是简化了由于超额认购导致的软件问题。

\subsection{在服务器内部高效利用能量}
PUE是WSC效率的度量，但没有哪个指标来描述进入 IT设备自身内部的能耗情况。因此，
还有另外一个可能导致电气效率不佳的因素（未在图6-5中给出），那就是服务器内部的电源，
它将输人的208伏或110伏转换为芯片和磁盘使用的电压，通常为3.3伏、5伏和12伏。根据
微处理器和存储器的要求，再将12伏电压变换为主板上的1.2至1.8伏。2007年，许多电源的
效率为 60%~80%，也就是说，服务器内部的损耗要大于从公共电力塔的高压线经过许多步骤
和电压转换直到服务器低压电线的损失。造成这一结果的一个原因是它们事先不知道主板上都
有哪些设备，必须为芯片和磁盘提供各种不同电压。第二个原因是电源的功率数通常大于主板
所需要的功率。另外，当带有 25%或更低负载时，这些电源的效率通常为最差状态，而图6-1
显示，许多WSC服务器都工作在这一范围。计算机主板也有一些电压调节模块（VRM），它们
的效率也比较低。

为了提高当前技术，表6-10给出了计算产业拯救气候计划标准（2007），用于评定电源的
等级及其目标。注意，这一标准除了指定100%满载情况下的要求之外，还给出了在20%和50%
质载情况下的要求。

表6-10
计算产业拯救气候计划中对电源不同时间的效率评估与目标
带载条件
基础
Bronze（2008年6月）
Silver《2009年6月）
20%
80%
82%
85%
50%
80%
85%
88%
100%
80%
82%
85%
Gold（2010年6月）
87%
90%
87%

*这些评估值是针对多输出电源单元的，是指非冗余系統中的台式机与服务器电源。针对单输出 ESU 有一个略高一点
的标准，通常用于冗余配置中（1U/2U单、双、四插槽及刀片服务）。

除了电源之外，Barroso 和 Hlzle ［2007］还指出，整个服务器的目标应当符合能耗比例；也
就是说，服务器消耗的能量应当与其执行的工作量成比例。图6-8使用 SPECpower 表明我们距
离这一理想目标的实现还有多远，SPECpower 是一种服务器基准测试，它测量在不同性能级别
消耗的能量（见第1章）。图中显示了一种服务器的实际电能使用情況（截至2010年7月，这
一服务器是运行 SPECpower 的最高效服务器），并在图中添加了能耗比例线。大多数服务器都
达不到这样的效率；这一最高效系统的指标大约优于该年度其他被测系统达2.5倍，在后来的
基准测试比赛中，经常为赢得基准测试而对系统进行特殊设置，它们通常不是该领域的典型系
统。例如，最佳 SPECpower服务器使用了固态磁盘，其容量要小于主存储器！即使如此，这一
非常高效的系统在空闲时也消耗了全部功率的30%，在仅有10%的负载时，消耗了全部功率的
50%。因此，能耗比例仍然是一个有待实现的远大目标，而不是令人自豪的现实。

图6-8
2010年7月的最佳SPECpower 结果与理想能耗比例特性的对比。此系统为 HP ProLiant SL2x170z G6，
它使用了由4个双插槽 Intel Xeon L5640组成的集群，每个插槽有6个核心，时钟频率为2.27GHz。
这个系统的 DRAM为64 GB，辅助存储很小，为60 GB SSD。（主存储器大于磁盘容量这一事实
暗示出这一系统是为此基准测试定制的。）所用软件为 IBM Java Virtual Machine V9 和 Windows
Server 2008 企业版

系统软件的设计思路是利用所有可用资源，只要这样可能提升性能即可，并不关心能耗方面
的影响。例如，操作系统为程序数据或文件缓存使用所有存储器，并不考虑许多数据可能根本就
用不到。在未来的设计中，软件架构师需要在考虑性能的同时也考虑能耗［Carter和 Rajamani2010］。

例題
根据图6-8所示类型的数据，五个利用率为10%的服务器与一个利用率为50%的服
务器相比，可以节省多少功率？
解答
对于单个服务器，负载为10%时为308瓦，负载为50%时为451 瓦。因此，节省
的功率为：
5×308/451=（1540/451） ~ 3.4

如果希望WSC非常环保，必须在利用率下降时对服务器进行整合，购买更符合能耗
比例特性的服务器，或者寻找其他一些措施，改进服务器在活跃性较低时的运行情况。
有了前面6节的背景知识，我们现在可以欣赏一下 Google WSC架构师的作品了。

\section{融会贯通：Google 仓库级计算机}
由于许多拥有 WSC的公司在市场上的竞争非常激烈，所以直到最近，它们也不愿意与公
众（及其他公司）分享自己的最新技术。2009年，Google 描述了2005年技术水平的 WSC。Google
慷慨地提供了2007年对其WSC的更新，从而使这一节成为 Google WSC 的最新介绍［Clidaras，
Johnson 和 Felderman 2010］。最近，Facebook 在 http://opencomputer.org 介绍了其最新的数据中心。

\subsection{集装箱}
Google 和 Microsoft都使用货运集装箱来构建WSC。用集装箱来构建 WSC的思想是实现
WSC设计的模块化。每个集装箱是独立的，仅有的外部连接是网络、电源和供水。这些集装箱
又向放在其内部的服务器提供联网、电源和制冷，所以 WSC的任务就是向这些集装箱提供联
网、电力和冷却水，并将产生的热水抽至外部的冷却塔和冷擬器。

我们正在研究的Google WSC包含45个长40英尺的集装箱，占用300英尺×250英尺的面积，
即75 000平方英尺（大约7000平方米）。为了放置在仓库中，30个此种容器两两摞在一起，构成
15 对堆叠集装箱。尽管其所在位置还没有公开，但当 Google 在俄勒冈州达尔斯开发 WSC 时已经兴
建了这一场所，这个位置的气候适宜，接近便宜的水力发电厂和因特网骨干光纤。这个WSC在先前
的12个月内提供了10兆瓦，PUE为1.23。在0.23的PUE开销中，85%属于制冷损耗（0.195 PUE），
15% （0.035）为电力损耗。这一系统于2005年11月建成，本节描述其在2007年的状态。
一个 Google 集装箱可以处理高达250千瓦的容量。这意味着这个容器可以处理780瓦/平
方英尺（0.09平方米），或者说133瓦/平方英尺，在整个75000平方英尺的空间中放置40个容
器。但是，这个 WSC中的集装箱平均值只有222千瓦。

图6-9是 Google集装箱的一个剖面图。集装箱容纳多达1160台服务器，所以45个集装箱
可以容纳52200台服务器。（这个WSC拥有大约40000台服务器。）这些服务器堆叠在机架中，
高度为20个服务器，构成很长的两行，每行29个机架（也称为分区），各位于集装箱的一侧。
机架交换机为48端口 1Gbps的以太网交换机，每隔一个机架放置一台机架交換机。

Google 定制了一个标准的1AAA 集装箱：40×8X9.5 英尺（12.2×2.4×2.9米）。这些服务器
在机架内堆叠在一起，高度为20个服务器，构成很长的两行，每行29个机架，各位于集装箱的
一侧。冷却通道在集装箱的中部下行，热空气回行管位于外部。悬挂式机架结构便于修理制冷系
统，不用移动服务器。为便于集装箱内的人员修理组件，它包含了用于火灾探测与细雾式灭火、
紧急出口与照明和紧急切断供电等安全系统。集装箱内还有许多传感器：温度、气流压力、气体
泄露检测和移动感应照明。在 http://www.google.com /corporate /green/datacenters/summit.htanl 可以
找到数据中心的一个视频漫游。微软、雅虎和其他许多公司现在都基于这些思想构建了模块化数
据中心，但由于尺寸不太方便，所以他们已经停止使用ISO标准集装箱

\subsection{Google WSC中的冷却与供电}
图6-10 是集装箱的一个横截面，显示了空气流动情况。计算机机架被固定在集装箱的天花
板上。集装箱的地面被升高，制冷系统就位于地面下方，向机架之间的通道中吹送冷风。热空
气从机架的后面返回。集装箱中的有限空间防止冷热空间相混合，提高了冷却效率。变速风扇
以冷却机架所需的最低转速旋转，而不是一直恒定运转。

“冷”空气保持在81T（27°C），与许多传统数据中心的温度相比，这一温度要温和得多。
传统数据中心的运行温度之所以非常低，不是因为IT设备的原因，而是因为为了保证数据中心
的热区不会导致隔离问题。通过精心控制空气流可以避免热区的产生，集装箱的正常运行温度
可以高出许多。

图6-10
图 6-9所示集装箱中的空气流。这个横截图给出了两个各位于集装箱一一侧的机架。冷空气吹入集
装箱中部的通道，然后被吸入服务器。热空气在服务器的边缘返回。这一设计将冷热空气流隔离
开来

外部冷凝器有一些断路器，在天气合适时，只需要室外冷却塔对水进行降温。如果离开冷
却塔的水温不高于70T（21°C），则跳过冷凝器。

注意，如果外部温度过低，冷却塔需要加热器，以防止结冰。将 WSC放在达尔斯的一个
好处就是这里的年度湿球温度范围为15T到66（-9°C到19°C），平均为41（S°C），所以冷
凝器可以经常关闭。与之相对，内华达州的拉斯维加斯的变化范围为-42下到 62（-41CC到
17C，平均为29下（-2°C）。此外，由于只需要将集装箱中的温度冷却到81T（27°C），所以很
有可能仅靠大自然就可以完成水的冷却任务了。

图6-11展示了 Google 为这个WSC设计的服务器。为了提高电源的效率，它仅向主板提供
12 伏电压，主板仅为板上的若干磁盘提供足够的电源。（便携式电脑采用类似方式为其磁盘供
电。）服务器标准是直接提供磁盘和芯片需要的许多种电平。这种简化意味着 2007年的电源能
够以92%的效率工作，远高于2010年黄金级电源的效率（见表6-10）。

图6-11 Google WSC 的服务暑。电源在左侧，两块磁盘位于顶部。低于左側磁盘的两个风扇覆盖了 AMD
Barcelona 微处理器的两个插槽，每个处理器有两个核心，工作频率为2.2GHz。右下方有8个
DIMIM，各自容量为1GB，总数为8GB。没有更多的金属片，服务器插入电池中，在机架内为
每个服务器准备一个独立空间，用于帮助控制气流。在一定程度上受电池高度的影响，一个机架
内装载了20台服务器

Google 工程师认识到，采用12伏电压的电源，每个架子上的标准电池就可以作为UPS。
因此，它没有采用独立的电池室（图6-9中显示其效率为94%），而是由每个服务器拥有自己的
铅酸电池，其效率为99.99%。这种“分布式 UPS”随每台机器递增部署，这就意味着没有因内
超容量配置而花费资金和电力。他们使用标准的开箱即用式UPS单元来保护网络交换机。

使用第1章介绍的动态电压频率调节（DVFS）来节省功率会怎么样呢？在这一系列的计算
机中没有部署 DVFS，这是因为它对延迟的影响过大，对于联机负载，仅在活跃性非常低的情
况下才是可行的，即使在这些情况下，系统范围的节省量也是非常小的。因此，部署 DVFS所
需要的复杂管理控制环可能是得不偿失的。

要使 PUE达到1.23，关键之一就是将测量设备（称为电流转換器）放在集装箱内各个地方
的所有电路内以及 WSC内的其他位置，以测量实际电能利用情况。这些测量允许 Google 在不
同时间调整 WSC的设计。

Google 每季度公布其WSC 的 PUE。图6-12 绘制了10个 Google WSC从2007年第三季度
到2010年第二季度的 PUE 曲线；这一节描述标识为 Google A 的WSC。GoogleE的PUE为1.16，
冷却系统仅为0.105，这是因为其工作温度较高，而且拥有冷凝器断流器。配电系统仅为 0.039，
这是因为采用了分布式UPS和单电压电源。最好的 WSC结果为1.12,Google A 为1.23。在2009
年3月，后面12个月的数据用所有数据中心的利用率进行加权之后，其平均值为1.19

1.4
1.3
1.2
1.1
468
1.0
P
團 6-12 10个 Google WSC 随时间变化的功率利用效率（PUE）。Google A 是本节描述的 WSC。它是
2007年第3季度和2010年第2季度的最高线。（据www.google.com/corporate/green/datacenters/
measuring.ht。）Facebook 最近宜布了一个新的数据中心，它给出了一个给人们留下深刻印象的
PUE，仅 1.07（见 http://opencompute.org/）。Prineville Oregon Facility 没有空调，没有冷却水。
它完全依靠外部空气：从建筑物的一侧引人外部空气，通过喷雾器进行过滤和冷却，再送人1T
设备的各个位置，然后通过排风扇送出机房。此外，服务器使用了一种定制电源，允许配电系统
跳过图6-5的一个电压变换步骤

\subsection{Google WSC中的服务器}
图 6-11 中的服务器有两个插槽，各包含一个双核心 AMID Opteron 处理器，工作频率为
2.2 GHz。相片中显示8个 DIMM，这些服务器通常配有8GB 的 DDR2 DRAM。一种新颖的特
征就是降低了存储器总线的时钟频率，由标准的666 MHz降至533 MHz，这是因为速度较慢的
总线对于性能没什么影响，但对功率的影响却非常显著。

基准设计是使用一块网络接口卡（NIC）来实现1Gbps 的以太网链接。尽管图6-11 中的相
片显示了两个 SATA磁盘驱动器，但基准服务器只有一个。基准设计的峰值功率大约为160瓦，
空闲功率为85瓦。

对这个基准节点进行了补充，以提供存储（或“磁盘满”）节点。第一，向服务器连接一个
包含10个 SATA 磁盘的第二盘塔。为了再多一块磁盘，在主板上的空插槽中放置了一个第二磁
盘，使存储节点达到12块 SATA 磁盘。最后，由于一个存储节点就可能使单个 1Gbps 以太网达
到饱和，所以添加了第二块以太网 NIC。存储节点的峰值功率大约为300瓦，空闲功率为198瓦。

注意，这个存储节点占去了机架的两个插槽，这也是 Google 在45 个集装箱中部署 40 000
台而非52200台服务器的原因之一。在这一设施中，对于每个存储节点，两个计算节点大约都
是这一比值，但在Google 的不同WSC中，这一比值的变化范围很广。因此，Google A 在2007
年大约拥有190000块磁盘，平均每台服务器差不多有5块磁盘。

\subsection{Google WSC中的联网}
40000台服务器被分为三个阵列，每个阵列中有超过10000台服务器。（在 Google术语中，
阵列被称为集群。）48端口的机架交换机使用40个端口连接到服务器，另外8个端口留给阵列
交换机的上行链路使用。

阵列交换机的配置最多支持480个 1 Gbps 以太网链接和一些10Gbps 端口。这些1 Gbit端
口用于连接这些机架交换机，每个机架交换机与每个阵列交换机之间有单一链路。10 Gbps 端口
分别连接到两个数据中心路由器，它将所有阵列路由器汇集在一起，提供与外部世界的连接性。
WSC使用两个数据中心路由器，以保证可靠性，所以单个数据中心路由器故障并不会使整个
WSC宕机。

每个机架交换机所用上行链路端口的数目不同，最小为2，最大为8。在双端口情况下，机
架交换机工作时的超额认购率为20：1。即，交换机内部的网络带宽为离开交換机带宽的20倍。
如果应用程序有大量超出机架之外的通信流量需求，那就容易受到网络性能不佳的困扰。因此，
对于有更多通信流量需求的阵列，采用了8端口上行链接设计（它的超额认购率要低得多，仅
有5:1）

\subsection{Google WSC的监控与修复}

对于一位要负责1000多台服务器的操作员来说，需要一种比较全面的监控基础设施和一定
程度的自动化，以帮助调度事件。

Google 部署监测软件来跟踪所有服务器和网络设备的健康状况。诊断程序无时无刻不在运
行。当系统发生故障时，许多可能出现的问题都有简单的自动化解决方案。在这种情况下，下
一步是重后计算机，然后尝试重新安装软件组件。因此，这一过程会处理大多数故障。
经过前面几步未能修复的机器被添加到待修机队列中。问题的诊断情况与故障机的ID一起
被放在队列中。

为了分摊修复费用，发生故障的机器由修理技师分批解决。当诊断软件确信自己的评估时，
该组件部分会立即被替换，无须经过人工诊断过程。例如，如果诊断软件说某个存储节点的第
3 号磁盘损坏，则立即更換该磁盘。对于没有诊断信息或诊断信息可信度较低的故障机器，需
要进行人工诊断。

目标是在任意时间内，人工修理队列中的节点数低于全部节点的1%。节点在修理队列中
停留的平均时间为一周，尽管由修理工人修复它所需要的时间要短得多。较长的延迟时间体现
了修理吞吐量的重要性，它影响到了操作成本。注意，自动化修复的第一步骤需要几分钟来重新
启动或重新安装，也可能需要几个小时来运行直接压力测试，以确保该机器能够真正胜任工作。
这些延迟没有考虑使故障服务器空闲下来的时间。原因在于：节点中的状态量是一个很大
的变数。如果在替换一个节点之前需要先将它的数据清空，那一个无状态节点所需要的时间要
比存储节点短得多。

\subsection{小结}
到2007年，Google 已经演示了几种用于提高其 WSC能耗效率的创新技术，使 Google A的
PUE达到1.23。
\begin{itemize}
    \item 除了提供一些廉价的外壳来封装服务器之外，经过改装的货运集装箱还将冷热空气通风
    道隔离开来，可以帮助降低服务器进风口的温度扰动。由于最糟糕情况下的热区没有那
    么严重，所以允许冷空气温度稍高。
    \item 这些集装箱还缩短了空气循环流通的距离，减少了移动空气的能耗。
    \item 让服务器运行在较高温度下，意味着只需要将空气冷却到 81T（27°C），而不是通常的
    64下至71（18°C至22°C）。
    \item 冷空气的目标温度较高，有助于更经常地将设施保持在通过蒸发冷却系统（冷都塔）所
    能维持的温度范围内，比传统的冷凝器更具能耗效率。
    \item 将WSC部署在适宜气候下，可以在一年中的较长时间内仅利用蒸发式冷却系统。
    \item 部署全面的监控软硬件，测量PUE的实际值与设计值，以提高运行效率。
    \item 实际运行的服务器要多于配电系统在最差情况下所能支持的服务器，这意味着：由于从
    统计学的角度来看，不太可能有数千台服务器同时处于高度繁忙状态，而是依靠监测系
    统，在它们不可能完成任务时卸载其中一部分［Fan,Weber 和 Barroso 2007］、［Ranganathan
    等人2006］。PUE 提高是因为此设施的工作状态接近于其满载设计容量，由于服务器和
    冷却系统不具备能耗比例特性，所以这种状态是最高效状态。这样提高利用率后，就可
    以降低对新服务器和新WSC的需求。
    \item 将主板设计为仅需要一个 12 伏的电源，这样只需要每个服务器的标准电池就可以提供
    UPS功能，不再需要电池同，从而降低了成本，减少了 WSC中的一个配电低效源。
    \item 细致周到地设计服务器主板，提高其能耗效率。例如，降低这些微处理器前端总线的时
    钟频率，在没有明显性能影响的情况下降低能耗。（注意，这些优化不会影响 PUE，但
    可以降低总 WSC能耗。）
\end{itemize}

这些年来 WSC设计肯定有了改进，Google 最佳WSC的 PUE 已经从 GoogleA的1.23降至1.12。
Facebook 在2011年宣布，在它们的新数据中心中已经将 PUE 降至 1.07（见 http://opencompute.
org/）。了解哪些创新仍然在进一步提高 WSC效率是一件很有意义的事情，以使我们成为优秀
的环境保护者。也许在将来，我们还会考虑 WSC内部设备的制造能耗成本［Chang等人2010］。

\section{谬论与易犯错误}
尽管 WSC还不到10岁，但诸如 Google的WSC架构师已经发现了有关 WSC 的许多谬论
和易犯错误，通常会把人们引领到一条艰难的道路上。我们在引言中已经说过，WSC架构师就
是当今的 Seymour Crays。

\textbf{谬论 云计算提供商在赔钱。}
关于云计算有一个很常见的问题：以如此之低的价格，能否贏利呢？

根据表6-8的AWS 定价，我们可能要花费每个服务器0.68 美元/小时的计算费用。
（0.08\$ 美元/小时的价格等价于一个 EC2计算单元的虚拟机的价格，不是整个服务器。）如果我
们可以销售50%的服务器运行时间，那每台服务器每小时就可以产生0.34美元的收人。（注意，
即使客户很少使用他们占用的服务器，也要支付费用，所以销售 50%的服务器运行时间并不一
定意味着平均服务器利用率为50%。）

另一种计算收入的方法是利用AWS保留实例，在这种方案中，客户支付一定的年费来保
留一个实例，然后再按小时来支付使用费用，其费率通常要低一些。将这些收费结合起来，在
一个整年中，AWS的每台服务器在每个小时内将产生0.45美元的收入。

如果我们能够以 AWS 价格为每台服务器售出750GB 的存储容量，那除了计算收入之外，
每台服务器每个月还会另外产生75美元的收入，也就是0.1美元/小时。

这些数字意味着：每台服务器每小时的平均收入为 0.44 美元（通过按需实例方式）至
0.55美元/小时（通过保留实例方式）。根据表6-6，我们计算出6.4节 WSC中每台服务器的费
用为0.11美元/小时。尽管表6-6中的成本只是一些估计值，井非基于AWS的实际成本，而且
销售 50%的处理器处理时间以及每台服务器存储达到750GB 的利用率，这些都只是例子，但
这些假设仍然能够间接表明毛利润在75%至80%之间。假定这些计算是合理的，那就意味着云
计算是可赢利的，对于服务企业尤为如此。

\textbf{谬论 WSC设施的构建成本高于它所容纳的服务器。}

尽管粗略浏览表6-6可能会让你得到上面的结论，但它忽略了整个 WSC中每一部分的成本
分摊时间。整个设施会持续10至15年，而服务器每3至4年就需要重新购买一次。分别利用
表6-6中10年和13年的分摊时间，整个设施在10年间的资本支出为7200万美元，而服务器则
为3.3×6700万美元，即2亿2100万美元。因此，WSC中服务器在10年中的构建费用要比 WSC
设施高3倍。

\textbf{易犯错误 尝试通过非活跃低功率模式使耗用功率低于活跃低功率模式。}

图6-1显示服务器的平均利用率介于10%~50%之间。在关注6.4节 WSC的运行成本时，
你可能会认为低功率模式也许会很有帮助。

第1章曾经提到，我们无法在这种非活跃低功率模式中访问 DRAM 或磁盘，因此，无论读
写速度多么缓慢，都必须返回完全活联模式才能完成。这一易犯错误在于：返回完全活跃模式
所需要的时间和能耗降低了非活跃低功率模式的吸引力。图6-1显示，几乎所有服务器的平均
利用率都至少为10%，所以我们可以预测它长期处于低活跃状态，而不是长时期的非活联状态。
作为对比，处理器仍然以低功率模式运行，其速率是常规速率的微小倍数，所以活跃低功
率模式更容易使用。注意，处理器返回完全活跃模式的时间也是以微秒来测量的，所以活跃低
功率模式还解决了有关低功率模式的延迟问题。

\textbf{易犯错误 在尝试提高WSC性价比时使用功能较差的处理器。}

Amdahl 定律对WSC仍然适用，这是因为对于每个请求都会有某一连续作业，如果它在缓
慢的服务器上运行，可能会增大请求延迟［Holzle 2010］，［Lim 等人2008］。如果这一连续作业增
大了延迟，那么使用功能较差的处理器时，其成本中必须要增加一部分软件开发成本，以对代
码进行优化，将其延迟降至原来的较低水平。许多缓慢服务器上有更多的线程，也更难以进行
调度和实现负载均衡，因此，线程性能的可变性可能导致更长时间的延迟。如果只有10项任务，
那千分之一的不良调度几率可能不会造成什么问题，但如果有1000项任务，那就的确是一个间
题了，你必须等待那个耗时最长的任务。许多较小的服务器也可能会导致利用率较低，显然，
需要调度的内容越少，调度起来越轻松。最后，如果问题的划分过于精细，那即使一些并行算
法的效率也可能会很低。Google的经验方法是使用低端服务器类计算机［Barroso 和 H6lzle 2009］。
作为一个具体例子，Reddi等人［2010］对比了运行 Bing搜索引擎的嵌入微处理器（Atom）
和服务器徽处理器（Nehalem Xeon）。他们发现，在 Atom 上运行查询的延迟大约是Xeon上的
3倍。此外，Xeon更可靠一些。当Xeon上的负载增加时，服务质量会逐渐、适度下降。而Atom
会因为尝试吸收增加的负载，而快速偏离其服务质量目标。

这一特性直接影响了搜索质量。因为延迟对用户的重要性，所以如表6-5所示，如果查询
延迟还没有超过截止延迟，Bing 搜索引擎会使用多种策略对搜索结果进行精炼。更大型 Xeon
节点的延迟较低，这意味着它们可以用更多的时间来精炼搜索结果。因此，即使在几乎没有负
载时，Atom 在1%的查询中会给出弱于 Xeon 的搜索结果。而在正常负载时，会有2%的搜索结
果弱于 Xeon。

\textbf{谬论 由于DRAM可靠性及WSC软件容错性的提高，所以不需要在WSC的ECC存储器上投入更
多。}

DRAM的测试结果已经宣称其每兆位的故障率为1000至5000FIT（每10亿工作小时的故
障数），而采用ECC会向每64位 DRAM 中添加8位，所以去除纠错码（ECC）有可能节省九
分之一的 DRAM成本［Tezzaron Semniconductor 2004］。

Schroeder、Pinheiro 和 Weber ［2009］在两年半的时间里对带有ECC保护的 DRAM进行了测
试研究，这一工作主要在Google 的WSC上完成，其中包含有数十万台服务器。他们发现测得
的FIT 率要比公布的错误率高15~25倍，即每兆位25 000到70 000次故障。这些故障影响到
超过8%的 DIMIM，DIMM 每年平均有4000次可纠正错误和0.2次不可纠正错误。在服务器上
的测量结果是，每年大约有三分之一的服务器遭遇过DRAM 错误，平均22000次可纠正错误
和1次不可纠正错误。也就是说，对于三分之一的服务器，每2.5小时纠正一次存储器错误。
注意，这些系统使用功能更强大的CHIPKILL代码，而不是简单的 SECDED代码。如果使用更
简单的机制，不可能纠正的错误率会提高至4~10倍。

在一个仅有奇偶校验位错误保护的 WSC中，每发生一个存储器奇偶校验位错误，服务器
都必须重新启动。如果重启时间为5分钟，那三分之一的机器会将 20%的时间花费在重启上！
这一特性会将这个价值1.5亿美元的设施性能降低6%。此外，这些系统可能会週到许多不可纠
正的错误，而操作人员根本就没有注意到这些错误的发生。

在前些年里，Google 使用了甚至没有奇偶校验保护的 DRAM。2000年，在交付下一版搜
索索引之前的测试过程中，它开始建议采用随机文档来回应文本查询［Barroso 和 HItlzle 2009］。
其原因是一些 DRAM 中的“固定o” 故障，它会损坏新的索引。Google 添加了一致性检验，以
检测将来的此类错误。随着WSC规模的增大、ECC DIMIM 价格变得更容易承受，ECC已经成
为 Google WSC的标准。ECC还有另外一个好处：可以在修复期间更轻松地找到损坏的DIMMM。
这些数据间接地表明了 Fermi GPU（第4章）为什么要向其存储器中添加 ECC，它的前辈
甚至连奇偶位保护也没有。此外，这些FIT 率使人们开始关注在 WSC 中使用 Intel Atom处理器
的努力（因为它的功率效率有所提高），2011年的芯片组不支持ECC DRAM。

\textbf{谬论 在低活跃期间关闭硬件可以提高WSC的性价比。}

表6-7显示，配电与冷却基础设施的分摊成本比每个月的全部电费高50%。因此，压缩工
作负载、关闭空闲机器当然可以节省一些费用，不过，即使你能节省一半的电能，也只能将每
月的运行费用降低7%。由于大范围的WSC监控基础设施需要能够探测设备，并查看其反应，
所以还存在一些需要克服的实际问题。能耗比例与活跃低功率模式的另一个好处是它们与 WSC
监控基础设施兼容，使一位操作员能够负责1000多台服务器。

传统的 wSC 精髓是在低活动期间运行其他重要任务，以便弥补配电和冷却方面的投人。
一个主要例子是创建搜索索引的批处理 MapReduce 作业。从低利用率中获取价值的另一个例子
是AWS的现货定价，表6-8的标题中对其进行了介绍。如果 AWS用户执行任务的时间比较灵
活，那就可以使用现货实例，让AWS更灵活地调度这些任务，比如安排在 WSC利用率较低的
时间执行，这样就可以节省计算费用，降低因数在2.7~3之间。

\textbf{谬论 用闪存替换所有磁盘可以提高WSC的性价比。}

对于一些 WSC工作负载，比如进行大量随机读写的负载，闪存的速度要比磁盘快得多。
例如，Fackbook在其 WSC 中部署了封装为固态磁盘（SSD）形式的闪存，作为写回缓存，称为
Flashcache，它是 WSC文件系统的一个组成部分。将访问频繁的文件存储在 Flash 中，访问较
少的文件放在磁盘上。但是，由于 WSC中的所有性能改进都必须从性价比的角度来评价，在
用SSD替换所有磁盘之前，我们关注的是每美元的实际1/0数/秒，以及每美元的存储容量。
在第2章已经看到，每GB闪存的成本至少比磁盘高20倍：2美元/GB比0.09美元/GB。

Narayanan 等人［2009］通过模拟小型与大型数据中心的工作负载轨迹，研究了从磁盘向SSD
迁移工作负载的效果。他们的结论是：由于 SSD 每美元的存储容量很低，所以对于任何工作负
载的成本效率都很低。为了达到平衡点，闪存存储设备需要将每美元的容量数提高3~3000倍，
具体取决于工作负载。

即使将功率代入公式中，对于那些不经常访问的数据，也很难证明用闪存代替磁盘是合理
的。根据6.4节每年2美元/瓦的经验值，一个1TB 的磁盘耗电大约10瓦，通过降低能耗，每
块磁盘每年最多可以节省20元。但是，2011年1TB 闪存的 CAPEX成本为2000美元，而磁盘
仅为90美元。

\section{结语}
WSC 的计算机架构师继承了构建世界上最大计算机的头衔，未来的 IT设备将使移动客户
端变得完善，而 WSC 的架构师正在设计其中的一大部分。我们中有许多人一天会多次使用
WSC，每天使用WSC的次数、使用WSC的人数在接下来的10年中一定会增长。这个星球上
的近70亿人中，已经有超过一半的人拥有移动电话。这些设备都已经为使用因特网做好了准备，
将有更多来自世界各地的人们能够从WSC中获益。

此外，通过WSC实现的规模经济已经实现了长久以来将计算作为公用设施的目标。云计
算意味着任何一个想出好主意、好业务模型的人可以在任何地方使用数千台服务器，几乎在瞬
间展现出其想像力。当然，关于标准、隐私和因特网带宽增长速度等方面还有一些重要的障碍
会限制云计算的发展，但我们预测它们将会得到解决，使云计算进入繁荣期。

由于每个芯片上核心数目的增长（见第5章），集群将会增长到包含数千个核心。我们相信
为运行 WSC而开发的技术对于集群也是有用的，所以集群将会运行那些为 WSC开发的虛拟机
和系统软件。一个好处是可以很轻松地支持“混合式”数据中心，在这种数据中心中，可以很
容易地将工作负载交给云中进行处理，然后再收缩回来，仅依靠本地计算进行处理。

云计算有许多富有吸引力的特性，其中之一就是从经济的角度来鼓励人们节省能源。对于
给定的基础设施投人成本，很难说服云计算供应商关闭未使用设备而节省能量，不过，要说服
云计算使用者放弃空闲实例是很轻松的，因为无论他们是不是在做有用的事情，都要为所使用
的实例支付费用。与此类似，按使用情况收费还会鼓励程序员高效地利用计算、通信和存储，
如果没有一种容易理解的定价方案，是很难有很大动力的。由于这种明确的定价方案使成本的
测量现在变得很容易，而且测量结果也是可信的，所以研究人员就有可能从性价比的角度对创
新技术进行评估，而不只是评估其性能。最后，云计算意味着研究人员可以在数千台计算机的
规模上评估自己的想法，这在过去只有大型公司才能承受得起。

我们相信 WSC 正在改变服务器设计的目标和原理，就像移动客户端的需求正在改变处理
器设计的目标和原理一样。这两者也都对软件行业产生革命性的影响。每美元实现的性能和每
焦耳实现的性能推动着移动客户端硬件和 WSC硬件的发展，并行是实现这些目标的关键。
未来世界是令人兴奋的，而架构师在它的各个方面都将扮演至关重要的角色。我们期待着
见证和使用将要到来的一切。

\section{历史回顾与参考文献}
附录L.8节介绍了集群的发展，它是WSC和公用计算的基础。（希望了解更多知识的读者可
以先阅读Barroso 和HSlzle ［2009］的参考文献，以及 James Hamilton 的博客与访谈：http://perspectives.
mvdirona.com）。
案例研究与练习（Parthasarathy Ranganathan 设计）
案例研究1：影响仓库级计算机设计决策的总拥有成本
本案例研究说明的概念
口总拥有成本（TCO）
口 服务器成本与功率对整个 WSC的影响
口 低功率服务器的优缺点
总拥有成本是衡量仓库级计算机（WSC）有效性的一个重要度量。TCO包含6.4节介绍的CAPEX 和
OPEX，反映了整个数据中心为获得特定性能级别所需要的拥有成本。考虑到不同的服务器、网络和存储
体系结构，数据中心的拥有者经常把TCO 作为一个重要的对比度量指标，以判断哪些选项是最佳的；但
是，TCO是一个考虑了许多不同因素的多维计算。这一案例研究的目标是详细研究 WSC、不同的体系结
构如何影响 TCO、TCO 如何推动经营者的决策。本案例研究将使用来自表6-6和6.4节的数字，并假定所
述WSC达到了经营者的目标性能级别。TCO经常用于对比拥有多种规模的不同服务器选项。本案例研究
中的练习如何在 WSC的上下文中进行此种对比，以及在作出此类决策时所涉及的复杂性。
6.1 ［5/5/10】<6.2、6.4>这一章已经讨论了数据级并行，将其作为 WSC 针对大型问题实现高性能的
一种方法。可以想到，利用高端服务器可以获得更高的性能；但是，更高性能的服务器通常伴
随着以非线性增长的价格。
2.［S］<6.4>假定服务器在同等利用率下快10%，但贵20%，WSC的CAPEX为多少？
b.［5］<6.4>如果这些服务器使用的电力还要多出15%，OPEX为多少？
c.［101 <6.2、6.4>给定以上速度与电力的增加比例，新服务器的成本必须为多少才能与原集群
具有可比性？（提示：根据这一TCO模型，可能需要改变这一设施的关键负载。）
6.2
［5/10］ <6.4、6.8>为了获得较低的 OPEX，一种富有吸引力的替代方法是使用服务器的低功率版
本，以减少运行服务器所需要的总电能；但是，与高端服务器类似，高端组件的低功率版本也
会有非线性权衡。
2.［5］<6.4、6.8>如果低版本服务器选项在提供相同性能的情况下可以降低15%的功率，但要贵
20%，这是不是一种好的权衡？
b. ［10］ <6.4、6.8>这些服务器的成本为多少时才能与原集群具有可比性？如果电能的价格加倍，
又应为多少？
案例研究与练习（Parthasarathy Ranganathan 设计）
355
6.3 ［S/10/15］<6.4、6.6>具有不同运行模式的服务器提供了在集群中动态运行不同配置的可能性，
以匹配工作负载的使用。对于一种给定的低功率服务器，使用表6-11 中关于功率/性能模式的
数据。
a.［5］<6.4、6.6>如果服务器操作人员决定以中等性能运行服务器来节省电力成本，需要多少服
多器才能实现相同级别的性能？
b.［10］<6.4、6.6>这种配置的CAPEX和 OPEX为多少？
c.［15］<6.4、6.6>如果有一种替代方法，一台便宜 20%的服务器，但速度更慢、消耗的电能更
少，给出性能-功率曲线，使其提供与基准服务器具有可比性的TCO。
表6-11 低功率服务的功率性能模式
模
式
性能
高
中
低
100%
75%
59%
功
率
100%
60%
38%
6.4
［讨论］<6.4>讨论练习6.3中两种选项的权衡与好处，假定在这些服务器上运行的工作负载是恒
定的。
6.5
［讨论］<6.2、6.4>WSC 与高性能计算（HPC）集群不同，经常会在一天之内经历大幅的工作负
载波动。讨论练习6.3中两个选项的折中和好处，这一次假定工作负载是变化的。
6.6
［讨论］<6.4、6.7>到目前为止，我们对给出的TCO模型进行了一些抽象，省略了大量低级细节。
讨论这些抽象对 TCO 模型整体准确度的影响。什么时候进行这些抽象是安全的？在哪些情况
下，提供更多的细节会给出明显不同的答案？
案例研究2：WSC中的资源分配与TCO
本案例研究说明的概念
口 WSC 中的服务器与电力供给
口 工作负载的时变特性
口 这些变化对 TCO 的影响
在部署高效 WSC时的一些关键挑战包括：正确供给资源、在最大限度上利用这些资源。由于 WSC
的规模以及所运行工作负载的潜在变化，这一问题可能非常复杂。本案例研究中的练习说明对资源的不同
应用会如何影响TCO。
6.7
［S/5/10］<6.4>为WSC供给资源时的一个挑战是：在给定设施规模的前提下，正确确定功率负载。
如本章所述，铭牌功率经常是很少遇到的峰值。
a. ［5］ <6.4> 如果铭牌功率为200瓦，成本为3000美元，估计每台服务器的TCO如何变化。
b.［5］<6.4>另考虑一种功率更高，但价格更低的选项，其功率为300瓦，成本为3000美元。
c.［10］<6.4>如果服务器的实际平均功率只是铭牌功率的70%，每台服务器的 TCO 如何变化？
6.8
［15/10］<6.2、6.4>TCO模型中有一个假定：此设施的临界负载是固定的，服务器的数量与这一
临界负载相吻合。实际上，由于服务器功率会根据负载发生变化，所以设施使用的临界功率可
能在任意给定时间发生变化。经营者最初必须根据其临界功率资源以及数据中心组件所用功率
量的估计值来为数据中心提供供给。
a.［15］ <6.2、6.4>扩展此TCO模型，以根据铭牌功率为300瓦的服务器为WSC提供初始供给，
并计算所使用的实际月度临界功率和TCO，假定服务器的平均利用率为40%，消耗225瓦。
未使用的容量为多少？
77
478
356
第6章 以仓库级计算机开发请求级、数据级并行
b.［10］ <6.2、6.4>用铭牌功率为500瓦的服务器、平均利用率为20%、消耗300瓦来重复这一
练习。
6.9 ［10］<6.4、6.5>6.5节曾经提到，在使用WSC时经常与终端用户进行交互。这种交互性应用经
常会导致一天之内不同时间的波动，其峰值与特定的时间段相关。例如，对于 Netfix 租用来说，
在晚上8~10点会有一个峰值；一天之内不同时间造成的这些影响在整体上是非常显著的。对
比数据中心在拥有不同容量时每台服务器的 TCO，一种容量值与早晨4点的利用率相对应，另
一种容量值与晚上9点的利用率相匹配。
6.10
［讨论/15］ <6.4、6.5>讨论一些选项，以便在非峰值时间内更好地利用多余的服务器，或者讨论
一些可以节省成本的选项。由于 WSC的交互性本质，为了积极地缩减电能的使用，会面对哪
些挑战？
6.11［讨论/25］<6.4、6.6>给出一种可行方法，通过专门降低服务器功率来提高 TCO。在评估这一方
法时会有哪些挑战？根据你的提议，估计TCO的增长。其优缺点有哪些？
练习
479
6.12 ［10/10/10］<6.1>推动WSC发展的一个重要因素就是有丰富的请求级并行，而不是指令级或线程
级并行。这个问题探讨不同类型的并行对计算机体系结构和系统设计有着什么样的影响。
a.［10］<6.1>讨论一些情景，其中提高指令级或线程线并行所得到的好处可大于通过请求级并行
所能实现的好处。
b.［10］<6.1>增大请求级并行对软件设计有什么样的影响？
c.［10］<6.1>提高请求级并行可能存在哪些缺点？
6.13［讨论/15/15］<6.2>当一个云计算服务提供商接到一些包含多个虚拟机（VM）的作业时（比如
MapReduce 作业），会有许多调度选项。可以用轮询方式来调度这些VM，将其分散在所有可
用处理器和处理器上，也可以对它们进行整合，以尽量减少所使用的处理器数目。利用这些调
度选项，如果提交了一项拥有24个VM的作业，并且云中有30个处理器可供使用（每个处理
器最多可以运行3个 VM），轮询过程将使用24个处理器，合并后的调度过程将使用8个处理
器。调度程序还可以在不同范围内寻找可用处理器核心，即：插槽、服务器、机架和机架阵列。
8.［讨论］<6.2>假定所提交的作业都是计算密集型工作负载，可能会有不同的存储器带宽需求，
就电力与冷却成本、性能和可靠性而言，轮询与合并调度的优缺点都有哪些？
b. ［15］<6.2>假定所提交的作业都是 1/O密集型工作负载，轮询与合并调度在不同范围内的优缺
点有哪些？
c［15］<6.2>個定所提交的作业都是网络密集型工作负载，轮询与合并调度在不同范围内的优敏
点有哪些？
6.14 ［15/15/10/10］<6.2、6.3>MapReduce 在多个节点上运行与数据无关的任务，从而开发出大量并行，
通常使用的是大众化商品硬件；但是，对于并行级别也存在一些限制。例如，为实现冗余，
MapReduce 会将数据块写到多个节点，占用磁盘，还可能占用网络带宽。假定数据集的总大小
为300 GB，网络带宽为1 Gbps,10 s/GB 映射速率、20s/GB约简率。还假定必须从远程节点
读取30%的数据，每个输出文件被写到其他两个节点，以实现冗余。所有其他参数采用表6-4
中的数据。
a. ［15］ <6.2、6.3>假定所有节点都在同一机架内。采用5个节点时的预期运行时间为多少？10
个节点、100个节点、1000个节点呢？讨论每种节点大小的瓶颈。
b. ［15］<6.2、6.3>假定每个机架有40个节点，任意远读取/写人进入任意节点的机会相等。100
个节点的预期运行时间为多少？1000个节点呢？
c.［10］<6.2、6.3>一个重要的考虑因素是尽可能减少数据移动。在从本地到机架、到阵列的访
问速度大幅减缓时，必须对软件进行有效优化，尽量提高局域特性。假定每个机架有40个
案例研究与练习（Parthasarathy Rangamathan 设计）
357
节点，在 MapReduce 作业中使用了1000个节点。如果远程访问在20%的时间都不超出同一
机架，则运行时间为多少？50%的时间呢？80%的时间呢？
d. ［10］ <6.2、6.3>给定6.2节中的简单 MapReduce 程序，讨论一些可能的优化方法，使工作负
载的局城性达到最大。
6.15 ［20/20/10/20/20/20］<6.2>WSC程序员经常使用数据复制来克服软件中的故障。比如，Hadoop
HDFS 采用三路复制（一个本地副本、机架内的一个远程副本、另一机架内的一个远程副本），
但值得研究一下何时需要这些复制。
a.［20］<6.2>Hadoop World 2010参与者调查表明：超过半数 Hadoop 集群的节点数不超过10个，
数据集大小不超过10TB。使用表6-1中的故障频率数据，在采用一路、两路和三路复制时，
10节点 Hadoop 集群的可用性如何？
b. ［20］<6.2>假定有表6-1中的故障数据和一个 1000 节点的Hadoop 集群，在采用一路、两路和
三路复制时，它的可用性如何？
c.［10］<6.2>复制的相对开销随每个本地计算机在每个小时内写人的数据量变化。对于一个
1000 节点、对1PB数据进行排序的Hadoop 作业，计算其额外的1/O通信流量和网络流量（机
架内和跨机架），其中数据混洗的中间结果被写到 FIDFS。
d. ［20］ <6.2>利用表6-4，计算两路与三路复制的时间开销。使用表6-1 所示的故障率，对比在
没有复制与两路及三路复制时的预测执行时间。
e.［20］<6.2>现在考虑一个向日志应用复制操作的数据库系统，假定每个事务平均访问一次硬
盘，生成1KB 的日志数据。计算两路与三路复制的时间开销。如果该事务在存储器内执行，
耗用10us，结果又会如何？
f.［20］<6.2>现在考虑一个采用ACID一致性的数据库系统，它需要两次网络往返进行两阶段确
认。为保持一致性和进行复制所需要的时间开销为多少？
6.16 ［L5/15/20/15/ <6.1、6.2、6.8>尽管请求级并行允许许多计算机并行处理同一问题，从而可以实
现更高的整体性能，但它面对的一个挑战就是避免将问题划分得过于精细。如果在服务级协议
（SLA） 的上下文中来研究这一问题，通过进一步划分来降低问题规模，可能需要更多的工作量
才能实现目标 SLA。假定一个 SAL要求95%的请求在0.5秒或更短时间内得到响应，那类似于
MapReduce 的并行体系结构可以启动多个冗余作业，以获得相同结果。对于以下问题，假定查
询响应时间曲线如图6-13所示。此曲线根据每秒执行的查询数目，显示了基准服务器以及使用
缓慢处理器模型的“小型” 服务器的响应延迟。
3
2.5
2
8Is
1
0.5
0
基
一 小型
480
1
2
3
4
5
6
7
8
9
10
一台服务器每秒的查询数
图6-13 查询响应时间曲线
8.［15］ <6.1、6.2、6.8>假定 WSC每秒接收30000个查询，查询响应时间曲线如图6-13所示，
那么需要多少个服务器来实现该 SLA？给定这一响应时间概率曲线，需要多少个“小型”服
．
358
481
482
第6章 以仓库级计算机开发请求级、数据级并行
务器来实现这一SLA？如果仅关注服务器成本，对于目标 SLA 而言，“廉价”服务器必须
比正常服务器便宣多少才能实现该成本优势？
b.［15］<6.1、6.2、6.8>由于采用了更便宣的组件，所以“小型”服务器的可靠性通常更差一些。
使用图6-1中的数字，假定由于计算机廉价、存储器不良所导致的事件数提高 30%。现在需
要多少“小型”服务器？这些服务器必须比标准服务器便宜多少？
c.［20］<6.1、6.2、6.8>现在假定有一个批处理环境。“小型”服务器提供的总性能为常规服务
器的30%。仍然假定练习6.15b部分中的可靠性数字，需要多少“廉价”节点才能提供一个
2400 节点标准服务器阵列的同等预期吞吐量？假定阵列性能与节点规模之间具有完美的线
性扩展关系，每个节点的平均任务长度为10分钟。如果此扩展为80%呢？60%呢？
d. ［15］<6.1、6.2、6.8>这一扩展通常不是线性函数，而是对数函数。一个很自然的想法可能是
购买更大型的节点，使每个节点拥有更强的计算能力，从而使阵列大小降至最低。讨论这一
体系结构的一些优缺点。
6.17
［10/10/15］<6.3、6.8>高端服务器中的一个趋势是在存储器层次结构中包含非易失性闪存，或者
是通过固态磁盘（SSD）形式，或者是通过 PCI Express 卡。典型SSD的带宽为250 MB/s，延
迟为75us，而PCle卡的带宽为600 MIS/s，延迟为35us。
a.［10］根据图 6-3，并在本地服务器层次结构中包含这些点。假定在不同层次级别可以实现与
DRAM相同的性能扩展因数，当跨机架访问时，这些闪存装置的性能如何？如果是跨阵列呢？
b.［10J讨论一些基于软件的优化方式，以利用存储器层次结构的这个新级别。
c.［25］重复（a）部分，但这次假定每个节点有一个32 GB 的PCle卡，能够缓存50%的全部磁
盘访问。
d. ［15］根据“谬论与易犯错误”（见6.8节）中的讨论，用SSD代替所有磁盘并不一定是一种
具有高成本效率的策略。假定有 WSC经营者使用SSD来提供云服务。讨论一些利用SSD或
其他闪存会有所帮助的情景。
6.18
［20/20/讨论］<6.3>存储静层次结构：在某些 WSC设计中广泛使用缓存来降低延迟，有许多缓存
选项可用于满足不同的访问模式和需求。
2.［20］考虑一些设计选项，用于以流式获取来自 Web（例如，Nethlix）的丰富媒体。首先，我
们需要估计电影数、每部电影的编码格式数、当前正在观看的用户数。2010年，Nettix 有
12 000 部线上媒体，每部至少有四种编码格式（分别为500、1000、1600和2200 kbps）。我
们復定整个网站同时有100000位观看者，每部电影平均长1个小时。估计总存储容量、1/0
与网络带宽以及与视频流相关的计算需求。
b.［20］每位用户、每部电影以及所有电影的访问模式与引用局域性特性如何？（提示：是随机
还是顺序，时域与空域局域性是好还是差，工作集的大小是较小还是较大。）
c.［讨论］利用DRAM、SSD和硬盘，存在哪些电影存储选项？对比它们的性能和TCO。
6.19 ［10/20/20/讨论/计论］<6.3＞考忠一个社交网站，有1亿活跃用户张贴有关自己的更新（以文本和
图片形式），他们在社交网络上浏览更新并进行互动。为了降低延迟，Facebook 和许多其他网
站都使用了 memcached 作为缓存层，放在后端存储/数据库层之前。
2.［10］估计每位用户和整个网站的数据生成与请求率。
b.［20］对于这里讨论的社交网站，需要多少 DRAM来托管其工作集？使用各拥有 96 GB DRAM
的存储器，估计需要多少本地、远程存储器访问来生成一位用户的主页？
c.［20］ 现在考虑两种备选的 memcached 服务器设计，一种使用传统的Xeon 处理器，另一种使
用较小的核心，比如 Atom 处理器。假定 memcached 需要大容量的物理存储器，但 CPU利
用率很低，这两种设计有哪些优缺点？
d. ［讨论］今天，存储器模块和处理器紧密耦合在一起，通常需要增加CPU插槽数，以支持更大
容量的存储器。请列举其他一些设计，能够提供大容量物理存储器，但不会按比例增大服务
案例研究与练习（Parthasarathy Ranganathan 设计）
359
器中的插槽数目。对比这些设计的性能、功率、成本和可靠性。
e.［讨论］同一用户的信息可以存储在 mmemcached 服务器和存储服务器中，可以采用不同方式对
这些服务器进行物理托管。讨论 WSC中以下服务器布局方式的优缺点：（1） memcached 服务
器与存储服务器是同一服务器；（2） memcached 服务器和存储服务器位于同一机架的不同节
点上；（3） memcached 服务器位于同一机架上，存储器服务器位于其他机架上。
6.20
［5/5/10/10/讨论/讨论］≤6.3、6.6>数据中心联网：MapReduce 和 WSC是一种功能强大的组合方
式，可以应对大规模的数据处理；例如，2008年，Google 使用4000台服务器和48 000块硬盘
对1PB记录进行排序，只用了6小时稍多一点的时间。
a.［5］从表6-1 和相关文本中推断磁盘带宽，需要多少秒的时间将数据读人主存储器并写回排序
后的结果？
b.［5］假定每台服务器有两个 1 Gbps 的以太网接口卡（NIC），WSC交换机基础设施的超额认
购系数为4，需要多少秒的时间才能将 4000台服务器上的整个数据集混洗完毕？
c.［10］假定网络传输是PB 级排序的性能瓶颈，能否估计Google 数据中心中的超额认购比？
d. ［10］现在让我们研究拥有10 Gbps 以太网（没有超额认购）的好处，比如使用48端口10 Gbps
以太网（2010年Indy 排序基础测试获胜者 TritonSort 就是采用这一配置）。需要多少时间才
能将1PB数据混洗完毕？
e.［讨论］对比下面两种方法：（1）采用高网络超额认购比的大规模扩展方法；（2）采用高带宽网络的
小规模系统。它们的潜在瓶颈是什么？就可伸缩性和 TCO 而言，它们有哪些优势和劣势？
f.［讨论］排序和许多重要的科学计算工作负载都是计算密集的，而许多其他工作负载则并非如
此。列举三种不会从高速联网中获益的工作负载示例。对于这两类工作负载，你建议使用哪
种EC2实例？
6.21 ［10/25/讨论］<6.4、6.6>由于 WSC的超大规模，根据需要运行的工作负载恰当地分配网络资源
是极为重要的。不同的分配方法可能会对性能和总拥有成本产生严重影响。
a.［10］利用表6-6 中给出的具体数字，每个访问层交换机的超额认购率为多少？如果超额认购
率折半，对TCO有什么影响？如果加倍呢？
b.［25］如果工作负载受网络限制，那降低超额认购率可能会提高性能。假定一项 MapReduce 作
业使用120台服务器，读取5TB数据。假定如表6-2中2009年9月的读取/中间/输出数据比，
并使用表64来确定存储器层次结构的带宽。关于数据读取，假定有50%的数据是从远程磁
盘读取的；其中，80%是从机架内读取，20%是从阵列中读取。对于中间数据和输出数据，假
定30%的数据使用远程磁盘；其中90%在机架范围内，10%在阵列范围内。将超额认购率折
半时，整体性能提高多少？如果超额认购率加倍，性能又变为多少？计算每种情况下的TCO。
c.［讨论］我们看到每个系统正在向更多个核心发展的趋势。我们还看到光纤通信的应用越来越
多（其带宽可能更高，能耗效率也有改进）。你认为这些及其他一些新兴技术趋势将会如何
影啊未来 WSC的设计？
6.22 ［5/15/15/20/25］<6.5>认识 Amazon Web服务的容量：设想你是一家 Alexa.com 顶级网站的网站
运行与基础设施管理员，正在考虑使用 Amazon Web 服务（AWS）。在决定是否迁移到 AWS
之前，需要考虑哪些因素？要使用哪些服务和实例类型，以及将会节省多少成本？可以使用
Alexa 和网站通信流量信息（例如，维基百科上提供了页面查看数据），以估计一个顶级网站
接收到通信流量数目，或者可以从 Web 上选择一些具体示例，比如下面来自 2010年 DrupalCon
San Francisco的示例：http://2bits.com/sites/2bits.com/files/drupal-single-server-2.8-million-page-viewsa-
day.pdf。这些幻灯片描述了一个 Alexa \#3400网站，它每天接收280万个页面视图，使用单个
服务器。这台服务器有两个四核 Xeon 2.5GHiz处理器，8GB DRAM和三个采用 RAIDI 配置的
15 K RPMSAS硬盘，每个月大约消耗400美元。这个网站大量使用缓存，CPU 利用率的变化
范围为 50%~250%（大约有0.5至2.5个核心处于繁忙状态）。
483
360
485
486
第6章 以仓库级计算机开发请求级、数据级并行
a.［5］查看可用的EC2实例（http://aws.amazon.com/ec2/instance-types/），哪些实例类型可以匹
配或超出当前的服务器配置？
b. ［15］查看 EC2 定价信息 （http://aws.amazon.com/ec2/pricing/），选择最具成本效率的 EC2实
例（允许采用组合方式），在AWS上托管此网站。EC2每个月的费用是多少？
c.［15］现在向公式中添加 IP地址和网络通信流量的成本，并假定网站每天在因特网上传输的流
量为100GB。该网站现在每月的费用是多少？
d. ［20］AWS 还向新客户提供1年期的免费微实例，提供15GB带宽，供进出 AWS的通信流量使用。
根据你对自己部门 Web服务器的峰值、平均通信流量的估计，能否将它免费托管在 AWS上？
e.［25］Netfix.com 是一个大得多的网站，它也已经将自己的流化基础设施和编码基础设施迁称
到AWS。根据它们的服务特性，Netflix 可以使用哪些AWS服务？用于何种目的？
6.23［讨论/讨论/20/20/讨论］<6.4>表6-5给出了用户感受的响应时间对收人的影响，激发了在保持低
延迟的情况下实现高吞吐量的需求。
a.［讨论］以Web 搜索为例，有哪些可能缩短查询延迟的方法？
b.［讨论］可以收集哪些监控统计数字，以帮助理解时间花费在哪里？你计划怎样来实现这样一
种监控工具？
c.［20］假定每个查询的磁盘访问数符合正态分布，其均值为2，标准差为3，需要哪种磁盘访问
延迟使95%的查询的延迟 SLA 为0.1秒？
d. ［20］在存储器中进行缓存可以减少长延迟事件（比如，访问硬盘）的频率。假定稳态命中率
为4%，命中延迟为0.05秒，缺失延迟为0.2秒，进行缀存是否有助于满足95%查询的延迟
SLA 为0.1秒的要求？
e. ［讨论］缓存内容什么时候过时，甚至变得不一致？发生这种情况的频繁程度如何？可以怎样
检测这种内容并使其失效？
6.24
［15/15/20］<6.4>典型电源单元（PSU）的效率随负载的变化而变化。例如，在带有40%的负载
时（例如，从100瓦 PSU输出40瓦），PSU效率大约为80%，当负载介于20%与40%之间时，
此效率为75%，当负载低于20%时，效率为65%。
a.［15］假定有一个功率比例的服务器，其功率与CPU 的利用率成比例，利用率曲线如图6-1所
示。平均PSU效率为多少？
6.［15］假定此服务器为 PSU采用了2N冗余（即，将PSU 数目加倍），以确保一个 PSU发生故
障时能够提供稳定电源。平均 PSU效率为多少？
c.［20］刀片服务器供应商使用一种共享的PSU池，不仅用于提供冗余，还可以使PSU 的数量与
服务器的实际功率相匹配。HP c7000封装为总共16个服务器使用了多达6个PSU。在这种
情况下，对于具有相同利用率曲线的服务器封装，平均PSU效率为多少？
6.25
［5/讨论/10/15/讨论/讨论/讨论］<6.4>功率闲置（Power stranding）一词是指提供给数据中心但未
被使用的功率容量。考虑图 6-13针对不同机器组给出的数据［Fan、Weber 和 Barroso 2007］。（注
意，这篇论文中所说的“集群”就是本章所说的“阵列”。）
8. ［5］在（1）机架级别，（2）配电单元级别和（3）阵列（集群）级别的“闲置”功率为多少？在更
大型的机器组中，功率容量超额认购的趋势如何？
b.［讨论］你认为是什么导致不同机器组之间的闲置功率有所不同？
c.［10］考忠一组阵列级别的机器，其中全部机器消耗的功率绝对不超过功率总和的 72%（有时将
其称为“总和的峰值”和“峰值的总和”之比）。利用本案例研究的成本模型，通过对比一个针
对峰值容量进行供电的数据中心和针对实际使用进行供电的数据中心，计算可以节省多少成本。
d.［15］假定数据中心设计者选择在阵列级别包含更多服务器，以充分利用闲置功率。利用图
6-14a 部分中的示例配置和假定，对于所提供的同等总功率，计算现在可以在此仓库级计算
机中多包含多少服务器。
案例研究与练习（Parthasarathy Ranganathan 设计）
361
e.［讨论］在现实部暑中，需要怎样才能使本题（d） 部分中的优化生效？（提示：在很罕见的情况
下，阵列中的所有服务器都耗用峰值功率，考忠在此情况下需要怎样才能划定功率上限。）
t.［讨论］可以设想两种策略来管理功率上限［Ranganathan 等人2006］：（1）先发式策略，功率预
算为事先确定（“不要假定自己可以使用更多的功率，在此之前先行申请！”）或（2）反应
式策略，在超出功率预算时调整功率预算（“得到禁令之前，尽可根据需要任意耗用功率！”）。
讨论这些方法的优缺点，以及在什么情况下使用哪一种方法。
8.［讨论］如果提高了系统的能耗比例特性（假定工作负载与表6-3 中的工作负载相似），总闲
置功率会有什么变化？
0.8
0.6
Ox
0.4
0.2
0
0.4
机架一
PDU⋯…
集群
0.9
0.99
0.98
0.97
0.96
0.95
0.65
机架
PDU…⋯⋯
集群
0.9
0.95
0.5
0.6
0.7
0.8
1
0.7
0.75
归一化功率
（a） 完整分布
0.8
0.85
归一化功率
（b）放大视图
图6-14 真实数据中心的累积分布函数（CDF）
6.26 ［5/20/讨论］<6.4、6.7>6.7节讨论了在Google设计中使用各服务器的电池作为电源。让我们来
研究这一设计的结果。
a. ［5］假定利用电池作为微服务器级别 UPS的效率为99.99%，不再需要在整个设施范围内使用
UPS，后者的效率仅为92%。假定分站交换的效率为99.7%，PDU、递減变换及其他电力断
路器的效率分别为98%、98%和99%。请计算为各个服务器使用电池备份使整个电力基础设
计的效率提高了多少。
b.［20］假定 UPS 的成本占 IT设备的10%。其余假定采用本案例研究成本模型中的数据，电池
的平衡点为多少（以其占单个服务器成本的比例表示），达到此平衡点时，采用电池解决方
案时的总拥有成本优于设施范围内UPS的拥有成本。
c.［讨论］这两种方法之间进行了哪些其他折中？具体来说，你认为这两种不同设计的可管理性
和故障模型会如何变化？
6.27［5/5/讨论］<6.4>对于本练习，考虑一个用于计算 WSC总运行功率的简化公式，如下所示：总运
行功率=（1+冷却低效乘数）xIT设备功率。
a. ［5］假定一个8MW数据中心的功率利用率为80%，电力费用为0.1 美元/千瓦时，冷却低效乘
数为0.8。对比以下优化方式所能节省的成本：（1）将冷却效率提高20%，（2）将TT设备的能
耗效率提高20%。
b.［SJIT 设备能耗效率提高百分之多少才能与冷却效率提高20%节省的成本相匹配？
c.［讨论/10］从服务器能耗效率和冷却系统能耗效率来看，关于这些优化的相对重要性可以得出
哪些结论？
6.28 ［5/5/讨论】<6.4>如本章中的讨论，WSC中的冷却设备本身会消耗许多能量。通过主动控制温度，
可以降低冷却成本。人们已经提出一种优化方式，在安排工作负载时对温度加以考虑，以控制
温度，降低冷却成本。其思想是确定给定房间中的冷却分布图，将较热的系统与较冷的区域相
487
362
488
6.29
489
6.30
第6章 以仓库级计算机开发请求级、数据级并行
对应，从而降低了 WSC级别的整体冷却需求。
a.［5］ CRAC单元的性能系数（COP）定义为所清除的热量（Q）与清除该热量所做的功（W）
之比。CRAC 单元压入通风系统的空气温度升高时，CRAC单元的COP也随之增大。如果返
回 CRAC单元的空气温度为20摄氏度，当COP为1.9时，要清除10 KW的热量，需要在
CRAC 单元中消耗多少能量？如果要对同等数量的空气进行冷却，但这次返回温度为25摄
氏度，取COP 为3.1，则现在要在CRAC单元中消耗多少能量？
b.［S］恨定某种工作负载分配算法能够将热工作负载很好地与冷区域相匹配，使计算机房空调
（CRAC）单元能够运行在较高温度，以提高上面练习中的冷却效率。上述两种情况相比可以
节省多少功率？
c.［讨论］鉴于 WSC 系统的规模，功率管理问题可能非常复杂，涉及多个方面。为提高能耗效率
所采取的改进可以在硬件和软件中实施，可以在系统级、集群级针对 IT 设备或冷却设备实
施，等等。在为 WSC设计整体功耗效率解决方案时，考虑这些交互作用是非常重要的。考
思一种整合算法，它查看服务器利用率，并整合同一计算机上的不同工作负载类别，以提高
服务器利用率。（如果系统不具备能耗比例特性，这一做法可能会使系统运行时具有较高的
能耗效率。）这一优化如何与尝试使用不同功率状态的并发算法交互？请参见 ACPI（高级
配置功率接口）中的一些例子。关于 WSC 中可能相互冲突的多种优化方式，你能想到其他
哪些例子？如何解决这一问题？
［S/10/15/20］<6.2>能耗比例特性（有时也称为“能耗比例缩减”）是系统的一种属性，当系统
空闲时不消耗功率，当活跃性与所完成的工作量增加时，所消耗的功率会逐渐增加，并与其完
成的工作量成比例。在这个练习中，我们将研究能量消耗对不同能耗比例模型的敏感性。在下
面的练习中，除非另行指出，否则默认使用表6-3中的数据。
a.［5］推导能耗比例特性的一种简单方法是假定活跃性与所使用的功率之间存在线性关系。只要
使用表6-3中的峰值功率和空闲功率，并利用线性插值，就可以画出当活性变化时的能耗
效率趋势。（能耗效率表示为“性能/”。）当空闲功率（即活跃度为0%时）为表6-3中所
示数据的一半时，会出现什么情况？如果空闲功率为0，又会发生什么情况？
b.［10］绘制在活性变化时的能耗效率趋势，使用表6-3中第3列的功率变化数据。假定空闲
功率（仅这一指标）为表6-3所示数据的一半，绘制能耗效率。将这些曲线与上一练习中的
线性模型进行对比。关于仅关注空闲功率的后果，可以得到哪些结论？
c.［1S］假定如表6-3第7列的混合系统利用率数据。为简便起见，假定1000台服务器的利用率
为离散分布，109台的利用率为0%，80台服务器的利用率为10%，等等。利用（a）部分和
（b）部分的假定，计算对于这一混合工作负载的总性能和总能耗。
d. ［20］有人可能设计一种系统，当负载级别介于0%~50%之间时，其功率与负载的关系满足亚
线性。这种系统的能耗效率曲线可能在较低利用率时达到峰值（为高利用率为代价）。给出
表6-3中第3列的一个新版本，用以展示这一能耗效率曲线。假定采用表6-3中第7列的混
合系统利用率数据。为简单起见，假定1000台服务器的利用率为离散分布，109台的利用率
为0%，80台服务器的利用率为10%，等等。计算对于这一混合工作负载的总性能和总能耗。
［15/20/20］≤6.2、6.6>这个练习说明了能耗比例模型之间的一些相互作用，这些模型采用了一些
优化方法，比如服务器整合与高能耗效率的服务器设计。考虑表6-12和表6-13所示的情景。
a.［15］考虑两台服务器，其功率分布如表6-12所示：情景 A（图6-4中考虑的服务器）和情景
B（其能耗比例特性低于情景 A，但服务器的能耗效率高于情景A）。假定有表6-3 中第7
列的混合系统利用率。为筒单起见，假定1000台服务器的利用率为离散分布，109台的利用
率为 0%，80台服务器的利用率为10%，等等，如表6-13第1行所示。假定其性能变化如表
6-3第2列所示。对比这两种服务器类型在采用这一混合工作负载时的总性能和总能耗。
b. ［20］考虑一个由1000台服务器组成的集群，其数据类似于表6-3所示数据（汇总在表6-12和
案例研究与练习（Parthasarathy Ranganathan 设计）
363
表6-13的第一行内）。根据这些假定，在此混合工作负载情况下的总性能和总能耗为多少？
现在假定我们能够整合工作负载，对情景C中所示的分布进行建模（表6-13 的第二行）。
现在的总性能和总能耗是多少？某一系统具有线性能耗比例模型，其空闲功率为0瓦，峰值
功率为662瓦，与这一系统相比，前面计算的总能耗如何？
c［201重复（b）部分，但这一次采用服务器B的功率模型，并与（a）部分的结果进行对比。
表6-12
两台服务暑的功率分布
\begin{verbatim}
    活跃度〈%）
    10
    20
    30
    40
    50
    60
    70
    80
    80
    100
    功率、情景A（W）
    181
    308
    351
    382
    416
    451
    490
    $33
    576
    617
    662
    功率、情景B（W）
    250
    275
    325
    340
    395
    405
    415
    425
    440
    445
    450
    表6-13
    集群中在无、有合并情景下的利用率分布
    活跃度（%）
    o
    10
    20
    30
    40
    50
    60
    70
    80
    90
    100
    无、服务器、情景A和B
    109
    80
    153
    246
    191
    115
    $1
    21
    15
    12
    8
    无、服务器、情景C
    504
    6
    8
    11
    26
    57
    95
    123
    76
    40
    \$4
\end{verbatim}
6.31［10/讨论］<6.2、6.4、6.6>系统级能耗比例趋势。考虑一台服务器功率消耗的如下平衡点：
CPU,50%；存储器，
23%；磁盘，11%；网络/其他，16%
CPU,33%；存储器，
30%；磁盘，10%；网络/其他，27%
a.［10］假定一个 CPU的动态功率范围为3.0倍（即，CPU 在空闲时的功率消耗是其峰億工作负
载时功率消耗的三分之一。）假定上述存储器系统、磁盘和网络/其他类别的动态范围分别为
2.0、3.0和1.2倍。这两种情况下，总系统的整体动态范围为多少？
b.［讨论/10］从（a）部分的结果中能够学到些什么？如何在系统级别实现更好的能耗比例特性？
（提示：仅通过CPU优化不能实现系统級的能耗比例特性，而是需要对所有组件进行改进。）
6.32
［30］<6.4>Pitt Tumer IV 等人［2008］对数据中心层的分类进行了很好的综述。层分类确定了网站
基础设施的性能。为简单起见，考虑如表6-14所示的关键差别（摘自 Pitt Turner IV 等人［2008］）。
使用此案例研究中的 TCO模型，对比所显示的不同层对成本的影响。
表6-14 数据中心层分类概述
第1层
用于配电和冷都的单一路径，没有冗余组件
99.0%
第2𡲢
（N+1）元余=两个配电与冷却路径
99.7%
第3层
（N+2）冗余=三系配电与冷却路径，即使在维护期间也能正常工作
99.98%
第4层
两个活跃配电与冷却路径，每条路径有冗余组件，以容忍任何单一
99.9995%
设备故障，不对负载产生影响
* 摘自 Pitt Turner IV 等人［2008］。
6.33［讨论］<6.4>根据表6-6中的观察结果，关于宕机时间的收入损失和正常工作所需要的成本，可
以定性地说点什么？
6.34 ［15/讨论］<6.4>最近的一些研究定义了名为 TPUE 的度量，表示“真正PUE”（true PUE）或“总
PUE” （total PUE）的意思。TPUE定义为 PUE × SPUE。PUE 表示功率利用效率，在第6.4节
中定义，为总设施功率与总 IT设备功率之比。SPUE（服务器PUE）是一个与 PUE类似的新度
量，但它不适用于计算设备，而是定义为总服务器输入功率与其有用功率之比，其中有用功率
定义为直接参与计算的电子组件所消耗的功率，这些电子组件包括：主板、磁盘、CPU、DRAM、
1/O卡，等等。换句话说，SPUE 度量衡量的是一台服务器中所包含的电源、电压调节器和风扇
等器件的效率不佳程度。
8. ［15］<6.4>考虑一种设计，它为 CRAC单元提供的空气温度较高。CRAC 单元的效率大约与
490
364
491
492
493
第6章 以仓库级计算机开发请求级、数据级并行
温度的4次方成正比，这一设计从而提高了整体PUE，假定提高了7%假定基准 PUE为1.7）
但是，服务嚣级别的较高温度会触发板载风扇控制器，使其控制的风扇转速高出许多。风扇
功率是速度的三次函数，提高风扇速度会导致 SPUE的下降。假定风扇的功率模型为：
风扇功率=284×ns xns ×ns-75 xns XnS
其中，ns是归一化风扇转速=风扇转速/18000（rps）。
基准服务器功率为350W。如果风扇转速（1）从 10 000 t/in 增加到12 500 r/min，（2）从 10 000 t/min
增加到18000r/in，计算SPUE。对比这两种情况下的PUE 和TPUE。（为简单起见，忽略
在 SPUE 模型中功率输出的低效程度。）
b.［讨论］（a）部分说明：尽管PUE 对于衡量设备开销是一个出色的度量，但它并不能衡量TT设备本
身的低效程度。你能否指出另外一种设计，其TPUE 可能低于 PUE？（提示：参见练习6.26。）
6.35
［讨论/30/讨论］<6.2＞要衡量服务器中的能耗效率，以最近发布的两个基准测试作为起点是非
常合适的：\verb|SPECpower_ssj| 2008 基准测试（可从 \verb|http://wrww.spec.org/powe._ssj2008/|获取）和
JouleSort 度量（可从 http://sortbenchmark.org/获取）。
a.［讨论］<6.2>查看关于这两个基准测试的描述。它们有哪些相似之处？有哪些不同之处？为了
改进这两个基准测试，以更好地与提高WSC能耗效率的目标保持一致，可以做些什么？
b.［30］ <6.2>JouleSort 测量全部系统能耗，以执行核心外的排序，并尝试推导出一种度量，用来
对比从嵌入式设备到超级计算机的各种不同系统。查看 http://sortbenchmark.org 中关于
JouleSort的说明。下载该排序算法的公共使用版本，并在不同类型的计算机上运行它，比如
便携式计算机、PC 和移动电话等，或者采用不同配置运行该算法。从不同设置的 JouleSort
评价结果中可以学到些什么？
c.［讨论］<6.2>考虑在上述试验中获得最佳 JouleSort评级的系统。如何提高能耗效率呢？例如，
尝试重写该排序代码，以提高 JouleSort评级。
6.36
［10/10/15］<6.1、6.2>表6-1 是一个服务器阵列的停运列表。在应对大规模的WSC时，做到集群
设计与软件体系结构的平衡是非常重要的，以在不增加大量成本的情况下实现所需要的正常运
行时间。这个问题研究仅通过硬件来实现可用性的含义。
a.［10］<6.1、6.2>假定经营者希望通过仅改进服务器硬件来实现95%的可用性，必须减少多少
各类事件？现在，假定通过冗余机器完全解决了个别服务器崩溃问题。
b.［10］<6.1、6.2>如果50%的时间内是通过冗余处理了个别服务器崩溃问题，（a）部分的答案将
如何变化？20%的时间呢？0%的时间呢？
c.［15］<6.1、6.2>讨论软件冗余性对于实现高级可用性的重要性。如果 WSC经营者考虑购买一
些机器，这些机器略便宣一些，但可靠性也降低10%，这对软件体系结构可能有哪些影响？
与软件冗余相关联的挑战有哪些？
6.37 ［15］<6.1、6.8>查看标准 DDR3 DRAM与带有纠错码（ECC）的 DDR3 DRAM 的当前价格。为
T实现ECC所提供的更高可靠性，每比特的价格会提高多少？仅使用这些 DRAM 价格以及6.8
节提供的数据，对于采用非 ECC DRAM与有 ECC DRAM的 WSC，每美元的工作时间是多少？
6.38 ［5/讨论】<6.1>WSC 可靠性和可管理性方面的考虑因素。
a.［5］考虑一个服务器的集群，其中每台服多器的成本为2000美元。假定年度故障率为5%，每
次修复的平均服务时间为1小时，每次故障更换零件时需要系统成本的10%，每台服务器的
年度维护费用为多少？假定一个服务技师每小时收费为100美元。
b.［讨论］解释这一可管理性模型与传统企业数据中心可管理性模型的差别，在这一传统企业数
据中心中，有大量中小型应用程序，分别运行在自己的专用硬件基础设施上。